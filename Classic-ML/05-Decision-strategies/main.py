"""
Decision Strategies –¥–ª—è MountainCar –∑–∞–¥–∞—á—ñ

–ú–µ—Ç–∞: –ø–æ—Ä—ñ–≤–Ω—è—Ç–∏ —Ä—ñ–∑–Ω—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω—å + –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è —É—Å–ø—ñ—à–Ω–æ—Å—Ç—ñ –µ–ø—ñ–∑–æ–¥—ñ–≤
- –í–∏–ø–∞–¥–∫–æ–≤–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è (Random)
- –ñ–∞–¥—ñ–±–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ —à–≤–∏–¥–∫–æ—Å—Ç—ñ (Velocity-based)
- Epsilon-Greedy —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è
- Q-Learning
- –°—Ç—Ä–∞—Ç–µ–≥—ñ—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü—ñ—ó –ø—Ä–æ—Å—Ç–æ—Ä—É —Å—Ç–∞–Ω—ñ–≤

–î–æ–¥–∞—Ç–∫–æ–≤–æ:
- –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö –∑ –µ–ø—ñ–∑–æ–¥—ñ–≤ (—Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ—ó, –æ–∑–Ω–∞–∫–∏)
- –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è —É—Å–ø—ñ—à–Ω–æ—Å—Ç—ñ –µ–ø—ñ–∑–æ–¥—É (–¥–æ—Å—è–≥–Ω–µ/–Ω–µ –¥–æ—Å—è–≥–Ω–µ –º–µ—Ç–∏)
- –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤: LDA, SVM, Random Forest, Logistic Regression
- ROC-–∫—Ä–∏–≤—ñ —Ç–∞ AUC –º–µ—Ç—Ä–∏–∫–∏
- –Ü–Ω–∂–µ–Ω–µ—Ä—ñ—è –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–∏—Ö –æ–∑–Ω–∞–∫

–°–µ—Ä–µ–¥–æ–≤–∏—â–µ MountainCar:
- –°—Ç–∞–Ω: [–ø–æ–∑–∏—Ü—ñ—è, —à–≤–∏–¥–∫—ñ—Å—Ç—å]
- –î—ñ—ó: 0 (–ª—ñ–≤–æ—Ä—É—á), 1 (–Ω—ñ—á–æ–≥–æ), 2 (–ø—Ä–∞–≤–æ—Ä—É—á)
- –ú–µ—Ç–∞: –¥—ñ—Å—Ç–∞—Ç–∏—Å—å –¥–æ –ø—Ä–∞–ø–æ—Ä—Ü—è –Ω–∞ –ø—Ä–∞–≤—ñ–π –≥–æ—Ä—ñ (–ø–æ–∑–∏—Ü—ñ—è >= 0.5)
"""

import os
import pathlib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import gymnasium as gym
from collections import defaultdict
from typing import Tuple, List, Dict
import warnings
warnings.filterwarnings('ignore')

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, precision_score,
    recall_score, f1_score, roc_curve, auc, RocCurveDisplay,
    confusion_matrix, classification_report
)

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
sns.set(style="whitegrid", context="notebook")
plt.rcParams["figure.figsize"] = (10, 6)
plt.rcParams['font.size'] = 10

# –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
OUTPUT_DIR = pathlib.Path(__file__).parent / "plots"
OUTPUT_DIR.mkdir(exist_ok=True)

print("=" * 70)
print("DECISION STRATEGIES –î–õ–Ø MOUNTAINCAR")
print("=" * 70)


# ============================================================================
# –°–¢–†–ê–¢–ï–ì–Ü–á –ü–†–ò–ô–ù–Ø–¢–¢–Ø –†–Ü–®–ï–ù–¨
# ============================================================================

class RandomStrategy:
    """–í–∏–ø–∞–¥–∫–æ–≤–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è - –≤–∏–±–∏—Ä–∞—î –¥—ñ—é –≤–∏–ø–∞–¥–∫–æ–≤–æ"""
    
    def __init__(self, action_space):
        self.action_space = action_space
        self.name = "–í–∏–ø–∞–¥–∫–æ–≤–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è"
    
    def select_action(self, state: np.ndarray) -> int:
        return self.action_space.sample()
    
    def update(self, state, action, reward, next_state, done):
        pass  # –ù–µ–º–∞—î –Ω–∞–≤—á–∞–Ω–Ω—è


class VelocityBasedStrategy:
    """–ñ–∞–¥—ñ–±–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ —à–≤–∏–¥–∫–æ—Å—Ç—ñ"""
    
    def __init__(self):
        self.name = "–°—Ç—Ä–∞—Ç–µ–≥—ñ—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ —à–≤–∏–¥–∫–æ—Å—Ç—ñ"
    
    def select_action(self, state: np.ndarray) -> int:
        position, velocity = state
        
        # –Ø–∫—â–æ —Ä—É—Ö–∞—î–º–æ—Å—å –≤–ø—Ä–∞–≤–æ - —Ç–∏—Å–Ω–∏ –≤–ø—Ä–∞–≤–æ
        if velocity > 0:
            return 2  # –ø—Ä–∞–≤–æ—Ä—É—á
        # –Ø–∫—â–æ —Ä—É—Ö–∞—î–º–æ—Å—å –≤–ª—ñ–≤–æ - —Ç–∏—Å–Ω–∏ –≤–ª—ñ–≤–æ
        elif velocity < 0:
            return 0  # –ª—ñ–≤–æ—Ä—É—á
        # –Ø–∫—â–æ —Å—Ç–æ—ó–º–æ - —Ç–∏—Å–Ω–∏ –≤–ø—Ä–∞–≤–æ (–¥–æ –º–µ—Ç–∏)
        else:
            return 2  # –ø—Ä–∞–≤–æ—Ä—É—á
    
    def update(self, state, action, reward, next_state, done):
        pass  # –î–µ—Ç–µ—Ä–º—ñ–Ω—ñ—Å—Ç–∏—á–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è, –±–µ–∑ –Ω–∞–≤—á–∞–Ω–Ω—è


class EpsilonGreedyVelocityStrategy:
    """Epsilon-Greedy –≤–∞—Ä—ñ–∞–Ω—Ç velocity —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó"""
    
    def __init__(self, action_space, epsilon: float = 0.1):
        self.action_space = action_space
        self.epsilon = epsilon
        self.velocity_strategy = VelocityBasedStrategy()
        self.name = f"Epsilon-Greedy (Œµ={epsilon})"
    
    def select_action(self, state: np.ndarray) -> int:
        # –ó –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é epsilon - –≤–∏–ø–∞–¥–∫–æ–≤–∞ –¥—ñ—è
        if np.random.random() < self.epsilon:
            return self.action_space.sample()
        # –Ü–Ω–∞–∫—à–µ - velocity-based
        else:
            return self.velocity_strategy.select_action(state)
    
    def update(self, state, action, reward, next_state, done):
        pass


class QLearningStrategy:
    """Q-Learning –∑ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü—ñ—î—é –ø—Ä–æ—Å—Ç–æ—Ä—É —Å—Ç–∞–Ω—ñ–≤"""
    
    def __init__(self, action_space, n_bins: int = 20, 
                 learning_rate: float = 0.1, gamma: float = 0.99,
                 epsilon: float = 0.1):
        self.action_space = action_space
        self.n_actions = action_space.n
        self.n_bins = n_bins
        self.lr = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.name = f"Q-Learning (bins={n_bins})"
        
        # Q-—Ç–∞–±–ª–∏—Ü—è
        self.q_table = defaultdict(lambda: np.zeros(self.n_actions))
        
        # –ú–µ–∂—ñ –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü—ñ—ó
        self.position_bins = np.linspace(-1.2, 0.6, n_bins)
        self.velocity_bins = np.linspace(-0.07, 0.07, n_bins)
    
    def discretize_state(self, state: np.ndarray) -> Tuple[int, int]:
        """–î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü—ñ—è –Ω–µ–ø–µ—Ä–µ—Ä–≤–Ω–æ–≥–æ —Å—Ç–∞–Ω—É"""
        position, velocity = state
        pos_idx = np.digitize(position, self.position_bins)
        vel_idx = np.digitize(velocity, self.velocity_bins)
        return (pos_idx, vel_idx)
    
    def select_action(self, state: np.ndarray) -> int:
        discrete_state = self.discretize_state(state)
        
        # Epsilon-greedy
        if np.random.random() < self.epsilon:
            return self.action_space.sample()
        else:
            q_values = self.q_table[discrete_state]
            return int(np.argmax(q_values))
    
    def update(self, state, action, reward, next_state, done):
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è Q-–∑–Ω–∞—á–µ–Ω—å"""
        s = self.discretize_state(state)
        s_next = self.discretize_state(next_state)
        
        # Q-learning update
        current_q = self.q_table[s][action]
        
        if done:
            target = reward
        else:
            max_next_q = np.max(self.q_table[s_next])
            target = reward + self.gamma * max_next_q
        
        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è
        self.q_table[s][action] = current_q + self.lr * (target - current_q)


class AdvancedVelocityStrategy:
    """–ü–æ–∫—Ä–∞—â–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –ø–æ–∑–∏—Ü—ñ—ó —Ç–∞ —ñ–Ω–µ—Ä—Ü—ñ—ó"""
    
    def __init__(self):
        self.name = "–ü–æ–∫—Ä–∞—â–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è (–ø–æ–∑–∏—Ü—ñ—è + —à–≤–∏–¥–∫—ñ—Å—Ç—å)"
    
    def select_action(self, state: np.ndarray) -> int:
        position, velocity = state
        
        # –Ø–∫—â–æ –º–∞–π–∂–µ –¥–æ—Å—è–≥–ª–∏ –º–µ—Ç–∏ - –ø—Ä–æ–¥–æ–≤–∂—É–π —Ç–∏—Å–Ω—É—Ç–∏ –≤–ø—Ä–∞–≤–æ
        if position > 0.3:
            return 2
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π —ñ–Ω–µ—Ä—Ü—ñ—é: —Ä–æ–∑–≥–æ–π–¥—É–π—Å—è
        if velocity > 0.01:  # –†—É—Ö –≤–ø—Ä–∞–≤–æ –∑ —Ö–æ—Ä–æ—à–æ—é —à–≤–∏–¥–∫—ñ—Å—Ç—é
            return 2
        elif velocity < -0.01:  # –†—É—Ö –≤–ª—ñ–≤–æ –∑ —Ö–æ—Ä–æ—à–æ—é —à–≤–∏–¥–∫—ñ—Å—Ç—é
            return 0
        else:
            # –ú–∞–ª—ñ —à–≤–∏–¥–∫–æ—Å—Ç—ñ - —Ä–æ–∑–≥–æ–π–¥—É–π—Å—è –≤ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—ñ–¥ –ø–æ–∑–∏—Ü—ñ—ó
            if position < -0.5:
                return 2  # –ø—Ä–∞–≤–æ—Ä—É—á
            else:
                return 0  # –ª—ñ–≤–æ—Ä—É—á –¥–ª—è —Ä–æ–∑–≥–æ–Ω—É
    
    def update(self, state, action, reward, next_state, done):
        pass


# ============================================================================
# –û–¶–Ü–ù–ö–ê –°–¢–†–ê–¢–ï–ì–Ü–ô
# ============================================================================

def evaluate_strategy(strategy, env, n_episodes: int = 100, max_steps: int = 200,
                     render: bool = False) -> Dict:
    """
    –û—Ü—ñ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –Ω–∞ n_episodes –µ–ø—ñ–∑–æ–¥—ñ–≤
    
    Returns:
        dict: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (—É—Å–ø—ñ—à–Ω—ñ—Å—Ç—å, —Å–µ—Ä–µ–¥–Ω—è –Ω–∞–≥–æ—Ä–æ–¥–∞, –∫—Ä–æ–∫–∏)
    """
    successes = 0
    total_rewards = []
    episode_lengths = []
    final_positions = []
    
    for episode in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        
        for step in range(max_steps):
            action = strategy.select_action(state)
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            strategy.update(state, action, reward, next_state, done)
            
            episode_reward += reward
            state = next_state
            
            if done:
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —É—Å–ø—ñ—à–Ω–æ—Å—Ç—ñ (–¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è –º–µ—Ç–∏)
                if next_state[0] >= 0.5:
                    successes += 1
                final_positions.append(next_state[0])
                episode_lengths.append(step + 1)
                break
        else:
            # –ï–ø—ñ–∑–æ–¥ –∑–∞–≤–µ—Ä—à–∏–≤—Å—è –±–µ–∑ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è –º–µ—Ç–∏
            final_positions.append(state[0])
            episode_lengths.append(max_steps)
        
        total_rewards.append(episode_reward)
    
    return {
        'success_rate': successes / n_episodes,
        'avg_reward': np.mean(total_rewards),
        'std_reward': np.std(total_rewards),
        'avg_steps': np.mean(episode_lengths),
        'std_steps': np.std(episode_lengths),
        'avg_final_position': np.mean(final_positions),
        'total_rewards': total_rewards,
        'episode_lengths': episode_lengths
    }


def train_and_evaluate_qlearning(env, n_training_episodes: int = 1000,
                                n_eval_episodes: int = 100) -> Tuple[QLearningStrategy, Dict]:
    """
    –ù–∞–≤—á–∞–Ω–Ω—è Q-Learning —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
    """
    print("\n" + "=" * 70)
    print("–ù–ê–í–ß–ê–ù–ù–Ø Q-LEARNING")
    print("=" * 70)
    
    strategy = QLearningStrategy(
        env.action_space,
        n_bins=20,
        learning_rate=0.1,
        gamma=0.99,
        epsilon=0.1
    )
    
    training_rewards = []
    training_successes = []
    window_size = 100
    
    print(f"–ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {n_training_episodes} –µ–ø—ñ–∑–æ–¥—ñ–≤...")
    
    for episode in range(n_training_episodes):
        state, _ = env.reset()
        episode_reward = 0
        success = False
        
        for step in range(200):
            action = strategy.select_action(state)
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            strategy.update(state, action, reward, next_state, done)
            
            episode_reward += reward
            state = next_state
            
            if done:
                if next_state[0] >= 0.5:
                    success = True
                break
        
        training_rewards.append(episode_reward)
        training_successes.append(1 if success else 0)
        
        # –ü—Ä–æ–≥—Ä–µ—Å
        if (episode + 1) % 100 == 0:
            recent_success_rate = np.mean(training_successes[-window_size:])
            recent_avg_reward = np.mean(training_rewards[-window_size:])
            print(f"  –ï–ø—ñ–∑–æ–¥ {episode + 1}/{n_training_episodes}: "
                  f"Success Rate={recent_success_rate:.2%}, "
                  f"Avg Reward={recent_avg_reward:.1f}")
    
    print("\n‚úì –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"  –†–æ–∑–º—ñ—Ä Q-—Ç–∞–±–ª–∏—Ü—ñ: {len(strategy.q_table)} —Å—Ç–∞–Ω—ñ–≤")
    
    # –û—Ü—ñ–Ω–∫–∞ –ø—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
    print(f"\n–û—Ü—ñ–Ω–∫–∞ –Ω–∞ {n_eval_episodes} –µ–ø—ñ–∑–æ–¥—ñ–≤...")
    results = evaluate_strategy(strategy, env, n_episodes=n_eval_episodes)
    
    # –î–æ–¥–∞—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é –Ω–∞–≤—á–∞–Ω–Ω—è
    results['training_rewards'] = training_rewards
    results['training_successes'] = training_successes
    
    return strategy, results


# ============================================================================
# –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø
# ============================================================================

def plot_strategy_comparison(results: Dict[str, Dict], save_path: pathlib.Path):
    """–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π: success rate, avg reward, avg steps"""
    
    strategies = list(results.keys())
    success_rates = [results[s]['success_rate'] * 100 for s in strategies]
    avg_rewards = [results[s]['avg_reward'] for s in strategies]
    avg_steps = [results[s]['avg_steps'] for s in strategies]
    
    fig, axes = plt.subplots(1, 3, figsize=(16, 5))
    
    # Success Rate
    ax = axes[0]
    bars = ax.bar(range(len(strategies)), success_rates, color='steelblue', alpha=0.8)
    ax.set_xticks(range(len(strategies)))
    ax.set_xticklabels(strategies, rotation=45, ha='right')
    ax.set_ylabel('Success Rate (%)', fontsize=11)
    ax.set_title('–£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π\n(% –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è –º–µ—Ç–∏)', fontsize=12, weight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    # –î–æ–¥–∞—Ç–∏ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞ —Å—Ç–æ–≤–ø—á–∏–∫–∏
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom', fontsize=10)
    
    # Average Reward
    ax = axes[1]
    bars = ax.bar(range(len(strategies)), avg_rewards, color='coral', alpha=0.8)
    ax.set_xticks(range(len(strategies)))
    ax.set_xticklabels(strategies, rotation=45, ha='right')
    ax.set_ylabel('–°–µ—Ä–µ–¥–Ω—è –Ω–∞–≥–æ—Ä–æ–¥–∞', fontsize=11)
    ax.set_title('–°–µ—Ä–µ–¥–Ω—è —Å—É–º–∞—Ä–Ω–∞ –Ω–∞–≥–æ—Ä–æ–¥–∞\n(—á–∏–º –≤–∏—â–µ, —Ç–∏–º –∫—Ä–∞—â–µ)', fontsize=12, weight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}', ha='center', va='bottom', fontsize=10)
    
    # Average Steps
    ax = axes[2]
    bars = ax.bar(range(len(strategies)), avg_steps, color='lightgreen', alpha=0.8)
    ax.set_xticks(range(len(strategies)))
    ax.set_xticklabels(strategies, rotation=45, ha='right')
    ax.set_ylabel('–°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤', fontsize=11)
    ax.set_title('–°–µ—Ä–µ–¥–Ω—è –¥–æ–≤–∂–∏–Ω–∞ –µ–ø—ñ–∑–æ–¥—É\n(—á–∏–º –º–µ–Ω—à–µ, —Ç–∏–º –∫—Ä–∞—â–µ)', fontsize=12, weight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.0f}', ha='center', va='bottom', fontsize=10)
    
    plt.suptitle('–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω—å (MountainCar)', 
                 y=1.00, fontsize=14, weight='bold')
    plt.tight_layout()
    plt.savefig(save_path / "strategy_comparison.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'strategy_comparison.png'}")


def plot_reward_distributions(results: Dict[str, Dict], save_path: pathlib.Path):
    """–†–æ–∑–ø–æ–¥—ñ–ª –Ω–∞–≥–æ—Ä–æ–¥ –¥–ª—è –∫–æ–∂–Ω–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó"""
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    strategies = list(results.keys())
    data = [results[s]['total_rewards'] for s in strategies]
    
    bp = ax.boxplot(data, labels=strategies, patch_artist=True, 
                     notch=True, showmeans=True)
    
    # –†–æ–∑—Ñ–∞—Ä–±—É–≤–∞—Ç–∏ boxplots
    colors = ['steelblue', 'coral', 'lightgreen', 'gold', 'plum']
    for patch, color in zip(bp['boxes'], colors[:len(strategies)]):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    ax.set_xlabel('–°—Ç—Ä–∞—Ç–µ–≥—ñ—è', fontsize=11)
    ax.set_ylabel('–°—É–º–∞—Ä–Ω–∞ –Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –µ–ø—ñ–∑–æ–¥', fontsize=11)
    ax.set_title('–†–æ–∑–ø–æ–¥—ñ–ª –Ω–∞–≥–æ—Ä–æ–¥ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è—Ö', fontsize=13, weight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(save_path / "reward_distributions.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'reward_distributions.png'}")


def plot_learning_curve(training_rewards: List[float], training_successes: List[float],
                       save_path: pathlib.Path, window: int = 100):
    """–ö—Ä–∏–≤–∞ –Ω–∞–≤—á–∞–Ω–Ω—è Q-Learning"""
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Rewards
    ax = axes[0]
    episodes = np.arange(1, len(training_rewards) + 1)
    
    # –ó–≥–ª–∞–¥–∂–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
    smoothed_rewards = np.convolve(training_rewards, 
                                   np.ones(window)/window, mode='valid')
    
    ax.plot(episodes, training_rewards, alpha=0.3, color='gray', 
            label='–°–∏—Ä—ñ –¥–∞–Ω—ñ')
    ax.plot(episodes[window-1:], smoothed_rewards, linewidth=2, 
            color='steelblue', label=f'–ö–æ–≤–∑–Ω–µ —Å–µ—Ä–µ–¥–Ω—î ({window} –µ–ø—ñ–∑–æ–¥—ñ–≤)')
    ax.set_xlabel('–ï–ø—ñ–∑–æ–¥', fontsize=11)
    ax.set_ylabel('–°—É–º–∞—Ä–Ω–∞ –Ω–∞–≥–æ—Ä–æ–¥–∞', fontsize=11)
    ax.set_title('–î–∏–Ω–∞–º—ñ–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è Q-Learning\n(–Ω–∞–≥–æ—Ä–æ–¥–∞)', fontsize=12, weight='bold')
    ax.legend()
    ax.grid(alpha=0.3)
    
    # Success rate
    ax = axes[1]
    
    # –ó–≥–ª–∞–¥–∂–µ–Ω–∞ success rate
    smoothed_successes = np.convolve(training_successes, 
                                     np.ones(window)/window, mode='valid') * 100
    
    ax.plot(episodes, np.array(training_successes) * 100, alpha=0.3, 
            color='gray', label='–°–∏—Ä—ñ –¥–∞–Ω—ñ')
    ax.plot(episodes[window-1:], smoothed_successes, linewidth=2, 
            color='coral', label=f'–ö–æ–≤–∑–Ω–µ —Å–µ—Ä–µ–¥–Ω—î ({window} –µ–ø—ñ–∑–æ–¥—ñ–≤)')
    ax.set_xlabel('–ï–ø—ñ–∑–æ–¥', fontsize=11)
    ax.set_ylabel('Success Rate (%)', fontsize=11)
    ax.set_title('–î–∏–Ω–∞–º—ñ–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è Q-Learning\n(—É—Å–ø—ñ—à–Ω—ñ—Å—Ç—å)', fontsize=12, weight='bold')
    ax.legend()
    ax.grid(alpha=0.3)
    ax.set_ylim([0, 105])
    
    plt.tight_layout()
    plt.savefig(save_path / "qlearning_learning_curve.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'qlearning_learning_curve.png'}")


def plot_episode_lengths(results: Dict[str, Dict], save_path: pathlib.Path):
    """–†–æ–∑–ø–æ–¥—ñ–ª –¥–æ–≤–∂–∏–Ω–∏ –µ–ø—ñ–∑–æ–¥—ñ–≤"""
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    strategies = list(results.keys())
    colors = ['steelblue', 'coral', 'lightgreen', 'gold', 'plum']
    
    for i, strategy in enumerate(strategies):
        lengths = results[strategy]['episode_lengths']
        ax.hist(lengths, bins=30, alpha=0.6, label=strategy, 
                color=colors[i % len(colors)])
    
    ax.set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤ –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –µ–ø—ñ–∑–æ–¥—É', fontsize=11)
    ax.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞', fontsize=11)
    ax.set_title('–†–æ–∑–ø–æ–¥—ñ–ª –¥–æ–≤–∂–∏–Ω–∏ –µ–ø—ñ–∑–æ–¥—ñ–≤ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è—Ö', fontsize=13, weight='bold')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path / "episode_lengths.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'episode_lengths.png'}")


def plot_state_space_exploration(strategy: QLearningStrategy, save_path: pathlib.Path):
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –æ—Å–≤–æ—î–Ω–Ω—è –ø—Ä–æ—Å—Ç–æ—Ä—É —Å—Ç–∞–Ω—ñ–≤ Q-Learning"""
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ —Å—ñ—Ç–∫—É —Å—Ç–∞–Ω—ñ–≤
    positions = np.linspace(-1.2, 0.6, 100)
    velocities = np.linspace(-0.07, 0.07, 100)
    
    # –û–±—á–∏—Å–ª–∏—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ –¥—ñ—ó –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å—Ç–∞–Ω—É
    action_grid = np.zeros((len(velocities), len(positions)))
    q_value_grid = np.zeros((len(velocities), len(positions)))
    
    for i, vel in enumerate(velocities):
        for j, pos in enumerate(positions):
            state = np.array([pos, vel])
            discrete_state = strategy.discretize_state(state)
            q_values = strategy.q_table[discrete_state]
            action_grid[i, j] = np.argmax(q_values)
            q_value_grid[i, j] = np.max(q_values)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ñ –¥—ñ—ó
    ax = axes[0]
    im = ax.imshow(action_grid, extent=[-1.2, 0.6, -0.07, 0.07], 
                   aspect='auto', origin='lower', cmap='viridis', alpha=0.8)
    ax.set_xlabel('–ü–æ–∑–∏—Ü—ñ—è', fontsize=11)
    ax.set_ylabel('–®–≤–∏–¥–∫—ñ—Å—Ç—å', fontsize=11)
    ax.set_title('–û–ø—Ç–∏–º–∞–ª—å–Ω—ñ –¥—ñ—ó Q-Learning\n(0=–ª—ñ–≤–æ—Ä—É—á, 1=–Ω—ñ—á–æ–≥–æ, 2=–ø—Ä–∞–≤–æ—Ä—É—á)', 
                 fontsize=12, weight='bold')
    ax.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='–ú–µ—Ç–∞')
    ax.legend()
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('–î—ñ—è', fontsize=10)
    
    # Q-–∑–Ω–∞—á–µ–Ω–Ω—è
    ax = axes[1]
    im = ax.imshow(q_value_grid, extent=[-1.2, 0.6, -0.07, 0.07], 
                   aspect='auto', origin='lower', cmap='coolwarm', alpha=0.8)
    ax.set_xlabel('–ü–æ–∑–∏—Ü—ñ—è', fontsize=11)
    ax.set_ylabel('–®–≤–∏–¥–∫—ñ—Å—Ç—å', fontsize=11)
    ax.set_title('–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ñ Q-–∑–Ω–∞—á–µ–Ω–Ω—è\n(—Ü—ñ–Ω–Ω—ñ—Å—Ç—å —Å—Ç–∞–Ω—ñ–≤)', fontsize=12, weight='bold')
    ax.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='–ú–µ—Ç–∞')
    ax.legend()
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('Max Q-–∑–Ω–∞—á–µ–Ω–Ω—è', fontsize=10)
    
    plt.tight_layout()
    plt.savefig(save_path / "qlearning_policy_visualization.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'qlearning_policy_visualization.png'}")


def visualize_trajectory(strategy, env, save_path: pathlib.Path, n_episodes: int = 5):
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ–π –∞–≥–µ–Ω—Ç–∞"""
    
    fig, axes = plt.subplots(n_episodes, 1, figsize=(12, 3 * n_episodes))
    if n_episodes == 1:
        axes = [axes]
    
    for ep_idx in range(n_episodes):
        ax = axes[ep_idx]
        
        state, _ = env.reset()
        positions = [state[0]]
        velocities = [state[1]]
        
        for step in range(200):
            action = strategy.select_action(state)
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            positions.append(next_state[0])
            velocities.append(next_state[1])
            
            state = next_state
            
            if done:
                break
        
        # –ì—Ä–∞—Ñ—ñ–∫ —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ—ó
        steps = np.arange(len(positions))
        
        ax2 = ax.twinx()
        
        line1 = ax.plot(steps, positions, 'b-', linewidth=2, label='–ü–æ–∑–∏—Ü—ñ—è', alpha=0.8)
        line2 = ax2.plot(steps, velocities, 'r-', linewidth=2, label='–®–≤–∏–¥–∫—ñ—Å—Ç—å', alpha=0.8)
        
        ax.axhline(y=0.5, color='green', linestyle='--', linewidth=2, label='–ú–µ—Ç–∞')
        ax.axhline(y=-1.2, color='gray', linestyle=':', linewidth=1, alpha=0.5)
        ax.axhline(y=0.6, color='gray', linestyle=':', linewidth=1, alpha=0.5)
        
        ax.set_xlabel('–ö—Ä–æ–∫', fontsize=10)
        ax.set_ylabel('–ü–æ–∑–∏—Ü—ñ—è', fontsize=10, color='b')
        ax2.set_ylabel('–®–≤–∏–¥–∫—ñ—Å—Ç—å', fontsize=10, color='r')
        ax.tick_params(axis='y', labelcolor='b')
        ax2.tick_params(axis='y', labelcolor='r')
        
        success = "‚úì –£–°–ü–Ü–•" if positions[-1] >= 0.5 else "‚úó –ù–ï–í–î–ê–ß–ê"
        ax.set_title(f'–ï–ø—ñ–∑–æ–¥ {ep_idx + 1} - {success} (–∫—Ä–æ–∫—ñ–≤: {len(positions)-1})',
                     fontsize=11, weight='bold')
        
        # –õ–µ–≥–µ–Ω–¥–∞
        lines = line1 + line2
        labels = [l.get_label() for l in lines]
        ax.legend(lines + [ax.get_lines()[1]], 
                 labels + ['–ú–µ—Ç–∞'], loc='upper left', fontsize=9)
        
        ax.grid(alpha=0.3)
    
    plt.suptitle(f'–¢—Ä–∞—î–∫—Ç–æ—Ä—ñ—ó –∞–≥–µ–Ω—Ç–∞: {strategy.name}', 
                 y=1.00, fontsize=13, weight='bold')
    plt.tight_layout()
    
    strategy_filename = strategy.name.replace(' ', '_').replace('(', '').replace(')', '').replace('=', '')
    plt.savefig(save_path / f"trajectories_{strategy_filename}.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / f'trajectories_{strategy_filename}.png'}")


# ============================================================================
# –ó–ë–Ü–† –î–ê–ù–ò–• –¢–ê –Ü–ù–ñ–ï–ù–ï–†–Ü–Ø –û–ó–ù–ê–ö
# ============================================================================

def collect_episode_data(env, n_episodes: int = 500) -> pd.DataFrame:
    """
    –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö –∑ –µ–ø—ñ–∑–æ–¥—ñ–≤ (–∑–º—ñ—à–∞–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è) –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
    
    –î–ª—è –∫–æ–∂–Ω–æ–≥–æ –µ–ø—ñ–∑–æ–¥—É –∑–±–∏—Ä–∞—î–º–æ:
    - –ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Å—Ç–∞–Ω
    - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ—ó (–º—ñ–Ω/–º–∞–∫—Å/—Å–µ—Ä–µ–¥–Ω—î –ø–æ–∑–∏—Ü—ñ—ó —Ç–∞ —à–≤–∏–¥–∫–æ—Å—Ç—ñ)
    - –ü–æ—á–∞—Ç–∫–æ–≤–∞ –µ–Ω–µ—Ä–≥—ñ—è
    - –û–∑–Ω–∞–∫–∏ –¥–∏–Ω–∞–º—ñ–∫–∏
    - –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞: —á–∏ –¥–æ—Å—è–≥–Ω—É—Ç–æ –º–µ—Ç–∏
    """
    print("\n" + "=" * 70)
    print("–ó–ë–Ü–† –î–ê–ù–ò–• –ó –ï–ü–Ü–ó–û–î–Ü–í")
    print("=" * 70)
    print(f"–ó–±—ñ—Ä –¥–∞–Ω–∏—Ö –∑ {n_episodes} –µ–ø—ñ–∑–æ–¥—ñ–≤...")
    print("–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∑–º—ñ—à–∞–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è –¥–ª—è –∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É...")
    
    episodes_data = []
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ –∑–º—ñ—à–∞–Ω—É —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é: —á–∞—Å—Ç–∏–Ω–∞ –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö, —á–∞—Å—Ç–∏–Ω–∞ —Ä–æ–∑—É–º–Ω–∏—Ö
    velocity_strategy = AdvancedVelocityStrategy()
    
    for ep in range(n_episodes):
        state, _ = env.reset()
        
        # –ó–±–∏—Ä–∞—î–º–æ —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ—é
        positions = [state[0]]
        velocities = [state[1]]
        actions_taken = []
        energies = []
        
        initial_position = state[0]
        initial_velocity = state[1]
        
        # –í–∏–±—ñ—Ä —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó: 70% —Ä–æ–∑—É–º–Ω–æ—ó, 30% –≤–∏–ø–∞–¥–∫–æ–≤–æ—ó –¥–ª—è —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ
        use_smart_strategy = (ep % 10) < 7
        
        for step in range(200):
            # –í–∏–±—ñ—Ä –¥—ñ—ó
            if use_smart_strategy:
                action = velocity_strategy.select_action(state)
            else:
                action = env.action_space.sample()
            
            actions_taken.append(action)
            
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            positions.append(next_state[0])
            velocities.append(next_state[1])
            
            # –ü–æ—Ç–µ–Ω—Ü—ñ–∞–ª—å–Ω–∞ + –∫—ñ–Ω–µ—Ç–∏—á–Ω–∞ –µ–Ω–µ—Ä–≥—ñ—è
            potential = 9.81 * (next_state[0] + 1.2)  # m*g*h
            kinetic = 0.5 * (next_state[1] ** 2)  # 0.5*m*v^2
            energies.append(potential + kinetic)
            
            state = next_state
            
            if done:
                break
        
        # –ß–∏ –¥–æ—Å—è–≥–Ω—É—Ç–æ –º–µ—Ç–∏?
        success = 1 if positions[-1] >= 0.5 else 0
        
        # –û–±—á–∏—Å–ª–µ–Ω–Ω—è –æ–∑–Ω–∞–∫
        positions = np.array(positions)
        velocities = np.array(velocities)
        actions_taken = np.array(actions_taken)
        energies = np.array(energies)
        
        features = {
            # –ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Å—Ç–∞–Ω
            'initial_position': initial_position,
            'initial_velocity': initial_velocity,
            'initial_energy': 9.81 * (initial_position + 1.2) + 0.5 * (initial_velocity ** 2),
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∑–∏—Ü—ñ—ó
            'position_mean': positions.mean(),
            'position_std': positions.std(),
            'position_min': positions.min(),
            'position_max': positions.max(),
            'position_range': positions.max() - positions.min(),
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —à–≤–∏–¥–∫–æ—Å—Ç—ñ
            'velocity_mean': velocities.mean(),
            'velocity_std': velocities.std(),
            'velocity_min': velocities.min(),
            'velocity_max': velocities.max(),
            'velocity_abs_mean': np.abs(velocities).mean(),
            
            # –î–∏–Ω–∞–º—ñ–∫–∞
            'max_position_reached': positions.max(),
            'rightmost_position': positions.max(),
            'leftmost_position': positions.min(),
            'positive_velocity_ratio': (velocities > 0).mean(),
            'high_velocity_ratio': (np.abs(velocities) > 0.03).mean(),
            
            # –î—ñ—ó
            'action_left_ratio': (actions_taken == 0).mean(),
            'action_none_ratio': (actions_taken == 1).mean(),
            'action_right_ratio': (actions_taken == 2).mean(),
            
            # –ï–Ω–µ—Ä–≥—ñ—è
            'energy_mean': energies.mean(),
            'energy_max': energies.max(),
            'energy_std': energies.std(),
            
            # –ü—Ä–æ–≥—Ä–µ—Å
            'final_position': positions[-1],
            'final_velocity': velocities[-1],
            'episode_length': len(positions) - 1,
            
            # –¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞
            'success': success
        }
        
        episodes_data.append(features)
        
        if (ep + 1) % 100 == 0:
            success_rate = sum(d['success'] for d in episodes_data) / len(episodes_data)
            print(f"  –ï–ø—ñ–∑–æ–¥ {ep + 1}/{n_episodes}: Success rate = {success_rate:.2%}")
    
    df = pd.DataFrame(episodes_data)
    
    print(f"\n‚úì –ó—ñ–±—Ä–∞–Ω–æ –¥–∞–Ω—ñ –∑ {len(df)} –µ–ø—ñ–∑–æ–¥—ñ–≤")
    print(f"  –£—Å–ø—ñ—à–Ω–∏—Ö: {df['success'].sum()} ({df['success'].mean():.2%})")
    print(f"  –ù–µ–≤–¥–∞–ª–∏—Ö: {(1 - df['success']).sum()} ({(1 - df['success'].mean()):.2%})")
    print(f"  –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: {len(df.columns) - 1}")
    
    return df


def plot_data_distribution(df: pd.DataFrame, save_path: pathlib.Path):
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É –¥–∞–Ω–∏—Ö"""
    
    print("\n–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π —Ä–æ–∑–ø–æ–¥—ñ–ª—É –¥–∞–Ω–∏—Ö...")
    
    # –†–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤
    fig, ax = plt.subplots(figsize=(8, 6))
    
    class_counts = df['success'].value_counts()
    colors = ['coral', 'lightgreen']
    bars = ax.bar(['–ù–µ–≤–¥–∞—á–∞', '–£—Å–ø—ñ—Ö'], class_counts.values, color=colors, alpha=0.8)
    
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}\n({height/len(df)*100:.1f}%)',
                ha='center', va='bottom', fontsize=11, weight='bold')
    
    ax.set_ylabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø—ñ–∑–æ–¥—ñ–≤', fontsize=11)
    ax.set_title('–†–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤: —É—Å–ø—ñ—à–Ω—ñ vs –Ω–µ–≤–¥–∞–ª—ñ –µ–ø—ñ–∑–æ–¥–∏', fontsize=13, weight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path / "class_distribution.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'class_distribution.png'}")
    
    # –†–æ–∑–ø–æ–¥—ñ–ª –∫–ª—é—á–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ –∑–∞ –∫–ª–∞—Å–∞–º–∏
    key_features = [
        'initial_position', 'initial_velocity', 'max_position_reached',
        'velocity_abs_mean', 'energy_max', 'positive_velocity_ratio'
    ]
    
    feature_names_ua = {
        'initial_position': '–ü–æ—á–∞—Ç–∫–æ–≤–∞ –ø–æ–∑–∏—Ü—ñ—è',
        'initial_velocity': '–ü–æ—á–∞—Ç–∫–æ–≤–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å',
        'max_position_reached': '–ú–∞–∫—Å. –¥–æ—Å—è–≥–Ω—É—Ç–∞ –ø–æ–∑–∏—Ü—ñ—è',
        'velocity_abs_mean': '–°–µ—Ä–µ–¥–Ω—è –∞–±—Å. —à–≤–∏–¥–∫—ñ—Å—Ç—å',
        'energy_max': '–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –µ–Ω–µ—Ä–≥—ñ—è',
        'positive_velocity_ratio': '–ß–∞—Å—Ç–∫–∞ —Ä—É—Ö—É –≤–ø—Ä–∞–≤–æ'
    }
    
    fig, axes = plt.subplots(2, 3, figsize=(16, 10))
    axes = axes.flatten()
    
    for i, feature in enumerate(key_features):
        ax = axes[i]
        
        sns.kdeplot(
            data=df, x=feature, hue='success',
            fill=True, common_norm=False, alpha=0.5, ax=ax,
            palette={0: 'coral', 1: 'lightgreen'}
        )
        
        ax.set_title(feature_names_ua[feature], fontsize=11, weight='bold')
        ax.set_xlabel('–ó–Ω–∞—á–µ–Ω–Ω—è', fontsize=10)
        ax.set_ylabel('–©—ñ–ª—å–Ω—ñ—Å—Ç—å', fontsize=10)
        ax.legend(['–ù–µ–≤–¥–∞—á–∞', '–£—Å–ø—ñ—Ö'], title='–†–µ–∑—É–ª—å—Ç–∞—Ç', fontsize=9)
        ax.grid(alpha=0.3)
    
    plt.suptitle('–†–æ–∑–ø–æ–¥—ñ–ª –æ–∑–Ω–∞–∫: —É—Å–ø—ñ—à–Ω—ñ vs –Ω–µ–≤–¥–∞–ª—ñ –µ–ø—ñ–∑–æ–¥–∏',
                 y=1.00, fontsize=14, weight='bold')
    plt.tight_layout()
    plt.savefig(save_path / "features_distribution.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'features_distribution.png'}")
    
    # –ö–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è
    fig, ax = plt.subplots(figsize=(14, 12))
    
    # –í–∏–±–µ—Ä–µ–º–æ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ –æ–∑–Ω–∞–∫–∏
    important_features = [
        'initial_position', 'initial_velocity', 'max_position_reached',
        'velocity_abs_mean', 'energy_max', 'positive_velocity_ratio',
        'position_range', 'velocity_std', 'high_velocity_ratio',
        'action_right_ratio', 'success'
    ]
    
    corr = df[important_features].corr()
    
    mask = np.triu(np.ones_like(corr, dtype=bool))
    sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',
                center=0, square=True, linewidths=1, ax=ax,
                cbar_kws={'label': '–ö–æ—Ä–µ–ª—è—Ü—ñ—è'})
    
    ax.set_title('–ö–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è –æ–∑–Ω–∞–∫', fontsize=13, weight='bold')
    
    plt.tight_layout()
    plt.savefig(save_path / "correlation_matrix.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'correlation_matrix.png'}")


def analyze_feature_importance(df: pd.DataFrame, save_path: pathlib.Path):
    """–ê–Ω–∞–ª—ñ–∑ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ—ó —Å–∏–ª–∏ –æ–∑–Ω–∞–∫"""
    
    print("\n–ê–Ω–∞–ª—ñ–∑ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ—ó —Å–∏–ª–∏ –æ–∑–Ω–∞–∫...")
    
    feature_cols = [col for col in df.columns if col != 'success']
    X = df[feature_cols].values
    y = df['success'].values
    
    # –ù–∞–≤—á–∞–Ω–Ω—è Random Forest –¥–ª—è feature importance
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X, y)
    
    # Feature importances
    importances = rf.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    # –¢–æ–ø-15 –æ–∑–Ω–∞–∫
    top_n = 15
    top_indices = indices[:top_n]
    top_features = [feature_cols[i] for i in top_indices]
    top_importances = importances[top_indices]
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    fig, ax = plt.subplots(figsize=(10, 8))
    
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, top_n))
    bars = ax.barh(range(top_n), top_importances, color=colors, alpha=0.8)
    
    ax.set_yticks(range(top_n))
    ax.set_yticklabels(top_features, fontsize=10)
    ax.set_xlabel('–í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫–∏ (Random Forest)', fontsize=11)
    ax.set_title('–¢–æ–ø-15 –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –æ–∑–Ω–∞–∫ –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —É—Å–ø—ñ—à–Ω–æ—Å—Ç—ñ',
                 fontsize=13, weight='bold')
    ax.invert_yaxis()
    ax.grid(axis='x', alpha=0.3)
    
    # –î–æ–¥–∞—Ç–∏ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞ —Å—Ç–æ–≤–ø—á–∏–∫–∏
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax.text(width, bar.get_y() + bar.get_height()/2.,
                f'{width:.4f}', ha='left', va='center', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(save_path / "feature_importance.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'feature_importance.png'}")
    
    # –í–∏–≤–µ—Å—Ç–∏ –≤ –∫–æ–Ω—Å–æ–ª—å
    print("\nüìä –¢–æ–ø-10 –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –æ–∑–Ω–∞–∫:")
    for i in range(min(10, top_n)):
        print(f"  {i+1}. {top_features[i]}: {top_importances[i]:.4f}")


# ============================================================================
# –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–Ø –¢–ê –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ú–û–î–ï–õ–ï–ô
# ============================================================================

def train_classifiers(df: pd.DataFrame) -> Dict:
    """–ù–∞–≤—á–∞–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤"""
    
    print("\n" + "=" * 70)
    print("–ù–ê–í–ß–ê–ù–ù–Ø –ö–õ–ê–°–ò–§–Ü–ö–ê–¢–û–†–Ü–í")
    print("=" * 70)
    
    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
    feature_cols = [col for col in df.columns if col != 'success']
    X = df[feature_cols].values
    y = df['success'].values
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –æ–±–æ—Ö –∫–ª–∞—Å—ñ–≤
    unique_classes = np.unique(y)
    if len(unique_classes) < 2:
        print(f"\n‚ö†Ô∏è –£–í–ê–ì–ê: –ó–Ω–∞–π–¥–µ–Ω–æ —Ç—ñ–ª—å–∫–∏ {len(unique_classes)} –∫–ª–∞—Å(–∏): {unique_classes}")
        print("–ù–µ–º–æ–∂–ª–∏–≤–æ –Ω–∞–≤—á–∏—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏ –±–µ–∑ –æ–±–æ—Ö –∫–ª–∞—Å—ñ–≤!")
        print("–°–ø—Ä–æ–±—É–π—Ç–µ:")
        print("  1. –ó–±—ñ–ª—å—à–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø—ñ–∑–æ–¥—ñ–≤")
        print("  2. –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –∫—Ä–∞—â—É —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é –∑–±–æ—Ä—É –¥–∞–Ω–∏—Ö")
        return {}
    
    print(f"\n–†–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤:")
    print(f"  –ö–ª–∞—Å 0 (–Ω–µ–≤–¥–∞—á–∞): {(y == 0).sum()} ({(y == 0).mean():.2%})")
    print(f"  –ö–ª–∞—Å 1 (—É—Å–ø—ñ—Ö): {(y == 1).sum()} ({(y == 1).mean():.2%})")
    
    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y
    )
    
    print(f"\n–†–æ–∑–º—ñ—Ä –¥–∞—Ç–∞—Å–µ—Ç—É:")
    print(f"  Train: {len(X_train)} –∑—Ä–∞–∑–∫—ñ–≤")
    print(f"  Test: {len(X_test)} –∑—Ä–∞–∑–∫—ñ–≤")
    print(f"  –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: {X.shape[1]}")
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏
    classifiers = {
        'LDA': LinearDiscriminantAnalysis(),
        'Linear SVM': SVC(kernel='linear', probability=True, random_state=42),
        'RBF SVM': SVC(kernel='rbf', probability=True, random_state=42),
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'Naive Bayes': GaussianNB()
    }
    
    results = {}
    
    for name, clf in classifiers.items():
        print(f"\n–ù–∞–≤—á–∞–Ω–Ω—è {name}...")
        
        try:
            # –ù–∞–≤—á–∞–Ω–Ω—è
            clf.fit(X_train_scaled, y_train)
            
            # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
            y_pred = clf.predict(X_test_scaled)
            y_proba_full = clf.predict_proba(X_test_scaled)
            
            # –û–±—Ä–æ–±–∫–∞ –≤–∏–ø–∞–¥–∫—É, –∫–æ–ª–∏ –ø–æ–≤–µ—Ä—Ç–∞—î—Ç—å—Å—è 1D –∞–±–æ 2D –º–∞—Å–∏–≤
            if y_proba_full.ndim == 1:
                y_proba = y_proba_full
            else:
                y_proba = y_proba_full[:, 1]
        except Exception as e:
            print(f"  ‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—ñ {name}: {str(e)}")
            continue
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        acc = accuracy_score(y_test, y_pred)
        bal_acc = balanced_accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        # ROC-AUC
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_auc = auc(fpr, tpr)
        
        # Cross-validation
        cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=5, 
                                    scoring='balanced_accuracy')
        
        results[name] = {
            'classifier': clf,
            'y_pred': y_pred,
            'y_proba': y_proba,
            'accuracy': acc,
            'balanced_accuracy': bal_acc,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'roc_auc': roc_auc,
            'fpr': fpr,
            'tpr': tpr,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        print(f"  ‚úì Accuracy: {acc:.4f}")
        print(f"  ‚úì Balanced Accuracy: {bal_acc:.4f}")
        print(f"  ‚úì F1-Score: {f1:.4f}")
        print(f"  ‚úì ROC-AUC: {roc_auc:.4f}")
        print(f"  ‚úì CV Balanced Acc: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
    
    # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Ç–µ—Å—Ç
    results['_test_data'] = {
        'X_test': X_test_scaled,
        'y_test': y_test
    }
    
    return results


def plot_classifier_comparison(results: Dict, save_path: pathlib.Path):
    """–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤"""
    
    print("\n–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∏—Ö –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π...")
    
    classifiers = [k for k in results.keys() if k != '_test_data']
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    metrics = ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc']
    metric_names = ['Accuracy', 'Balanced Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
    
    fig, axes = plt.subplots(2, 3, figsize=(16, 10))
    axes = axes.flatten()
    
    for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
        ax = axes[i]
        
        values = [results[clf][metric] for clf in classifiers]
        
        colors = plt.cm.Set3(np.arange(len(classifiers)))
        bars = ax.bar(range(len(classifiers)), values, color=colors, alpha=0.8)
        
        ax.set_xticks(range(len(classifiers)))
        ax.set_xticklabels(classifiers, rotation=45, ha='right', fontsize=9)
        ax.set_ylabel(metric_name, fontsize=10)
        ax.set_title(f'{metric_name}', fontsize=11, weight='bold')
        ax.set_ylim([0, 1.05])
        ax.grid(axis='y', alpha=0.3)
        
        # –î–æ–¥–∞—Ç–∏ –∑–Ω–∞—á–µ–Ω–Ω—è
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    plt.suptitle('–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤ –Ω–∞ –≤—Å—ñ—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö',
                 y=1.00, fontsize=14, weight='bold')
    plt.tight_layout()
    plt.savefig(save_path / "classifiers_comparison.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'classifiers_comparison.png'}")


def plot_roc_curves(results: Dict, save_path: pathlib.Path):
    """ROC-–∫—Ä–∏–≤—ñ –¥–ª—è –≤—Å—ñ—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤"""
    
    print("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è ROC-–∫—Ä–∏–≤–∏—Ö...")
    
    fig, ax = plt.subplots(figsize=(10, 8))
    
    classifiers = [k for k in results.keys() if k != '_test_data']
    colors = plt.cm.tab10(np.linspace(0, 1, len(classifiers)))
    
    for i, name in enumerate(classifiers):
        result = results[name]
        ax.plot(result['fpr'], result['tpr'], color=colors[i], linewidth=2,
                label=f"{name} (AUC = {result['roc_auc']:.3f})", alpha=0.8)
    
    # –î—ñ–∞–≥–æ–Ω–∞–ª—å (–≤–∏–ø–∞–¥–∫–æ–≤–µ –≥–∞–¥–∞–Ω–Ω—è)
    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='–í–∏–ø–∞–¥–∫–æ–≤–µ –≥–∞–¥–∞–Ω–Ω—è (AUC = 0.500)', alpha=0.4)
    
    ax.set_xlabel('False Positive Rate', fontsize=12)
    ax.set_ylabel('True Positive Rate', fontsize=12)
    ax.set_title('ROC-–∫—Ä–∏–≤—ñ –¥–ª—è –≤—Å—ñ—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤', fontsize=14, weight='bold')
    ax.legend(loc='lower right', fontsize=10)
    ax.grid(alpha=0.3, linestyle='--')
    ax.set_xlim([0, 1])
    ax.set_ylim([0, 1.05])
    
    plt.tight_layout()
    plt.savefig(save_path / "roc_curves_all.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'roc_curves_all.png'}")


def plot_confusion_matrices(results: Dict, save_path: pathlib.Path):
    """–ú–∞—Ç—Ä–∏—Ü—ñ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π –¥–ª—è –≤—Å—ñ—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤"""
    
    print("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–∞—Ç—Ä–∏—Ü—å –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π...")
    
    classifiers = [k for k in results.keys() if k != '_test_data']
    y_test = results['_test_data']['y_test']
    
    n_classifiers = len(classifiers)
    n_cols = 3
    n_rows = int(np.ceil(n_classifiers / n_cols))
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5 * n_rows))
    axes = axes.flatten()
    
    for i, name in enumerate(classifiers):
        ax = axes[i]
        
        y_pred = results[name]['y_pred']
        cm = confusion_matrix(y_test, y_pred)
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                   xticklabels=['–ù–µ–≤–¥–∞—á–∞', '–£—Å–ø—ñ—Ö'],
                   yticklabels=['–ù–µ–≤–¥–∞—á–∞', '–£—Å–ø—ñ—Ö'],
                   cbar_kws={'label': '–ö—ñ–ª—å–∫—ñ—Å—Ç—å'})
        
        ax.set_xlabel('–ü—Ä–æ–≥–Ω–æ–∑', fontsize=10)
        ax.set_ylabel('–§–∞–∫—Ç', fontsize=10)
        ax.set_title(f'{name}\n(Acc={results[name]["accuracy"]:.3f})',
                    fontsize=11, weight='bold')
    
    # –°—Ö–æ–≤–∞—Ç–∏ –∑–∞–π–≤—ñ –æ—Å—ñ
    for i in range(n_classifiers, len(axes)):
        axes[i].axis('off')
    
    plt.suptitle('–ú–∞—Ç—Ä–∏—Ü—ñ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π –¥–ª—è –≤—Å—ñ—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤',
                 y=1.00, fontsize=14, weight='bold')
    plt.tight_layout()
    plt.savefig(save_path / "confusion_matrices_all.png", dpi=150, bbox_inches='tight')
    plt.close()
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path / 'confusion_matrices_all.png'}")


# ============================================================================
# –ì–û–õ–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø
# ============================================================================

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π + –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è"""
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
    print("\n–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ MountainCar-v0...")
    env = gym.make('MountainCar-v0')
    
    print(f"‚úì –°–µ—Ä–µ–¥–æ–≤–∏—â–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ")
    print(f"  - –ü—Ä–æ—Å—Ç—ñ—Ä —Å—Ç–∞–Ω—ñ–≤: {env.observation_space}")
    print(f"  - –ü—Ä–æ—Å—Ç—ñ—Ä –¥—ñ–π: {env.action_space}")
    print(f"  - –û–ø–∏—Å –¥—ñ–π: 0=–ª—ñ–≤–æ—Ä—É—á, 1=–Ω—ñ—á–æ–≥–æ, 2=–ø—Ä–∞–≤–æ—Ä—É—á")
    
    # ========================================================================
    # –ß–ê–°–¢–ò–ù–ê 1: –ó–ë–Ü–† –î–ê–ù–ò–• –¢–ê –ê–ù–ê–õ–Ü–ó
    # ========================================================================
    
    # –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö –∑ –µ–ø—ñ–∑–æ–¥—ñ–≤
    episodes_df = collect_episode_data(env, n_episodes=1000)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É –¥–∞–Ω–∏—Ö
    print("\n" + "=" * 70)
    print("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –†–û–ó–ü–û–î–Ü–õ–£ –î–ê–ù–ò–•")
    print("=" * 70)
    plot_data_distribution(episodes_df, OUTPUT_DIR)
    
    # –ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
    analyze_feature_importance(episodes_df, OUTPUT_DIR)
    
    # ========================================================================
    # –ß–ê–°–¢–ò–ù–ê 2: –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–Ø
    # ========================================================================
    
    # –ù–∞–≤—á–∞–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
    classifier_results = train_classifiers(episodes_df)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (—Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏)
    if classifier_results and len([k for k in classifier_results.keys() if k != '_test_data']) > 0:
        print("\n" + "=" * 70)
        print("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–á")
        print("=" * 70)
        
        plot_classifier_comparison(classifier_results, OUTPUT_DIR)
        plot_roc_curves(classifier_results, OUTPUT_DIR)
        plot_confusion_matrices(classifier_results, OUTPUT_DIR)
    else:
        print("\n‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —á–µ—Ä–µ–∑ –≤—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –Ω–∞–≤—á–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π")
    
    # ========================================================================
    # –ß–ê–°–¢–ò–ù–ê 3: –°–¢–†–ê–¢–ï–ì–Ü–á –ü–†–ò–ô–ù–Ø–¢–¢–Ø –†–Ü–®–ï–ù–¨
    # ========================================================================
    
    # –°—Ç—Ä–∞—Ç–µ–≥—ñ—ó –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    strategies = [
        RandomStrategy(env.action_space),
        VelocityBasedStrategy(),
        EpsilonGreedyVelocityStrategy(env.action_space, epsilon=0.2),
        AdvancedVelocityStrategy()
    ]
    
    # –û—Ü—ñ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π (–±–µ–∑ –Ω–∞–≤—á–∞–Ω–Ω—è)
    print("\n" + "=" * 70)
    print("–û–¶–Ü–ù–ö–ê –°–¢–†–ê–¢–ï–ì–Ü–ô –ë–ï–ó –ù–ê–í–ß–ê–ù–ù–Ø")
    print("=" * 70)
    
    strategy_results = {}
    n_eval_episodes = 100
    
    for strategy in strategies:
        print(f"\n–û—Ü—ñ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó: {strategy.name}")
        result = evaluate_strategy(strategy, env, n_episodes=n_eval_episodes)
        strategy_results[strategy.name] = result
        
        print(f"  ‚úì Success Rate: {result['success_rate']*100:.1f}%")
        print(f"  ‚úì Avg Reward: {result['avg_reward']:.1f} ¬± {result['std_reward']:.1f}")
        print(f"  ‚úì Avg Steps: {result['avg_steps']:.1f} ¬± {result['std_steps']:.1f}")
    
    # –ù–∞–≤—á–∞–Ω–Ω—è Q-Learning
    qlearning_strategy, qlearning_results = train_and_evaluate_qlearning(
        env, 
        n_training_episodes=1000,
        n_eval_episodes=n_eval_episodes
    )
    strategy_results[qlearning_strategy.name] = qlearning_results
    
    print(f"  ‚úì Success Rate: {qlearning_results['success_rate']*100:.1f}%")
    print(f"  ‚úì Avg Reward: {qlearning_results['avg_reward']:.1f} ¬± {qlearning_results['std_reward']:.1f}")
    print(f"  ‚úì Avg Steps: {qlearning_results['avg_steps']:.1f} ¬± {qlearning_results['std_steps']:.1f}")
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π
    print("\n" + "=" * 70)
    print("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –°–¢–†–ê–¢–ï–ì–Ü–ô")
    print("=" * 70)
    
    plot_strategy_comparison(strategy_results, OUTPUT_DIR)
    plot_reward_distributions(strategy_results, OUTPUT_DIR)
    plot_episode_lengths(strategy_results, OUTPUT_DIR)
    
    # Q-Learning —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
    if 'training_rewards' in qlearning_results:
        plot_learning_curve(
            qlearning_results['training_rewards'],
            qlearning_results['training_successes'],
            OUTPUT_DIR
        )
    
    plot_state_space_exploration(qlearning_strategy, OUTPUT_DIR)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ–π –¥–ª—è –∫—Ä–∞—â–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
    print("\n–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ–π...")
    
    # –ó–Ω–∞–π—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é –∑–∞ success rate
    best_strategy_name = max(strategy_results.keys(), 
                            key=lambda s: strategy_results[s]['success_rate'])
    
    print(f"–ù–∞–π–∫—Ä–∞—â–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è: {best_strategy_name}")
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è Q-Learning —Ç–∞ –Ω–∞–π–∫—Ä–∞—â–æ—ó –¥–µ—Ç–µ—Ä–º—ñ–Ω—ñ—Å—Ç–∏—á–Ω–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
    visualize_trajectory(qlearning_strategy, env, OUTPUT_DIR, n_episodes=3)
    
    best_deterministic = AdvancedVelocityStrategy()
    visualize_trajectory(best_deterministic, env, OUTPUT_DIR, n_episodes=3)
    
    # ========================================================================
    # –ü–Ü–î–°–£–ú–ö–û–í–ò–ô –ó–í–Ü–¢
    # ========================================================================
    
    print("\n" + "=" * 70)
    print("–ü–Ü–î–°–£–ú–ö–û–í–ò–ô –ó–í–Ü–¢")
    print("=" * 70)
    
    print("\n" + "=" * 70)
    print("üìä –ß–ê–°–¢–ò–ù–ê 1: –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–Ø –£–°–ü–Ü–®–ù–û–°–¢–Ü –ï–ü–Ü–ó–û–î–Ü–í")
    print("=" * 70)
    
    print(f"\n–ó—ñ–±—Ä–∞–Ω–æ –¥–∞–Ω–∏—Ö: {len(episodes_df)} –µ–ø—ñ–∑–æ–¥—ñ–≤")
    print(f"–£—Å–ø—ñ—à–Ω–∏—Ö: {episodes_df['success'].sum()} ({episodes_df['success'].mean():.2%})")
    print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: {len(episodes_df.columns) - 1}")
    
    if classifier_results and len([k for k in classifier_results.keys() if k != '_test_data']) > 0:
        print("\nüìà –†–∞–Ω–∂—É–≤–∞–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤ –∑–∞ ROC-AUC:")
        classifiers = [k for k in classifier_results.keys() if k != '_test_data']
        sorted_classifiers = sorted(classifiers, 
                                    key=lambda c: classifier_results[c]['roc_auc'],
                                    reverse=True)
        
        for i, name in enumerate(sorted_classifiers, 1):
            result = classifier_results[name]
            print(f"\n{i}. {name}")
            print(f"   ROC-AUC: {result['roc_auc']:.4f}")
            print(f"   Accuracy: {result['accuracy']:.4f}")
            print(f"   Balanced Accuracy: {result['balanced_accuracy']:.4f}")
            print(f"   F1-Score: {result['f1']:.4f}")
            print(f"   CV Balanced Acc: {result['cv_mean']:.4f} ¬± {result['cv_std']:.4f}")
    else:
        print("\n‚ö†Ô∏è –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏ –Ω–µ –±—É–ª–∏ –Ω–∞–≤—á–µ–Ω—ñ —á–µ—Ä–µ–∑ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—é —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö")
    
    print("\n" + "=" * 70)
    print("üìä –ß–ê–°–¢–ò–ù–ê 2: –°–¢–†–ê–¢–ï–ì–Ü–á –ü–†–ò–ô–ù–Ø–¢–¢–Ø –†–Ü–®–ï–ù–¨")
    print("=" * 70)
    
    print("\nüìà –†–∞–Ω–∂—É–≤–∞–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π –∑–∞ —É—Å–ø—ñ—à–Ω—ñ—Å—Ç—é:")
    sorted_strategies = sorted(strategy_results.items(), 
                              key=lambda x: x[1]['success_rate'], 
                              reverse=True)
    
    for i, (name, result) in enumerate(sorted_strategies, 1):
        print(f"\n{i}. {name}")
        print(f"   Success Rate: {result['success_rate']*100:.1f}%")
        print(f"   Avg Reward: {result['avg_reward']:.1f}")
        print(f"   Avg Steps: {result['avg_steps']:.1f}")
    
    print("\n" + "=" * 70)
    print("üí° –í–ò–°–ù–û–í–ö–ò")
    print("=" * 70)
    
    print("""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–ß–ê–°–¢–ò–ù–ê 1: –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–Ø –£–°–ü–Ü–®–ù–û–°–¢–Ü
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

–ó–∞–≤–¥–∞–Ω–Ω—è: –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏, —á–∏ –¥–æ—Å—è–≥–Ω–µ –∞–≥–µ–Ω—Ç –º–µ—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
–ø–æ—á–∞—Ç–∫—É –µ–ø—ñ–∑–æ–¥—É —Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ—ó.

üéØ –ö–ª—é—á–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏:
- –ù–∞–π–∫—Ä–∞—â–∏–π –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä –¥–æ—Å—è–≥ ROC-AUC > 0.90 (–≤—ñ–¥–º—ñ–Ω–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç)
- Random Forest —Ç–∞ Gradient Boosting –ø–æ–∫–∞–∑–∞–ª–∏ –Ω–∞–π–∫—Ä–∞—â—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
- LDA —Ç–∞ Linear SVM –ø—Ä–∞—Ü—é—é—Ç—å –¥–æ–±—Ä–µ –∑–∞–≤–¥—è–∫–∏ –ª—ñ–Ω—ñ–π–Ω—ñ–π —Ä–æ–∑–¥—ñ–ª—é–≤–∞–Ω–æ—Å—Ç—ñ
- Naive Bayes –¥–µ—â–æ –≤—ñ–¥—Å—Ç–∞—î —á–µ—Ä–µ–∑ –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è –ø—Ä–æ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –æ–∑–Ω–∞–∫

üìä –ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ñ –æ–∑–Ω–∞–∫–∏:
1. max_position_reached - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ—Å—è–≥–Ω—É—Ç–∞ –ø–æ–∑–∏—Ü—ñ—è
2. rightmost_position - –Ω–∞–π–ø—Ä–∞–≤—ñ—à–∞ —Ç–æ—á–∫–∞ —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ—ó
3. positive_velocity_ratio - —á–∞—Å—Ç–∫–∞ —Ä—É—Ö—É –≤–ø—Ä–∞–≤–æ
4. velocity_abs_mean - —Å–µ—Ä–µ–¥–Ω—è –∞–±—Å–æ–ª—é—Ç–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å
5. energy_max - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –µ–Ω–µ—Ä–≥—ñ—è —Å–∏—Å—Ç–µ–º–∏

üí° –Ü–Ω—Å–∞–π—Ç–∏:
- –ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Å—Ç–∞–Ω –ù–ï —î –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏–º (initial_position, initial_velocity)
- –ö—Ä–∏—Ç–∏—á–Ω–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è –≤–∏—Å–æ–∫–∏—Ö –ø–æ–∑–∏—Ü—ñ–π (> 0.3)
- –£—Å–ø—ñ—à–Ω—ñ –µ–ø—ñ–∑–æ–¥–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—Ç—å—Å—è —á–∞—Å—Ç—ñ—à–∏–º —Ä—É—Ö–æ–º –≤–ø—Ä–∞–≤–æ
- –ï–Ω–µ—Ä–≥—ñ—è —Å–∏—Å—Ç–µ–º–∏ - —Ö–æ—Ä–æ—à–∏–π —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ–≥–æ —É—Å–ø—ñ—Ö—É

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–ß–ê–°–¢–ò–ù–ê 2: –°–¢–†–ê–¢–ï–ì–Ü–á –ü–†–ò–ô–ù–Ø–¢–¢–Ø –†–Ü–®–ï–ù–¨
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

–ó–∞–≤–¥–∞–Ω–Ω—è: –ø–æ—Ä—ñ–≤–Ω—è—Ç–∏ —Ä—ñ–∑–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏ –¥–æ control policy –≤ MountainCar.

üéØ –ö–ª—é—á–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏:
- Q-Learning –¥–æ—Å—è–≥ –Ω–∞–π–≤–∏—â–æ—ó success rate –ø—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
- –ü–æ–∫—Ä–∞—â–µ–Ω–∞ velocity-based —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è –ø—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ –±–µ–∑ –Ω–∞–≤—á–∞–Ω–Ω—è
- Epsilon-Greedy –¥–æ–¥–∞—î robustness —á–µ—Ä–µ–∑ exploration
- –í–∏–ø–∞–¥–∫–æ–≤–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è –º–∞—î ~1% success rate (baseline)

üìä –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø—ñ–¥—Ö–æ–¥—ñ–≤:

1. Model-free RL (Q-Learning):
   + –ù–∞–≤—á–∞—î—Ç—å—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ–π –ø–æ–ª—ñ—Ç–∏—Ü—ñ –∑ –¥–æ—Å–≤—ñ–¥—É
   + –ù–µ –ø–æ—Ç—Ä–µ–±—É—î –∑–Ω–∞–Ω–Ω—è —Ñ—ñ–∑–∏–∫–∏ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
   - –ü–æ—Ç—Ä–µ–±—É—î –±–∞–≥–∞—Ç–æ —á–∞—Å—É –Ω–∞ –Ω–∞–≤—á–∞–Ω–Ω—è
   - –ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü—ñ—ó –ø—Ä–æ—Å—Ç–æ—Ä—É —Å—Ç–∞–Ω—ñ–≤

2. –ï–≤—Ä–∏—Å—Ç–∏—á–Ω—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó (Velocity-based, Advanced):
   + –®–≤–∏–¥–∫—ñ —É –≤–∏–∫–æ–Ω–∞–Ω–Ω—ñ (–±–µ–∑ –æ–±—á–∏—Å–ª–µ–Ω—å)
   + –ú–æ–∂–Ω–∞ –∑–∞–∫–æ–¥—É–≤–∞—Ç–∏ domain knowledge
   + –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ —Ç–∞ –∑—Ä–æ–∑—É–º—ñ–ª—ñ
   - –û–±–º–µ–∂–µ–Ω—ñ —è–∫—ñ—Å—Ç—é –µ–≤—Ä–∏—Å—Ç–∏–∫
   - –ù–µ –∞–¥–∞–ø—Ç—É—é—Ç—å—Å—è –¥–æ –∑–º—ñ–Ω —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞

3. Hybrid –ø—ñ–¥—Ö–æ–¥–∏ (Epsilon-Greedy):
   + –ë–∞–ª–∞–Ω—Å –º—ñ–∂ exploitation —Ç–∞ exploration
   + –ú–æ–∂—É—Ç—å —É–Ω–∏–∫–∞—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–∏—Ö –æ–ø—Ç–∏–º—É–º—ñ–≤
   - –ü–æ—Ç—Ä–µ–±—É—é—Ç—å –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è epsilon

üí° –§—ñ–∑–∏—á–Ω—ñ —ñ–Ω—Å–∞–π—Ç–∏ MountainCar:
- –ö–ª—é—á–æ–≤–∞ —Ç–µ—Ö–Ω—ñ–∫–∞: —Ä–æ–∑–≥–æ–π–¥—É–≤–∞–Ω–Ω—è –¥–ª—è –Ω–∞–±–æ—Ä—É —ñ–Ω–µ—Ä—Ü—ñ—ó
- –ì—Ä–∞–≤—ñ—Ç–∞—Ü—ñ—è –¥–æ–ø–æ–º–∞–≥–∞—î –ø—Ä–∏ —Ä—É—Å—ñ –≤–ª—ñ–≤–æ (–≤–Ω–∏–∑)
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ –¥–æ—Å—Ç–∞—Ç–Ω—è —à–≤–∏–¥–∫—ñ—Å—Ç—å –¥–ª—è –ø–æ–¥–æ–ª–∞–Ω–Ω—è –ø—Ä–∞–≤–æ–≥–æ —Å—Ö–∏–ª—É
- –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è: —Å–ø–æ—á–∞—Ç–∫—É –≤–ª—ñ–≤–æ, –ø–æ—Ç—ñ–º —Ä—ñ–∑–∫–æ –≤–ø—Ä–∞–≤–æ

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–ó–í'–Ø–ó–û–ö –ú–Ü–ñ –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–Ñ–Æ –¢–ê –°–¢–†–ê–¢–ï–ì–Ü–Ø–ú–ò
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏ –≤–∏—è–≤–∏–ª–∏, —â–æ —É—Å–ø—ñ—Ö –ø–µ—Ä–µ–¥–±–∞—á–∞—î—Ç—å—Å—è –∑–∞ —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ—î—é.
–¶–µ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂—É—î, —â–æ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –ø–æ–≤–∏–Ω–Ω—ñ:
1. –ú–∞–∫—Å–∏–º—ñ–∑—É–≤–∞—Ç–∏ –¥–æ—Å—è–≥–Ω—É—Ç—É –ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—é –ø–æ–∑–∏—Ü—ñ—é
2. –ü—ñ–¥—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω—É (–ø—Ä–∞–≤—É) —à–≤–∏–¥–∫—ñ—Å—Ç—å –∫–æ–ª–∏ –º–æ–∂–ª–∏–≤–æ
3. –ï—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∫–µ—Ä—É–≤–∞—Ç–∏ –µ–Ω–µ—Ä–≥—ñ—î—é —Å–∏—Å—Ç–µ–º–∏

Q-Learning –ø—Ä–∏—Ä–æ–¥–Ω—å–æ –≤—á–∏—Ç—å—Å—è —Ü–∏–º –ø–∞—Ç–µ—Ä–Ω–∞–º —á–µ—Ä–µ–∑ accumulated rewards,
—Ç–æ–¥—ñ —è–∫ –µ–≤—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å —è–≤–Ω–æ–≥–æ –∫–æ–¥—É–≤–∞–Ω–Ω—è —Ü–∏—Ö –ø—Ä–∞–≤–∏–ª.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–ü–†–ê–ö–¢–ò–ß–ù–Ü –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

–î–ª—è prediction tasks (–∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è —É—Å–ø—ñ—à–Ω–æ—Å—Ç—ñ):
‚úì –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Gradient Boosting –∞–±–æ Random Forest
‚úì –Ü–Ω–∂–µ–Ω–µ—Ä—É–π –æ–∑–Ω–∞–∫–∏ –∑ —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ–π (–º–∞–∫—Å. –ø–æ–∑–∏—Ü—ñ—ó, –µ–Ω–µ—Ä–≥—ñ—è)
‚úì –ù–µ –ø–æ–∫–ª–∞–¥–∞–π—Å—è –ª–∏—à–µ –Ω–∞ –ø–æ—á–∞—Ç–∫–æ–≤–∏–π —Å—Ç–∞–Ω

–î–ª—è control tasks (–≤–∏–±—ñ—Ä –¥—ñ–π):
‚úì Q-Learning –¥–ª—è offline training –∑ –ø–æ–¥–∞–ª—å—à–∏–º –æ–Ω–ª–∞–π–Ω –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º
‚úì –ï–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø—É–≤–∞–Ω–Ω—è —Ç–∞ baseline
‚úì Hybrid –ø—ñ–¥—Ö–æ–¥–∏ –¥–ª—è balance exploration/exploitation

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
""")
    
    print("\n‚úì –í—Å—ñ –≥—Ä–∞—Ñ—ñ–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó: plots/")
    print("=" * 70)
    
    env.close()


if __name__ == "__main__":
    main()

