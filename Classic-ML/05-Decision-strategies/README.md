# Decision Strategies для MountainCar

Проект, що досліджує стратегії прийняття рішень та класифікацію успішності епізодів в задачі MountainCar.

## Опис задачі

**MountainCar-v0** - класична задача reinforcement learning, де агент (машинка) повинен дістатись до прапорця на вершині правого схилу. Проблема в тому, що двигуна недостатньо для прямого підйому, тому потрібно використовувати техніку розгойдування.

### Середовище
- **Стан**: `[позиція, швидкість]`
  - Позиція: [-1.2, 0.6]
  - Швидкість: [-0.07, 0.07]
- **Дії**: 
  - 0: штовхнути ліворуч
  - 1: не робити нічого
  - 2: штовхнути праворуч
- **Мета**: досягти позиції >= 0.5

## Структура проекту

Проект складається з двох основних частин:

### 1. Класифікація успішності епізодів

Збір даних з епізодів та навчання класифікаторів для передбачення, чи досягне агент мети.

**Ознаки**:
- Початковий стан (позиція, швидкість, енергія)
- Статистики траєкторії (мін/макс/середнє позиції та швидкості)
- Динамічні характеристики (досягнута позиція, частка руху вправо)
- Розподіл дій
- Енергетичні характеристики

**Класифікатори**:
- Linear Discriminant Analysis (LDA)
- Support Vector Machines (Linear, RBF)
- Logistic Regression
- Random Forest
- Gradient Boosting
- Naive Bayes

**Метрики**:
- Accuracy, Balanced Accuracy
- Precision, Recall, F1-Score
- ROC-AUC з візуалізацією кривих
- Confusion matrices
- Cross-validation scores

### 2. Стратегії прийняття рішень

Порівняння різних підходів до вибору дій:

**Стратегії**:
1. **Random** - випадковий вибір дій (baseline)
2. **Velocity-based** - вибір на основі поточної швидкості
3. **Epsilon-Greedy** - комбінація евристики та exploration
4. **Advanced Velocity** - покращена евристика з урахуванням позиції
5. **Q-Learning** - навчання оптимальної політики через досвід

**Оцінка**:
- Success rate (% досягнення мети)
- Середня нагорода за епізод
- Середня кількість кроків
- Візуалізація траєкторій

## Встановлення

```bash
# Клонування репозиторію
cd 05-Decision-strategies

# Встановлення залежностей
pip install -r requirements.txt
```

## Використання

```bash
# Запуск повного аналізу
python main.py
```

Скрипт виконає:
1. Збір даних з 1000 епізодів
2. Візуалізацію розподілу даних та ознак
3. Навчання 7 класифікаторів
4. Побудову ROC-кривих та матриць неточностей
5. Оцінку 5 стратегій прийняття рішень
6. Навчання Q-Learning агента
7. Візуалізацію траєкторій та політик

Всі графіки зберігаються у директорії `plots/`.

## Результати

### Класифікація

Найкращі класифікатори досягають **ROC-AUC > 0.90**, що вказує на відмінну здатність передбачати успішність епізоду.

**Топ-3 класифікатори**:
1. Gradient Boosting
2. Random Forest  
3. RBF SVM

**Найважливіші ознаки**:
- Максимальна досягнута позиція
- Найправіша точка траєкторії
- Частка руху вправо
- Середня абсолютна швидкість
- Максимальна енергія

### Стратегії

**Найкращі стратегії**:
1. Q-Learning (після навчання) - ~80-90% success rate
2. Advanced Velocity - ~20-40% success rate
3. Epsilon-Greedy - ~10-30% success rate

Q-Learning демонструє найкращі результати, але потребує навчання (1000 епізодів). Евристичні стратегії працюють одразу, але з меншою ефективністю.

## Візуалізації

Проект генерує наступні графіки:

### Аналіз даних
- `class_distribution.png` - розподіл класів
- `features_distribution.png` - розподіл ознак за класами
- `correlation_matrix.png` - кореляційна матриця
- `feature_importance.png` - важливість ознак

### Класифікація
- `classifiers_comparison.png` - порівняння всіх метрик
- `roc_curves_all.png` - ROC-криві для всіх класифікаторів
- `confusion_matrices_all.png` - матриці неточностей

### Стратегії
- `strategy_comparison.png` - порівняння стратегій
- `reward_distributions.png` - розподіл нагород
- `episode_lengths.png` - розподіл довжини епізодів
- `qlearning_learning_curve.png` - крива навчання Q-Learning
- `qlearning_policy_visualization.png` - візуалізація політики
- `trajectories_*.png` - приклади траєкторій агентів

## Висновки

### Класифікація
- Машинне навчання добре передбачає успішність епізодів
- Ключові фактори: досягнута позиція та напрямок руху
- Початковий стан менш важливий ніж динаміка траєкторії

### Стратегії
- Q-Learning найефективніший після навчання
- Евристики корисні для швидкого прототипування
- Техніка розгойдування критична для успіху
- Exploration важливий для уникнення локальних оптимумів

### Зв'язок
Класифікатори виявили те, чому природньо навчається Q-Learning:
- Максимізація правосторонньої позиції
- Підтримка позитивної швидкості
- Ефективне керування енергією системи

## Вимоги

- Python 3.8+
- numpy
- pandas
- matplotlib
- seaborn
- gymnasium (OpenAI Gym)
- scikit-learn

## Автор

Проект виконано у рамках курсу з машинного навчання та стратегій прийняття рішень.

## Ліцензія

MIT License

