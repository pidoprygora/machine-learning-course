"""
–ë–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥ –ª—ñ–Ω—ñ–π–Ω–æ—ó –¥–∏—Å–∫—Ä–∏–º—ñ–Ω–∞—Ü—ñ—ó —Ç–∞ –≥–∞—É—Å—ñ–≤—Å—å–∫–æ—ó –º–æ–¥–µ–ª—ñ
–î–∞—Ç–∞—Å–µ—Ç: Wine dataset (sklearn.datasets.load_wine)

–ú–µ—Ç–∞: –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ —Ä—ñ–∑–Ω—ñ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏ –¥–ª—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è —Ç–∏–ø—ñ–≤ –≤–∏–Ω–∞
–∑–∞ —Ö—ñ–º—ñ—á–Ω–∏–º–∏ –≤–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—è–º–∏.

–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏:
- Linear Discriminant Analysis (LDA)
- Quadratic Discriminant Analysis (QDA)
- Gaussian Naive Bayes (GaussianNB)

–ú–µ—Ç—Ä–∏–∫–∏:
- Accuracy, Balanced Accuracy, Precision, Recall, F1-score
- ROC-AUC (One-vs-Rest –¥–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å–æ–≤–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó)
- Confusion Matrix
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, precision_score, 
    recall_score, f1_score, roc_curve, auc, confusion_matrix, 
    classification_report, roc_auc_score
)
from sklearn.preprocessing import label_binarize
from scipy import stats
from itertools import cycle

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
sns.set(style="whitegrid", context="notebook")
plt.rcParams["figure.figsize"] = (10, 6)
plt.rcParams['font.size'] = 10

def print_section(title):
    """–í–∏–≤–æ–¥–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü—ñ—ó"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def load_and_explore_data():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø–µ—Ä–≤–∏–Ω–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –¥–∞—Ç–∞—Å–µ—Ç—É Wine"""
    print_section("–ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –î–ê–ù–ò–•")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É Wine
    wine = load_wine()
    X = wine.data
    y = wine.target
    feature_names = wine.feature_names
    target_names = wine.target_names
    
    print(f"‚úì –î–∞—Ç–∞—Å–µ—Ç Wine –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
    print(f"  - –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤: {X.shape[0]}")
    print(f"  - –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: {X.shape[1]}")
    print(f"  - –ö–ª–∞—Å–∏: {target_names}")
    print(f"\nüìä –†–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤:")
    for i, name in enumerate(target_names):
        count = np.sum(y == i)
        print(f"  - –ö–ª–∞—Å {i} ({name}): {count} –∑—Ä–∞–∑–∫—ñ–≤ ({count/len(y)*100:.1f}%)")
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame –¥–ª—è –∑—Ä—É—á–Ω–æ—Å—Ç—ñ
    df = pd.DataFrame(X, columns=feature_names)
    df['target'] = y
    df['target_name'] = df['target'].map({i: name for i, name in enumerate(target_names)})
    
    print(f"\nüìù –û–ø–∏—Å –æ–∑–Ω–∞–∫:")
    print(df[feature_names].describe().T[['mean', 'std', 'min', 'max']].round(2))
    
    return df, X, y, feature_names, target_names


def engineer_features(df, feature_names):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ—ó —Å–∏–ª–∏"""
    print_section("–Ü–ù–ñ–ï–ù–ï–†–Ü–Ø –û–ó–ù–ê–ö")
    
    X_original = df[feature_names].copy()
    
    # 1. –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏ (–∫–≤–∞–¥—Ä–∞—Ç–∏ –≤–∞–∂–ª–∏–≤–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö)
    important_features = ['alcohol', 'flavanoids', 'color_intensity', 'proline', 'od280/od315_of_diluted_wines']
    for feat in important_features:
        if feat in feature_names:
            X_original[f'{feat}_squared'] = X_original[feat] ** 2
    
    # 2. –í–∑–∞—î–º–æ–¥—ñ—ó –º—ñ–∂ –≤–∞–∂–ª–∏–≤–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏
    X_original['alcohol_x_flavanoids'] = X_original['alcohol'] * X_original['flavanoids']
    X_original['color_x_hue'] = X_original['color_intensity'] * X_original['hue']
    X_original['phenols_x_flavanoids'] = X_original['total_phenols'] * X_original['flavanoids']
    
    # 3. –í—ñ–¥–Ω–æ—à–µ–Ω–Ω—è
    X_original['alcohol_proline_ratio'] = X_original['alcohol'] / (X_original['proline'] + 1)
    X_original['phenols_ratio'] = X_original['total_phenols'] / (X_original['nonflavanoid_phenols'] + 0.01)
    
    # 4. –õ–æ–≥–∞—Ä–∏—Ñ–º—ñ—á–Ω—ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–ª—è –∞—Å–∏–º–µ—Ç—Ä–∏—á–Ω–∏—Ö —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤
    X_original['log_proline'] = np.log1p(X_original['proline'])
    X_original['log_od280'] = np.log1p(X_original['od280/od315_of_diluted_wines'])
    
    # 5. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó (z-scores)
    for col in ['alcohol', 'malic_acid', 'ash']:
        if col in X_original.columns:
            X_original[f'{col}_zscore'] = stats.zscore(X_original[col])
    
    new_features = [col for col in X_original.columns if col not in feature_names]
    
    print(f"‚úì –°—Ç–≤–æ—Ä–µ–Ω–æ {len(new_features)} –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫")
    print(f"  - –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏: {len([f for f in new_features if 'squared' in f])}")
    print(f"  - –í–∑–∞—î–º–æ–¥—ñ—ó: {len([f for f in new_features if '_x_' in f])}")
    print(f"  - –í—ñ–¥–Ω–æ—à–µ–Ω–Ω—è: {len([f for f in new_features if 'ratio' in f])}")
    print(f"  - –õ–æ–≥–∞—Ä–∏—Ñ–º—ñ—á–Ω—ñ: {len([f for f in new_features if 'log_' in f])}")
    print(f"  - Z-scores: {len([f for f in new_features if 'zscore' in f])}")
    print(f"\n‚úì –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: {len(X_original.columns)}")
    
    return X_original.values, list(X_original.columns)


def visualize_distributions(df, feature_names, target_names):
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É –æ–∑–Ω–∞–∫ –ø–æ –∫–ª–∞—Å–∞—Ö"""
    print_section("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –†–û–ó–ü–û–î–Ü–õ–£ –î–ê–ù–ò–•")
    
    # –í–∏–±–∏—Ä–∞—î–º–æ —Ç–æ–ø-6 –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –æ–∑–Ω–∞–∫ (–∑–∞ –≤–∞—Ä—ñ–∞—Ü—ñ—î—é –º—ñ–∂ –∫–ª–∞—Å–∞–º–∏)
    top_features = ['alcohol', 'flavanoids', 'color_intensity', 'proline', 
                   'od280/od315_of_diluted_wines', 'hue']
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 9))
    axes = axes.flatten()
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    
    for i, feat in enumerate(top_features):
        ax = axes[i]
        
        for j, target_name in enumerate(target_names):
            data = df[df['target'] == j][feat]
            ax.hist(data, bins=15, alpha=0.5, label=target_name, color=colors[j], density=True)
            
            # –î–æ–¥–∞—î–º–æ KDE
            kde_data = np.linspace(data.min(), data.max(), 100)
            kde = stats.gaussian_kde(data)
            ax.plot(kde_data, kde(kde_data), color=colors[j], linewidth=2)
        
        ax.set_title(f'{feat}', fontsize=11, weight='bold')
        ax.set_xlabel('–ó–Ω–∞—á–µ–Ω–Ω—è', fontsize=9)
        ax.set_ylabel('–©—ñ–ª—å–Ω—ñ—Å—Ç—å', fontsize=9)
        ax.legend(fontsize=8)
        ax.grid(alpha=0.3)
    
    plt.suptitle('–†–æ–∑–ø–æ–¥—ñ–ª –∫–ª—é—á–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ –∑–∞ –∫–ª–∞—Å–∞–º–∏ –≤–∏–Ω–∞', 
                 fontsize=14, weight='bold', y=1.00)
    plt.tight_layout()
    plt.savefig('wine_distributions.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: wine_distributions.png")
    plt.show()
    
    # –ö–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è
    print("\nüìä –ü–æ–±—É–¥–æ–≤–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ...")
    fig, ax = plt.subplots(figsize=(12, 10))
    
    corr_matrix = df[top_features + ['target']].corr()
    
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', 
                cmap='coolwarm', center=0, square=True, 
                linewidths=1, cbar_kws={"shrink": 0.8}, ax=ax)
    
    ax.set_title('–ö–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è –æ–∑–Ω–∞–∫', fontsize=14, weight='bold', pad=20)
    plt.tight_layout()
    plt.savefig('wine_correlation.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: wine_correlation.png")
    plt.show()


def visualize_2d_projection(X_scaled, y, target_names):
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è 2D –ø—Ä–æ—î–∫—Ü—ñ—ó –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ LDA"""
    print("\nüìä –ü–æ–±—É–¥–æ–≤–∞ 2D –ø—Ä–æ—î–∫—Ü—ñ—ó —á–µ—Ä–µ–∑ LDA...")
    
    lda_viz = LinearDiscriminantAnalysis(n_components=2)
    X_lda = lda_viz.fit_transform(X_scaled, y)
    
    fig, ax = plt.subplots(figsize=(10, 7))
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    markers = ['o', 's', '^']
    
    for i, (color, marker, target_name) in enumerate(zip(colors, markers, target_names)):
        mask = y == i
        ax.scatter(X_lda[mask, 0], X_lda[mask, 1], 
                  c=color, marker=marker, s=100, alpha=0.6,
                  edgecolors='black', linewidth=0.5, label=target_name)
    
    ax.set_xlabel('LD1 (–ø–µ—Ä—à–∞ –¥–∏—Å–∫—Ä–∏–º—ñ–Ω–∞–Ω—Ç–Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞)', fontsize=11, weight='bold')
    ax.set_ylabel('LD2 (–¥—Ä—É–≥–∞ –¥–∏—Å–∫—Ä–∏–º—ñ–Ω–∞–Ω—Ç–Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞)', fontsize=11, weight='bold')
    ax.set_title('2D –ø—Ä–æ—î–∫—Ü—ñ—è –¥–∞—Ç–∞—Å–µ—Ç—É Wine —á–µ—Ä–µ–∑ Linear Discriminant Analysis', 
                fontsize=13, weight='bold')
    ax.legend(fontsize=10)
    ax.grid(alpha=0.3)
    
    # –î–æ–¥–∞—î–º–æ explained variance
    explained_var = lda_viz.explained_variance_ratio_
    textstr = f'LD1: {explained_var[0]:.1%}\nLD2: {explained_var[1]:.1%}'
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,
            verticalalignment='top', bbox=props)
    
    plt.tight_layout()
    plt.savefig('wine_lda_projection.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: wine_lda_projection.png")
    plt.show()


def train_and_evaluate_models(X_train, X_test, y_train, y_test, target_names):
    """–ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∞ —Ä—ñ–∑–Ω–∏—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤"""
    print_section("–ù–ê–í–ß–ê–ù–ù–Ø –¢–ê –û–¶–Ü–ù–ö–ê –ú–û–î–ï–õ–ï–ô")
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π
    models = {
        'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),
        'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis(),
        'Gaussian Naive Bayes': GaussianNB()
    }
    
    results = {}
    predictions = {}
    probabilities = {}
    
    for name, model in models.items():
        print(f"\n[{list(models.keys()).index(name) + 1}/{len(models)}] –ù–∞–≤—á–∞–Ω–Ω—è: {name}...")
        
        # –ù–∞–≤—á–∞–Ω–Ω—è
        model.fit(X_train, y_train)
        
        # –ü—Ä–æ–≥–Ω–æ–∑–∏
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)
        
        predictions[name] = y_pred
        probabilities[name] = y_proba
        
        # –û–±—á–∏—Å–ª–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫
        results[name] = {
            'Accuracy': accuracy_score(y_test, y_pred),
            'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred),
            'Precision (macro)': precision_score(y_test, y_pred, average='macro', zero_division=0),
            'Recall (macro)': recall_score(y_test, y_pred, average='macro', zero_division=0),
            'F1-score (macro)': f1_score(y_test, y_pred, average='macro', zero_division=0)
        }
        
        # Cross-validation
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
        results[name]['CV Accuracy (mean)'] = cv_scores.mean()
        results[name]['CV Accuracy (std)'] = cv_scores.std()
        
        print(f"  ‚úì Accuracy: {results[name]['Accuracy']:.4f}")
        print(f"  ‚úì CV Accuracy: {results[name]['CV Accuracy (mean)']:.4f} ¬± {results[name]['CV Accuracy (std)']:.4f}")
    
    # –¢–∞–±–ª–∏—Ü—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    print(f"\nüìä –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π:")
    results_df = pd.DataFrame(results).T
    print(results_df.round(4).to_string())
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    results_df.to_csv('model_comparison.csv')
    print("\n‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: model_comparison.csv")
    
    return models, results, predictions, probabilities


def plot_confusion_matrices(predictions, y_test, target_names):
    """–ü–æ–±—É–¥–æ–≤–∞ –º–∞—Ç—Ä–∏—Ü—å –ø–æ–º–∏–ª–æ–∫ –¥–ª—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π"""
    print_section("–ú–ê–¢–†–ò–¶–Ü –ü–û–ú–ò–õ–û–ö")
    
    fig, axes = plt.subplots(1, 3, figsize=(16, 4))
    
    for idx, (name, y_pred) in enumerate(predictions.items()):
        cm = confusion_matrix(y_test, y_pred)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è –ø–æ–º–∏–ª–æ–∫
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        ax = axes[idx]
        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', 
                   xticklabels=target_names, yticklabels=target_names,
                   cbar_kws={'format': '%.0f%%'}, ax=ax)
        
        ax.set_title(f'{name}\n(Accuracy: {accuracy_score(y_test, y_pred):.2%})', 
                    fontsize=11, weight='bold')
        ax.set_ylabel('–°–ø—Ä–∞–≤–∂–Ω—ñ–π –∫–ª–∞—Å', fontsize=10)
        ax.set_xlabel('–ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–∏–π –∫–ª–∞—Å', fontsize=10)
    
    plt.suptitle('–ú–∞—Ç—Ä–∏—Ü—ñ –ø–æ–º–∏–ª–æ–∫ (–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ)', fontsize=14, weight='bold', y=1.05)
    plt.tight_layout()
    plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: confusion_matrices.png")
    plt.show()


def plot_roc_curves(probabilities, y_test, target_names):
    """–ü–æ–±—É–¥–æ–≤–∞ ROC –∫—Ä–∏–≤–∏—Ö (One-vs-Rest) –¥–ª—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π"""
    print_section("ROC –ö–†–ò–í–Ü –¢–ê AUC")
    
    # –ë—ñ–Ω–∞—Ä–∏–∑–∞—Ü—ñ—è –º—ñ—Ç–æ–∫ –¥–ª—è OvR –ø—ñ–¥—Ö–æ–¥—É
    y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
    n_classes = y_test_bin.shape[1]
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    colors = cycle(['#FF6B6B', '#4ECDC4', '#45B7D1'])
    
    all_auc_scores = {}
    
    for idx, (model_name, y_proba) in enumerate(probabilities.items()):
        ax = axes[idx]
        
        # –û–±—á–∏—Å–ª–µ–Ω–Ω—è ROC –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
        
        # Micro-average ROC curve
        fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_proba.ravel())
        roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
        
        # Macro-average ROC curve
        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
        mean_tpr = np.zeros_like(all_fpr)
        for i in range(n_classes):
            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
        mean_tpr /= n_classes
        fpr["macro"] = all_fpr
        tpr["macro"] = mean_tpr
        roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])
        
        all_auc_scores[model_name] = roc_auc
        
        # –ú–∞–ª—é–≤–∞–Ω–Ω—è –∫—Ä–∏–≤–∏—Ö –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É
        color_iter = cycle(['#FF6B6B', '#4ECDC4', '#45B7D1'])
        for i, color, target_name in zip(range(n_classes), color_iter, target_names):
            ax.plot(fpr[i], tpr[i], color=color, lw=2, alpha=0.8,
                   label=f'{target_name} (AUC = {roc_auc[i]:.3f})')
        
        # Micro —Ç–∞ Macro average
        ax.plot(fpr["micro"], tpr["micro"], color='deeppink', lw=2, linestyle='--',
               label=f'Micro-avg (AUC = {roc_auc["micro"]:.3f})')
        ax.plot(fpr["macro"], tpr["macro"], color='navy', lw=2, linestyle='--',
               label=f'Macro-avg (AUC = {roc_auc["macro"]:.3f})')
        
        # –î—ñ–∞–≥–æ–Ω–∞–ª—å–Ω–∞ –ª—ñ–Ω—ñ—è (–≤–∏–ø–∞–¥–∫–æ–≤–∏–π –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä)
        ax.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.3, label='–í–∏–ø–∞–¥–∫–æ–≤–∏–π (AUC = 0.500)')
        
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate', fontsize=10, weight='bold')
        ax.set_ylabel('True Positive Rate', fontsize=10, weight='bold')
        ax.set_title(f'{model_name}', fontsize=11, weight='bold')
        ax.legend(loc="lower right", fontsize=8)
        ax.grid(alpha=0.3)
    
    plt.suptitle('ROC –∫—Ä–∏–≤—ñ (One-vs-Rest –º—É–ª—å—Ç–∏–∫–ª–∞—Å–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è)', 
                fontsize=14, weight='bold', y=1.02)
    plt.tight_layout()
    plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: roc_curves.png")
    plt.show()
    
    # –¢–∞–±–ª–∏—Ü—è AUC scores
    print("\nüìä AUC Scores:")
    auc_df = pd.DataFrame({
        model_name: {
            f'{target_names[i]}': scores[i] for i in range(n_classes)
        } | {
            'Micro-average': scores['micro'],
            'Macro-average': scores['macro']
        }
        for model_name, scores in all_auc_scores.items()
    }).T
    print(auc_df.round(4).to_string())
    
    auc_df.to_csv('auc_scores.csv')
    print("\n‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: auc_scores.csv")


def analyze_feature_importance(models, feature_names_all, target_names):
    """–ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫"""
    print_section("–ê–ù–ê–õ–Ü–ó –ü–†–ï–î–ò–ö–¢–ò–í–ù–û–á –°–ò–õ–ò –û–ó–ù–ê–ö")
    
    # –î–ª—è LDA –º–æ–∂–Ω–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
    lda_model = models['Linear Discriminant Analysis']
    
    if hasattr(lda_model, 'coef_'):
        # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É
        coefs = lda_model.coef_
        
        # –°–µ—Ä–µ–¥–Ω—è –∞–±—Å–æ–ª—é—Ç–Ω–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –ø–æ –≤—Å—ñ—Ö –∫–ª–∞—Å–∞—Ö
        mean_importance = np.abs(coefs).mean(axis=0)
        
        # –¢–æ–ø-15 –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –æ–∑–Ω–∞–∫
        top_indices = np.argsort(mean_importance)[-15:][::-1]
        
        fig, ax = plt.subplots(figsize=(12, 7))
        
        y_pos = np.arange(len(top_indices))
        ax.barh(y_pos, mean_importance[top_indices], color='steelblue', alpha=0.8)
        ax.set_yticks(y_pos)
        ax.set_yticklabels([feature_names_all[i] for i in top_indices], fontsize=9)
        ax.invert_yaxis()
        ax.set_xlabel('–°–µ—Ä–µ–¥–Ω—è –∞–±—Å–æ–ª—é—Ç–Ω–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å', fontsize=11, weight='bold')
        ax.set_title('–¢–æ–ø-15 –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –æ–∑–Ω–∞–∫ (Linear Discriminant Analysis)', 
                    fontsize=13, weight='bold')
        ax.grid(axis='x', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
        print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: feature_importance.png")
        plt.show()
        
        # –î–µ—Ç–∞–ª—å–Ω–∞ —Ç–∞–±–ª–∏—Ü—è –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤
        print("\nüìä –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ LDA –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É:")
        coef_df = pd.DataFrame(
            coefs.T,
            columns=[f'–ö–ª–∞—Å {i} ({name})' for i, name in enumerate(target_names)],
            index=feature_names_all
        )
        
        # –í–∏–≤–æ–¥–∏–º–æ —Ç–æ–ø-10
        coef_df['Mean_Abs'] = mean_importance
        coef_df_sorted = coef_df.sort_values('Mean_Abs', ascending=False)
        print(coef_df_sorted.head(10).round(4).to_string())
        
        coef_df_sorted.to_csv('feature_coefficients.csv')
        print("\n‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: feature_coefficients.csv")


def detailed_classification_reports(predictions, y_test, target_names):
    """–î–µ—Ç–∞–ª—å–Ω—ñ –∑–≤—ñ—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–ª—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π"""
    print_section("–î–ï–¢–ê–õ–¨–ù–Ü –ó–í–Ü–¢–ò –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–á")
    
    for name, y_pred in predictions.items():
        print(f"\n{'='*70}")
        print(f"  {name}")
        print('='*70)
        print(classification_report(y_test, y_pred, target_names=target_names, digits=4))


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    print("\n" + "="*70)
    print("  –ë–ï–ù–ß–ú–ê–†–ö–Ü–ù–ì –õ–Ü–ù–Ü–ô–ù–û–á –î–ò–°–ö–†–ò–ú–Ü–ù–ê–¶–Ü–á –¢–ê –ì–ê–£–°–Ü–í–°–¨–ö–û–á –ú–û–î–ï–õ–Ü")
    print("  –î–∞—Ç–∞—Å–µ—Ç: Wine (sklearn)")
    print("="*70)
    
    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    df, X, y, feature_names, target_names = load_and_explore_data()
    
    # 2. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤
    visualize_distributions(df, feature_names, target_names)
    
    # 3. –Ü–Ω–∂–µ–Ω–µ—Ä—ñ—è –æ–∑–Ω–∞–∫
    X_engineered, feature_names_all = engineer_features(df, feature_names)
    
    # 4. –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/test (—Å—Ç—Ä–∞—Ç–∏—Ñ—ñ–∫–æ–≤–∞–Ω–µ)
    print_section("–†–û–ó–î–Ü–õ–ï–ù–ù–Ø –î–ê–ù–ò–•")
    X_train, X_test, y_train, y_test = train_test_split(
        X_engineered, y, test_size=0.3, random_state=42, stratify=y
    )
    print(f"‚úì Train set: {len(X_train)} –∑—Ä–∞–∑–∫—ñ–≤ ({len(X_train)/len(X_engineered)*100:.1f}%)")
    print(f"‚úì Test set:  {len(X_test)} –∑—Ä–∞–∑–∫—ñ–≤ ({len(X_test)/len(X_engineered)*100:.1f}%)")
    
    # –†–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤ —É train —Ç–∞ test
    print("\nüìä –†–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤ —É train set:")
    for i, name in enumerate(target_names):
        count = np.sum(y_train == i)
        print(f"  - {name}: {count} ({count/len(y_train)*100:.1f}%)")
    
    print("\nüìä –†–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤ —É test set:")
    for i, name in enumerate(target_names):
        count = np.sum(y_test == i)
        print(f"  - {name}: {count} ({count/len(y_test)*100:.1f}%)")
    
    # 5. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è
    print_section("–°–¢–ê–ù–î–ê–†–¢–ò–ó–ê–¶–Ü–Ø –û–ó–ù–ê–ö")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    print(f"‚úì –û–∑–Ω–∞–∫–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω—ñ (mean=0, std=1)")
    print(f"  - Train mean: {X_train_scaled.mean():.6f}")
    print(f"  - Train std:  {X_train_scaled.std():.6f}")
    
    # 6. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è 2D –ø—Ä–æ—î–∫—Ü—ñ—ó
    visualize_2d_projection(X_train_scaled, y_train, target_names)
    
    # 7. –ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
    models, results, predictions, probabilities = train_and_evaluate_models(
        X_train_scaled, X_test_scaled, y_train, y_test, target_names
    )
    
    # 8. –ú–∞—Ç—Ä–∏—Ü—ñ –ø–æ–º–∏–ª–æ–∫
    plot_confusion_matrices(predictions, y_test, target_names)
    
    # 9. ROC –∫—Ä–∏–≤—ñ
    plot_roc_curves(probabilities, y_test, target_names)
    
    # 10. –ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
    analyze_feature_importance(models, feature_names_all, target_names)
    
    # 11. –î–µ—Ç–∞–ª—å–Ω—ñ –∑–≤—ñ—Ç–∏
    detailed_classification_reports(predictions, y_test, target_names)
    
    # –ü—ñ–¥—Å—É–º–æ–∫
    print_section("–ü–Ü–î–°–£–ú–û–ö")
    print("\n‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("\nüìÅ –°—Ç–≤–æ—Ä–µ–Ω—ñ —Ñ–∞–π–ª–∏:")
    print("  - wine_distributions.png - —Ä–æ–∑–ø–æ–¥—ñ–ª –æ–∑–Ω–∞–∫")
    print("  - wine_correlation.png - –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è")
    print("  - wine_lda_projection.png - 2D –ø—Ä–æ—î–∫—Ü—ñ—è —á–µ—Ä–µ–∑ LDA")
    print("  - confusion_matrices.png - –º–∞—Ç—Ä–∏—Ü—ñ –ø–æ–º–∏–ª–æ–∫")
    print("  - roc_curves.png - ROC –∫—Ä–∏–≤—ñ")
    print("  - feature_importance.png - –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫")
    print("  - model_comparison.csv - –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π")
    print("  - auc_scores.csv - AUC scores")
    print("  - feature_coefficients.csv - –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –æ–∑–Ω–∞–∫")
    
    print("\n" + "="*70)


if __name__ == "__main__":
    main()

