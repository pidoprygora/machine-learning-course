"""
–ë–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª—å–æ–≤–∞–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è (Unsupervised Learning)
–§–æ–∫—É—Å: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —ñ—Ç–µ—Ä–∞—Ü—ñ–π K-Means —Ç–∞ EM (Gaussian Mixture Models)

–ú–µ—Ç–∞: –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ —Ä—ñ–∑–Ω—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó –Ω–∞ 2D –¥–∞–Ω–∏—Ö
—Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø—Ä–æ—Ü–µ—Å –Ω–∞–≤—á–∞–Ω–Ω—è –ø–æ–∫—Ä–æ–∫–æ–≤–æ.

–ê–ª–≥–æ—Ä–∏—Ç–º–∏:
- K-Means Clustering
- Gaussian Mixture Models (EM Algorithm)
- DBSCAN (–¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è)
- Hierarchical Clustering (–¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è)

–ú–µ—Ç—Ä–∏–∫–∏:
- Silhouette Score
- Davies-Bouldin Index
- Calinski-Harabasz Index
- Adjusted Rand Index (—è–∫—â–æ —î —Å–ø—Ä–∞–≤–∂–Ω—ñ –º—ñ—Ç–∫–∏)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.datasets import make_blobs, make_moons, make_circles
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import (
    silhouette_score, davies_bouldin_score, calinski_harabasz_score,
    adjusted_rand_score, normalized_mutual_info_score, confusion_matrix
)
from matplotlib.animation import FuncAnimation
from matplotlib.patches import Ellipse
from scipy.spatial.distance import cdist
import matplotlib.patches as mpatches

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
sns.set(style="whitegrid", context="notebook")
plt.rcParams["figure.figsize"] = (12, 8)
plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'DejaVu Sans'

def print_section(title):
    """–í–∏–≤–æ–¥–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü—ñ—ó"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def generate_datasets():
    """–ì–µ–Ω–µ—Ä—É—î —Ä—ñ–∑–Ω—ñ —Ç–∏–ø–∏ 2D –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó"""
    print_section("–ì–ï–ù–ï–†–ê–¶–Ü–Ø –î–ê–ù–ò–•")
    
    np.random.seed(42)
    
    datasets = {}
    
    # 1. Blobs - —á—ñ—Ç–∫—ñ –≥–∞—É—Å—ñ–≤—Å—å–∫—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏ (—ñ–¥–µ–∞–ª—å–Ω–æ –¥–ª—è K-Means —ñ GMM)
    X_blobs, y_blobs = make_blobs(
        n_samples=600, 
        centers=4, 
        n_features=2,
        cluster_std=0.8,
        random_state=42
    )
    datasets['Blobs (4 –∫–ª–∞—Å—Ç–µ—Ä–∏)'] = (X_blobs, y_blobs)
    print("‚úì –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç 'Blobs': 600 —Ç–æ—á–æ–∫, 4 –∫–ª–∞—Å—Ç–µ—Ä–∏")
    
    # 2. Blobs –∑ —Ä—ñ–∑–Ω–æ—é –¥–∏—Å–ø–µ—Ä—Å—ñ—î—é
    X_varied, y_varied = make_blobs(
        n_samples=600,
        centers=5,
        n_features=2,
        cluster_std=[0.5, 1.0, 1.5, 0.7, 1.2],
        random_state=42
    )
    datasets['Blobs (—Ä—ñ–∑–Ω–∞ –¥–∏—Å–ø–µ—Ä—Å—ñ—è)'] = (X_varied, y_varied)
    print("‚úì –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç 'Varied Blobs': 600 —Ç–æ—á–æ–∫, 5 –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤")
    
    # 3. Anisotropic - –≤–∏—Ç—è–≥–Ω—É—Ç—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏
    random_state = np.random.RandomState(42)
    X_aniso = random_state.randn(600, 2)
    transformation = [[0.6, -0.6], [-0.4, 0.8]]
    X_aniso = np.dot(X_aniso, transformation)
    y_aniso = KMeans(n_clusters=3, random_state=42, n_init=10).fit_predict(X_aniso)
    datasets['Anisotropic'] = (X_aniso, y_aniso)
    print("‚úì –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç 'Anisotropic': 600 —Ç–æ—á–æ–∫, 3 –∫–ª–∞—Å—Ç–µ—Ä–∏")
    
    return datasets


def visualize_original_data(datasets):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ"""
    print_section("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –í–ò–•–Ü–î–ù–ò–• –î–ê–ù–ò–•")
    
    n_datasets = len(datasets)
    fig, axes = plt.subplots(1, n_datasets, figsize=(6*n_datasets, 5))
    
    if n_datasets == 1:
        axes = [axes]
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']
    
    for idx, (name, (X, y)) in enumerate(datasets.items()):
        ax = axes[idx]
        
        # –ú–∞–ª—é—î–º–æ —Ç–æ—á–∫–∏
        for i in range(len(np.unique(y))):
            mask = y == i
            ax.scatter(X[mask, 0], X[mask, 1], 
                      c=colors[i % len(colors)], 
                      s=50, alpha=0.6, 
                      edgecolors='black', linewidth=0.5,
                      label=f'–ö–ª–∞—Å {i}')
        
        ax.set_title(f'{name}\n({len(X)} —Ç–æ—á–æ–∫, {len(np.unique(y))} –∫–ª–∞—Å—ñ–≤)', 
                    fontsize=12, weight='bold')
        ax.set_xlabel('X‚ÇÅ', fontsize=11)
        ax.set_ylabel('X‚ÇÇ', fontsize=11)
        ax.legend(fontsize=9)
        ax.grid(alpha=0.3)
    
    plt.suptitle('–†–æ–∑–ø–æ–¥—ñ–ª –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤', fontsize=14, weight='bold', y=1.02)
    plt.tight_layout()
    plt.savefig('original_data_distribution.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: original_data_distribution.png")
    plt.show()


class KMeansVisualizer:
    """–ö–ª–∞—Å –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó —ñ—Ç–µ—Ä–∞—Ü—ñ–π K-Means"""
    
    def __init__(self, n_clusters=4, max_iter=20):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.centroids_history = []
        self.labels_history = []
        self.inertia_history = []
    
    def fit(self, X):
        """–ù–∞–≤—á–∞–Ω–Ω—è K-Means –∑—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º —ñ—Å—Ç–æ—Ä—ñ—ó"""
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤ (k-means++)
        np.random.seed(42)
        n_samples = X.shape[0]
        
        # –ü–µ—Ä—à–∏–π —Ü–µ–Ω—Ç—Ä–æ—ó–¥ –≤–∏–ø–∞–¥–∫–æ–≤–∏–π
        centroids = [X[np.random.randint(n_samples)]]
        
        # –†–µ—à—Ç–∞ —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤ –∑–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º k-means++
        for _ in range(1, self.n_clusters):
            distances = cdist(X, centroids, 'euclidean')
            min_distances = np.min(distances, axis=1)
            probabilities = min_distances ** 2
            probabilities /= probabilities.sum()
            cumulative_probs = np.cumsum(probabilities)
            r = np.random.random()
            for idx, cum_prob in enumerate(cumulative_probs):
                if r < cum_prob:
                    centroids.append(X[idx])
                    break
        
        centroids = np.array(centroids)
        
        # –Ü—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –ø—Ä–æ—Ü–µ—Å
        for iteration in range(self.max_iter):
            # E-step: –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–æ—á–æ–∫ –¥–æ –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤
            distances = cdist(X, centroids, 'euclidean')
            labels = np.argmin(distances, axis=1)
            
            # –û–±—á–∏—Å–ª–µ–Ω–Ω—è inertia
            inertia = np.sum([distances[i, labels[i]]**2 for i in range(len(X))])
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó
            self.centroids_history.append(centroids.copy())
            self.labels_history.append(labels.copy())
            self.inertia_history.append(inertia)
            
            # M-step: –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤
            new_centroids = np.array([X[labels == k].mean(axis=0) 
                                      for k in range(self.n_clusters)])
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å
            if np.allclose(centroids, new_centroids, rtol=1e-6):
                print(f"  –ó–±—ñ–∂–Ω—ñ—Å—Ç—å –¥–æ—Å—è–≥–Ω—É—Ç–∞ –Ω–∞ —ñ—Ç–µ—Ä–∞—Ü—ñ—ó {iteration + 1}")
                break
            
            centroids = new_centroids
        
        self.final_centroids = centroids
        self.final_labels = labels
        
        return self
    
    def visualize_iterations(self, X, save_path='kmeans_iterations.png'):
        """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª—é—á–æ–≤–∏—Ö —ñ—Ç–µ—Ä–∞—Ü—ñ–π"""
        # –í–∏–±–∏—Ä–∞—î–º–æ –∫–ª—é—á–æ–≤—ñ —ñ—Ç–µ—Ä–∞—Ü—ñ—ó –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        iterations_to_show = [0, 1, 2, 5, len(self.centroids_history)-1]
        iterations_to_show = [i for i in iterations_to_show if i < len(self.centroids_history)]
        
        n_plots = len(iterations_to_show)
        fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 4))
        
        if n_plots == 1:
            axes = [axes]
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']
        
        for plot_idx, iter_idx in enumerate(iterations_to_show):
            ax = axes[plot_idx]
            centroids = self.centroids_history[iter_idx]
            labels = self.labels_history[iter_idx]
            inertia = self.inertia_history[iter_idx]
            
            # –ú–∞–ª—é—î–º–æ —Ç–æ—á–∫–∏
            for k in range(self.n_clusters):
                mask = labels == k
                ax.scatter(X[mask, 0], X[mask, 1],
                          c=colors[k % len(colors)],
                          s=50, alpha=0.5,
                          edgecolors='black', linewidth=0.3)
            
            # –ú–∞–ª—é—î–º–æ —Ü–µ–Ω—Ç—Ä–æ—ó–¥–∏
            ax.scatter(centroids[:, 0], centroids[:, 1],
                      c='red', s=300, alpha=0.9,
                      marker='*', edgecolors='black', linewidth=2,
                      label='–¶–µ–Ω—Ç—Ä–æ—ó–¥–∏', zorder=10)
            
            # –ú–∞–ª—é—î–º–æ –ª—ñ–Ω—ñ—ó –≤—ñ–¥ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤ (—è–∫—â–æ —î)
            if iter_idx > 0:
                prev_centroids = self.centroids_history[iter_idx - 1]
                for k in range(self.n_clusters):
                    ax.plot([prev_centroids[k, 0], centroids[k, 0]],
                           [prev_centroids[k, 1], centroids[k, 1]],
                           'k--', alpha=0.3, linewidth=1)
            
            ax.set_title(f'–Ü—Ç–µ—Ä–∞—Ü—ñ—è {iter_idx + 1}\nInertia: {inertia:.2f}',
                        fontsize=11, weight='bold')
            ax.set_xlabel('X‚ÇÅ', fontsize=10)
            ax.set_ylabel('X‚ÇÇ', fontsize=10)
            ax.legend(fontsize=8)
            ax.grid(alpha=0.3)
        
        plt.suptitle(f'K-Means: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —ñ—Ç–µ—Ä–∞—Ü—ñ–π (k={self.n_clusters})',
                    fontsize=14, weight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path}")
        plt.show()
    
    def plot_inertia_curve(self):
        """–ì—Ä–∞—Ñ—ñ–∫ –∑–º—ñ–Ω–∏ inertia"""
        plt.figure(figsize=(10, 6))
        plt.plot(range(1, len(self.inertia_history) + 1), 
                self.inertia_history, 
                marker='o', linewidth=2, markersize=8, color='#FF6B6B')
        plt.xlabel('–Ü—Ç–µ—Ä–∞—Ü—ñ—è', fontsize=12, weight='bold')
        plt.ylabel('Inertia (—Å—É–º–∞ –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤ –≤—ñ–¥—Å—Ç–∞–Ω–µ–π)', fontsize=12, weight='bold')
        plt.title('–ó–±—ñ–∂–Ω—ñ—Å—Ç—å K-Means: –∑–º—ñ–Ω–∞ Inertia', fontsize=14, weight='bold')
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig('kmeans_convergence.png', dpi=300, bbox_inches='tight')
        print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: kmeans_convergence.png")
        plt.show()


class GMMVisualizer:
    """–ö–ª–∞—Å –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó —ñ—Ç–µ—Ä–∞—Ü—ñ–π Gaussian Mixture Models (EM)"""
    
    def __init__(self, n_components=4, max_iter=20):
        self.n_components = n_components
        self.max_iter = max_iter
        self.means_history = []
        self.covariances_history = []
        self.weights_history = []
        self.log_likelihood_history = []
    
    def fit(self, X):
        """–ù–∞–≤—á–∞–Ω–Ω—è GMM –∑—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º —ñ—Å—Ç–æ—Ä—ñ—ó"""
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ sklearn GMM –∑ –º–æ–¥–∏—Ñ—ñ–∫–∞—Ü—ñ—î—é
        gmm = GaussianMixture(
            n_components=self.n_components,
            covariance_type='full',
            max_iter=1,  # –ü–æ –æ–¥–Ω—ñ–π —ñ—Ç–µ—Ä–∞—Ü—ñ—ó –∑–∞ —Ä–∞–∑
            random_state=42,
            warm_start=True
        )
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
        gmm.fit(X)
        
        for iteration in range(self.max_iter):
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
            self.means_history.append(gmm.means_.copy())
            self.covariances_history.append(gmm.covariances_.copy())
            self.weights_history.append(gmm.weights_.copy())
            
            # –û–±—á–∏—Å–ª—é—î–º–æ log-likelihood
            log_likelihood = gmm.score(X) * len(X)
            self.log_likelihood_history.append(log_likelihood)
            
            # –†–æ–±–∏–º–æ —â–µ –æ–¥–Ω—É —ñ—Ç–µ—Ä–∞—Ü—ñ—é EM
            old_means = gmm.means_.copy()
            gmm.max_iter = iteration + 2
            gmm.fit(X)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å
            if np.allclose(old_means, gmm.means_, rtol=1e-4):
                print(f"  –ó–±—ñ–∂–Ω—ñ—Å—Ç—å –¥–æ—Å—è–≥–Ω—É—Ç–∞ –Ω–∞ —ñ—Ç–µ—Ä–∞—Ü—ñ—ó {iteration + 1}")
                break
        
        self.gmm = gmm
        self.final_labels = gmm.predict(X)
        
        return self
    
    def visualize_iterations(self, X, save_path='gmm_iterations.png'):
        """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª—é—á–æ–≤–∏—Ö —ñ—Ç–µ—Ä–∞—Ü—ñ–π GMM"""
        iterations_to_show = [0, 1, 2, 5, len(self.means_history)-1]
        iterations_to_show = [i for i in iterations_to_show if i < len(self.means_history)]
        
        n_plots = len(iterations_to_show)
        fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 4))
        
        if n_plots == 1:
            axes = [axes]
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']
        
        for plot_idx, iter_idx in enumerate(iterations_to_show):
            ax = axes[plot_idx]
            means = self.means_history[iter_idx]
            covariances = self.covariances_history[iter_idx]
            weights = self.weights_history[iter_idx]
            log_like = self.log_likelihood_history[iter_idx]
            
            # –ú–∞–ª—é—î–º–æ –≤—Å—ñ —Ç–æ—á–∫–∏ —Å—ñ—Ä–∏–º
            ax.scatter(X[:, 0], X[:, 1], c='lightgray', s=20, alpha=0.4)
            
            # –ú–∞–ª—é—î–º–æ –≥–∞—É—Å—ñ–∞–Ω–∏
            for k in range(self.n_components):
                mean = means[k]
                cov = covariances[k]
                
                # –ú–∞–ª—é—î–º–æ –µ–ª—ñ–ø—Å–∏ (1, 2, 3 sigma)
                for n_std in [1, 2, 3]:
                    self._plot_gaussian_ellipse(ax, mean, cov, n_std, 
                                                colors[k % len(colors)])
                
                # –¶–µ–Ω—Ç—Ä –≥–∞—É—Å—ñ–∞–Ω–∞
                ax.scatter(mean[0], mean[1], 
                          c=colors[k % len(colors)], 
                          s=200, marker='*',
                          edgecolors='black', linewidth=2,
                          zorder=10, alpha=0.9)
                
                # –ü—ñ–¥–ø–∏—Å –∑ –≤–∞–≥–æ—é
                ax.text(mean[0], mean[1] + 0.3, 
                       f'w={weights[k]:.2f}',
                       ha='center', fontsize=8,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
            
            ax.set_title(f'–Ü—Ç–µ—Ä–∞—Ü—ñ—è {iter_idx + 1}\nLog-Likelihood: {log_like:.1f}',
                        fontsize=11, weight='bold')
            ax.set_xlabel('X‚ÇÅ', fontsize=10)
            ax.set_ylabel('X‚ÇÇ', fontsize=10)
            ax.grid(alpha=0.3)
        
        plt.suptitle(f'GMM (EM): –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —ñ—Ç–µ—Ä–∞—Ü—ñ–π (k={self.n_components})',
                    fontsize=14, weight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {save_path}")
        plt.show()
    
    def _plot_gaussian_ellipse(self, ax, mean, cov, n_std, color):
        """–ú–∞–ª—é—î –µ–ª—ñ–ø—Å –¥–ª—è –≥–∞—É—Å—ñ–∞–Ω–∞"""
        # –í–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∞ –≤–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏
        eigenvalues, eigenvectors = np.linalg.eigh(cov)
        order = eigenvalues.argsort()[::-1]
        eigenvalues = eigenvalues[order]
        eigenvectors = eigenvectors[:, order]
        
        # –ö—É—Ç –ø–æ–≤–æ—Ä–æ—Ç—É
        angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))
        
        # –®–∏—Ä–∏–Ω–∞ —Ç–∞ –≤–∏—Å–æ—Ç–∞ –µ–ª—ñ–ø—Å–∞
        width, height = 2 * n_std * np.sqrt(eigenvalues)
        
        # –ú–∞–ª—é—î–º–æ –µ–ª—ñ–ø—Å
        ellipse = Ellipse(mean, width, height, angle=angle,
                         facecolor=color, alpha=0.1,
                         edgecolor=color, linewidth=1.5 if n_std == 2 else 0.5)
        ax.add_patch(ellipse)
    
    def plot_log_likelihood_curve(self):
        """–ì—Ä–∞—Ñ—ñ–∫ –∑–º—ñ–Ω–∏ log-likelihood"""
        plt.figure(figsize=(10, 6))
        plt.plot(range(1, len(self.log_likelihood_history) + 1), 
                self.log_likelihood_history, 
                marker='o', linewidth=2, markersize=8, color='#4ECDC4')
        plt.xlabel('–Ü—Ç–µ—Ä–∞—Ü—ñ—è', fontsize=12, weight='bold')
        plt.ylabel('Log-Likelihood', fontsize=12, weight='bold')
        plt.title('–ó–±—ñ–∂–Ω—ñ—Å—Ç—å GMM (EM): –∑–º—ñ–Ω–∞ Log-Likelihood', fontsize=14, weight='bold')
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig('gmm_convergence.png', dpi=300, bbox_inches='tight')
        print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: gmm_convergence.png")
        plt.show()


def benchmark_clustering_algorithms(X, y_true, dataset_name):
    """–ë–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥ —Ä—ñ–∑–Ω–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó"""
    print(f"\n{'='*70}")
    print(f"  –ë–ï–ù–ß–ú–ê–†–ö–Ü–ù–ì: {dataset_name}")
    print('='*70)
    
    n_clusters = len(np.unique(y_true))
    
    # –ê–ª–≥–æ—Ä–∏—Ç–º–∏ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    algorithms = {
        'K-Means': KMeans(n_clusters=n_clusters, random_state=42, n_init=10),
        'GMM (EM)': GaussianMixture(n_components=n_clusters, random_state=42),
        'DBSCAN': DBSCAN(eps=0.5, min_samples=5),
        'Hierarchical': AgglomerativeClustering(n_clusters=n_clusters)
    }
    
    results = {}
    predictions = {}
    
    for name, algorithm in algorithms.items():
        print(f"\n[{list(algorithms.keys()).index(name) + 1}/{len(algorithms)}] –ù–∞–≤—á–∞–Ω–Ω—è: {name}...")
        
        # –ù–∞–≤—á–∞–Ω–Ω—è
        if name == 'GMM (EM)':
            algorithm.fit(X)
            labels = algorithm.predict(X)
        else:
            labels = algorithm.fit_predict(X)
        
        predictions[name] = labels
        
        # –û–±—á–∏—Å–ª–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ (—Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –Ω–µ–º–∞—î —à—É–º–æ–≤–∏—Ö —Ç–æ—á–æ–∫ -1)
        if len(np.unique(labels)) > 1 and -1 not in labels:
            results[name] = {
                'Silhouette Score': silhouette_score(X, labels),
                'Davies-Bouldin Index': davies_bouldin_score(X, labels),
                'Calinski-Harabasz Index': calinski_harabasz_score(X, labels),
                'Adjusted Rand Index': adjusted_rand_score(y_true, labels),
                'Normalized Mutual Info': normalized_mutual_info_score(y_true, labels),
                'N Clusters': len(np.unique(labels))
            }
            print(f"  ‚úì Silhouette: {results[name]['Silhouette Score']:.4f}")
            print(f"  ‚úì ARI: {results[name]['Adjusted Rand Index']:.4f}")
        else:
            print(f"  ‚ö† –ê–ª–≥–æ—Ä–∏—Ç–º –∑–Ω–∞–π—à–æ–≤ {len(np.unique(labels))} –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (—î —à—É–º–æ–≤—ñ —Ç–æ—á–∫–∏)")
            results[name] = {
                'Silhouette Score': np.nan,
                'Davies-Bouldin Index': np.nan,
                'Calinski-Harabasz Index': np.nan,
                'Adjusted Rand Index': adjusted_rand_score(y_true, labels),
                'Normalized Mutual Info': normalized_mutual_info_score(y_true, labels),
                'N Clusters': len(np.unique(labels))
            }
    
    # –¢–∞–±–ª–∏—Ü—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    print(f"\nüìä –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤:")
    results_df = pd.DataFrame(results).T
    print(results_df.round(4).to_string())
    
    return results, predictions, results_df


def visualize_clustering_results(X, predictions, dataset_name):
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó"""
    n_algorithms = len(predictions)
    fig, axes = plt.subplots(1, n_algorithms, figsize=(5*n_algorithms, 4))
    
    if n_algorithms == 1:
        axes = [axes]
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#DDA0DD']
    
    for idx, (name, labels) in enumerate(predictions.items()):
        ax = axes[idx]
        
        unique_labels = np.unique(labels)
        
        for k in unique_labels:
            if k == -1:
                # –®—É–º–æ–≤—ñ —Ç–æ—á–∫–∏ (–¥–ª—è DBSCAN)
                mask = labels == k
                ax.scatter(X[mask, 0], X[mask, 1],
                          c='black', s=20, alpha=0.3,
                          marker='x', label='–®—É–º')
            else:
                mask = labels == k
                ax.scatter(X[mask, 0], X[mask, 1],
                          c=colors[k % len(colors)],
                          s=50, alpha=0.6,
                          edgecolors='black', linewidth=0.3,
                          label=f'–ö–ª–∞—Å—Ç–µ—Ä {k}')
        
        ax.set_title(f'{name}\n({len(unique_labels)} –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤)',
                    fontsize=11, weight='bold')
        ax.set_xlabel('X‚ÇÅ', fontsize=10)
        ax.set_ylabel('X‚ÇÇ', fontsize=10)
        ax.legend(fontsize=8, loc='best')
        ax.grid(alpha=0.3)
    
    plt.suptitle(f'–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó: {dataset_name}',
                fontsize=14, weight='bold', y=1.02)
    plt.tight_layout()
    
    safe_name = dataset_name.replace(' ', '_').replace('(', '').replace(')', '')
    plt.savefig(f'clustering_results_{safe_name}.png', dpi=300, bbox_inches='tight')
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: clustering_results_{safe_name}.png")
    plt.show()


def elbow_method_analysis(X, max_k=10):
    """–ú–µ—Ç–æ–¥ –ª—ñ–∫—Ç—è –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤"""
    print_section("–ú–ï–¢–û–î –õ–Ü–ö–¢–Ø (ELBOW METHOD)")
    
    inertias = []
    silhouette_scores = []
    K_range = range(2, max_k + 1)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        inertias.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(X, labels))
        print(f"  k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_score(X, labels):.4f}")
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # Elbow curve
    ax1.plot(K_range, inertias, marker='o', linewidth=2, markersize=8, color='#FF6B6B')
    ax1.set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (k)', fontsize=12, weight='bold')
    ax1.set_ylabel('Inertia', fontsize=12, weight='bold')
    ax1.set_title('–ú–µ—Ç–æ–¥ –ª—ñ–∫—Ç—è: Inertia vs k', fontsize=13, weight='bold')
    ax1.grid(alpha=0.3)
    
    # Silhouette scores
    ax2.plot(K_range, silhouette_scores, marker='o', linewidth=2, markersize=8, color='#4ECDC4')
    ax2.set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (k)', fontsize=12, weight='bold')
    ax2.set_ylabel('Silhouette Score', fontsize=12, weight='bold')
    ax2.set_title('Silhouette Score vs k', fontsize=13, weight='bold')
    ax2.grid(alpha=0.3)
    
    # –û–ø—Ç–∏–º–∞–ª—å–Ω–µ k
    optimal_k = K_range[np.argmax(silhouette_scores)]
    ax2.axvline(optimal_k, color='red', linestyle='--', linewidth=2, 
                label=f'–û–ø—Ç–∏–º–∞–ª—å–Ω–µ k={optimal_k}')
    ax2.legend(fontsize=10)
    
    plt.tight_layout()
    plt.savefig('elbow_method.png', dpi=300, bbox_inches='tight')
    print("\n‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: elbow_method.png")
    plt.show()
    
    print(f"\n‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (–∑–∞ Silhouette): k={optimal_k}")


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    print("\n" + "="*70)
    print("  –ë–ï–ù–ß–ú–ê–†–ö–Ü–ù–ì –ê–õ–ì–û–†–ò–¢–ú–Ü–í –ù–ï–ö–û–ù–¢–†–û–õ–¨–û–í–ê–ù–û–ì–û –ù–ê–í–ß–ê–ù–ù–Ø")
    print("  –§–æ–∫—É—Å: K-Means —Ç–∞ EM (Gaussian Mixture Models)")
    print("="*70)
    
    # 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
    datasets = generate_datasets()
    
    # 2. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    visualize_original_data(datasets)
    
    # –ü—Ä–∞—Ü—é—î–º–æ –∑ –ø–µ—Ä—à–∏–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ—ó –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
    main_dataset_name = 'Blobs (4 –∫–ª–∞—Å—Ç–µ—Ä–∏)'
    X, y_true = datasets[main_dataset_name]
    
    # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/test
    print_section("–†–û–ó–î–Ü–õ–ï–ù–ù–Ø –î–ê–ù–ò–•")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_true, test_size=0.3, random_state=42, stratify=y_true
    )
    print(f"‚úì Train set: {len(X_train)} —Ç–æ—á–æ–∫ ({len(X_train)/len(X)*100:.1f}%)")
    print(f"‚úì Test set:  {len(X_test)} —Ç–æ—á–æ–∫ ({len(X_test)/len(X)*100:.1f}%)")
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ, –∞–ª–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 3. –ú–µ—Ç–æ–¥ –ª—ñ–∫—Ç—è
    elbow_method_analysis(X_train_scaled, max_k=10)
    
    # 4. K-Means –∑ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—î—é —ñ—Ç–µ—Ä–∞—Ü—ñ–π
    print_section("K-MEANS: –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –Ü–¢–ï–†–ê–¶–Ü–ô")
    kmeans_viz = KMeansVisualizer(n_clusters=4, max_iter=20)
    kmeans_viz.fit(X_train_scaled)
    kmeans_viz.visualize_iterations(X_train_scaled, 'kmeans_iterations.png')
    kmeans_viz.plot_inertia_curve()
    
    # 5. GMM –∑ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—î—é —ñ—Ç–µ—Ä–∞—Ü—ñ–π
    print_section("GMM (EM): –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –Ü–¢–ï–†–ê–¶–Ü–ô")
    gmm_viz = GMMVisualizer(n_components=4, max_iter=20)
    gmm_viz.fit(X_train_scaled)
    gmm_viz.visualize_iterations(X_train_scaled, 'gmm_iterations.png')
    gmm_viz.plot_log_likelihood_curve()
    
    # 6. –ë–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥ –Ω–∞ –≤—Å—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
    all_results = {}
    for dataset_name, (X_data, y_data) in datasets.items():
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è
        X_scaled = scaler.fit_transform(X_data)
        
        # –ë–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥
        results, predictions, results_df = benchmark_clustering_algorithms(
            X_scaled, y_data, dataset_name
        )
        all_results[dataset_name] = results_df
        
        # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        visualize_clustering_results(X_scaled, predictions, dataset_name)
    
    # 7. –ó–≤–µ–¥–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    print_section("–ó–í–ï–î–ï–ù–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò")
    
    for dataset_name, results_df in all_results.items():
        print(f"\n{'='*70}")
        print(f"  {dataset_name}")
        print('='*70)
        print(results_df.to_string())
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è CSV
        safe_name = dataset_name.replace(' ', '_').replace('(', '').replace(')', '')
        results_df.to_csv(f'results_{safe_name}.csv')
        print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results_{safe_name}.csv")
    
    # –ü—ñ–¥—Å—É–º–æ–∫
    print_section("–ü–Ü–î–°–£–ú–û–ö")
    print("\n‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("\nüìÅ –°—Ç–≤–æ—Ä–µ–Ω—ñ —Ñ–∞–π–ª–∏:")
    print("  - original_data_distribution.png - –≤–∏—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ")
    print("  - elbow_method.png - –º–µ—Ç–æ–¥ –ª—ñ–∫—Ç—è")
    print("  - kmeans_iterations.png - —ñ—Ç–µ—Ä–∞—Ü—ñ—ó K-Means")
    print("  - kmeans_convergence.png - –∑–±—ñ–∂–Ω—ñ—Å—Ç—å K-Means")
    print("  - gmm_iterations.png - —ñ—Ç–µ—Ä–∞—Ü—ñ—ó GMM (EM)")
    print("  - gmm_convergence.png - –∑–±—ñ–∂–Ω—ñ—Å—Ç—å GMM")
    print("  - clustering_results_*.png - —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó")
    print("  - results_*.csv - –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É")
    
    print("\n" + "="*70)


if __name__ == "__main__":
    main()

