"""
–ë–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥ –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ñ–≤ –æ–∑–Ω–∞–∫ (Feature Extractors)
–§–æ–∫—É—Å: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è CNN —Ñ—ñ–ª—å—Ç—Ä—ñ–≤, –∞–∫—Ç–∏–≤–∞—Ü—ñ–π, –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —è–∫–æ—Å—Ç—ñ

–ú–µ—Ç–∞: –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ —Ä—ñ–∑–Ω—ñ feature extractors –Ω–∞ —è–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
—Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂.

–ï–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∏:
- –í–ª–∞—Å–Ω–∏–π CNN (–ø—Ä–æ—Å—Ç–∏–π)
- VGG16 (pretrained)
- ResNet50 (pretrained)
- MobileNetV2 (pretrained)
- Autoencoder (–≤–ª–∞—Å–Ω–∏–π)

–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó:
- CNN —Ñ—ñ–ª—å—Ç—Ä–∏ (–ø–µ—Ä—à—ñ —à–∞—Ä–∏)
- –ê–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø—Ä–æ–º—ñ–∂–Ω–∏—Ö —à–∞—Ä—ñ–≤
- PCA (2D/3D –ø—Ä–æ–µ–∫—Ü—ñ—ó)
- t-SNE (2D/3D –ø—Ä–æ–µ–∫—Ü—ñ—ó)

–ú–µ—Ç—Ä–∏–∫–∏:
- Silhouette Score
- Davies-Bouldin Index
- Calinski-Harabasz Index
- Adjusted Rand Index
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2
from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import (
    silhouette_score, davies_bouldin_score, calinski_harabasz_score,
    adjusted_rand_score, normalized_mutual_info_score
)

import time
from pathlib import Path

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è TensorFlow
tf.random.set_seed(42)
np.random.seed(42)

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
sns.set(style="whitegrid", context="notebook")
plt.rcParams["figure.figsize"] = (12, 8)
plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'DejaVu Sans'

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
OUTPUT_DIR = Path("results")
OUTPUT_DIR.mkdir(exist_ok=True)


def print_section(title):
    """–í–∏–≤–æ–¥–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü—ñ—ó"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)


def load_datasets():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞—Ç–∞—Å–µ—Ç–∏ –∑ —Ä—ñ–∑–Ω–∏–º —Ä—ñ–≤–Ω–µ–º —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ"""
    print_section("–ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –î–ê–¢–ê–°–ï–¢–Ü–í")
    
    datasets = {}
    
    # 1. MNIST - –ª–µ–≥–∫–æ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ (10 –∫–ª–∞—Å—ñ–≤ —Ü–∏—Ñ—Ä)
    print("\n[1/3] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è MNIST...")
    (X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()
    
    # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–∞ —Ä–µ—à–µ–π–ø
    X_train_mnist = X_train_mnist.astype('float32') / 255.0
    X_test_mnist = X_test_mnist.astype('float32') / 255.0
    X_train_mnist = np.expand_dims(X_train_mnist, -1)
    X_test_mnist = np.expand_dims(X_test_mnist, -1)
    
    # –í—ñ–∑—å–º–µ–º–æ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω—É –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
    indices_train = np.random.choice(len(X_train_mnist), 6000, replace=False)
    indices_test = np.random.choice(len(X_test_mnist), 2000, replace=False)
    
    datasets['MNIST (–ª–µ–≥–∫–æ)'] = {
        'X_train': X_train_mnist[indices_train],
        'y_train': y_train_mnist[indices_train],
        'X_test': X_test_mnist[indices_test],
        'y_test': y_test_mnist[indices_test],
        'n_classes': 10,
        'shape': (28, 28, 1),
        'description': '–¶–∏—Ñ—Ä–∏ 0-9, —á—ñ—Ç–∫—ñ –≥—Ä–∞–Ω–∏—Ü—ñ –º—ñ–∂ –∫–ª–∞—Å–∞–º–∏'
    }
    print(f"  ‚úì MNIST: train={len(datasets['MNIST (–ª–µ–≥–∫–æ)']['X_train'])}, test={len(datasets['MNIST (–ª–µ–≥–∫–æ)']['X_test'])}")
    
    # 2. Fashion-MNIST - –≤–∞–∂—á–µ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ (10 –∫–ª–∞—Å—ñ–≤ –æ–¥—è–≥—É)
    print("\n[2/3] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Fashion-MNIST...")
    (X_train_fmnist, y_train_fmnist), (X_test_fmnist, y_test_fmnist) = fashion_mnist.load_data()
    
    X_train_fmnist = X_train_fmnist.astype('float32') / 255.0
    X_test_fmnist = X_test_fmnist.astype('float32') / 255.0
    X_train_fmnist = np.expand_dims(X_train_fmnist, -1)
    X_test_fmnist = np.expand_dims(X_test_fmnist, -1)
    
    datasets['Fashion-MNIST (–≤–∞–∂–∫–æ)'] = {
        'X_train': X_train_fmnist[indices_train],
        'y_train': y_train_fmnist[indices_train],
        'X_test': X_test_fmnist[indices_test],
        'y_test': y_test_fmnist[indices_test],
        'n_classes': 10,
        'shape': (28, 28, 1),
        'description': '–û–¥—è–≥, —Å–∫–ª–∞–¥–Ω—ñ—à–∞ —Ç–µ–∫—Å—Ç—É—Ä–∞'
    }
    print(f"  ‚úì Fashion-MNIST: train={len(datasets['Fashion-MNIST (–≤–∞–∂–∫–æ)']['X_train'])}, test={len(datasets['Fashion-MNIST (–≤–∞–∂–∫–æ)']['X_test'])}")
    
    # 3. CIFAR-10 - —Å–µ—Ä–µ–¥–Ω—è —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å, –∫–æ–ª—ñ—Ä–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    print("\n[3/3] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è CIFAR-10...")
    (X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar) = cifar10.load_data()
    
    X_train_cifar = X_train_cifar.astype('float32') / 255.0
    X_test_cifar = X_test_cifar.astype('float32') / 255.0
    y_train_cifar = y_train_cifar.flatten()
    y_test_cifar = y_test_cifar.flatten()
    
    # –ü—ñ–¥–º–Ω–æ–∂–∏–Ω–∞
    indices_train_cifar = np.random.choice(len(X_train_cifar), 5000, replace=False)
    indices_test_cifar = np.random.choice(len(X_test_cifar), 1500, replace=False)
    
    datasets['CIFAR-10 (–∫–æ–ª—ñ—Ä–Ω—ñ)'] = {
        'X_train': X_train_cifar[indices_train_cifar],
        'y_train': y_train_cifar[indices_train_cifar],
        'X_test': X_test_cifar[indices_test_cifar],
        'y_test': y_test_cifar[indices_test_cifar],
        'n_classes': 10,
        'shape': (32, 32, 3),
        'description': '–û–±\'—î–∫—Ç–∏ –≤ –ø—Ä–∏—Ä–æ–¥–Ω–æ–º—É —Å–µ—Ä–µ–¥–æ–≤–∏—â—ñ'
    }
    print(f"  ‚úì CIFAR-10: train={len(datasets['CIFAR-10 (–∫–æ–ª—ñ—Ä–Ω—ñ)']['X_train'])}, test={len(datasets['CIFAR-10 (–∫–æ–ª—ñ—Ä–Ω—ñ)']['X_test'])}")
    
    return datasets


def visualize_datasets(datasets):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ –∫–æ–∂–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É"""
    print_section("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –î–ê–¢–ê–°–ï–¢–Ü–í")
    
    n_datasets = len(datasets)
    fig, axes = plt.subplots(n_datasets, 10, figsize=(15, 3 * n_datasets))
    
    if n_datasets == 1:
        axes = axes.reshape(1, -1)
    
    for row_idx, (dataset_name, data) in enumerate(datasets.items()):
        X_train = data['X_train']
        y_train = data['y_train']
        n_classes = data['n_classes']
        
        # –í—ñ–∑—å–º–µ–º–æ –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–∫–ª–∞–¥—É –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É
        for class_idx in range(min(10, n_classes)):
            ax = axes[row_idx, class_idx]
            
            # –ó–Ω–∞–π–¥–µ–º–æ –ø–µ—Ä—à–∏–π –ø—Ä–∏–∫–ª–∞–¥ —Ü—å–æ–≥–æ –∫–ª–∞—Å—É
            idx = np.where(y_train == class_idx)[0][0]
            img = X_train[idx]
            
            if img.shape[-1] == 1:
                ax.imshow(img.squeeze(), cmap='gray')
            else:
                ax.imshow(img)
            
            ax.set_title(f'–ö–ª–∞—Å {class_idx}', fontsize=9)
            ax.axis('off')
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ä—è–¥–∫–∞
        axes[row_idx, 0].text(-0.5, 0.5, dataset_name, 
                              transform=axes[row_idx, 0].transAxes,
                              fontsize=12, weight='bold', 
                              rotation=90, va='center')
    
    plt.suptitle('–ü—Ä–∏–∫–ª–∞–¥–∏ –∑–æ–±—Ä–∞–∂–µ–Ω—å –∑ –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤', fontsize=14, weight='bold', y=1.00)
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'dataset_samples.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/dataset_samples.png")
    plt.show()


class CustomCNN:
    """–í–ª–∞—Å–Ω–∏–π –ø—Ä–æ—Å—Ç–∏–π CNN –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä"""
    
    def __init__(self, input_shape, n_classes=10, name="CustomCNN"):
        self.input_shape = input_shape
        self.n_classes = n_classes
        self.name = name
        self.model = None
        self.feature_extractor = None
    
    def build(self):
        """–ë—É–¥—É—î –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É CNN"""
        inputs = keras.Input(shape=self.input_shape)
        
        # –ë–ª–æ–∫ 1
        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1')(inputs)
        x = layers.MaxPooling2D((2, 2), name='pool1')(x)
        x = layers.BatchNormalization(name='bn1')(x)
        
        # –ë–ª–æ–∫ 2
        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2')(x)
        x = layers.MaxPooling2D((2, 2), name='pool2')(x)
        x = layers.BatchNormalization(name='bn2')(x)
        
        # –ë–ª–æ–∫ 3
        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3')(x)
        x = layers.GlobalAveragePooling2D(name='gap')(x)
        
        # Feature vector
        features = layers.Dense(256, activation='relu', name='features')(x)
        features = layers.Dropout(0.5)(features)
        
        # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä
        outputs = layers.Dense(self.n_classes, activation='softmax', name='output')(features)
        
        self.model = models.Model(inputs, outputs, name=self.name)
        
        # Feature extractor (–¥–æ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ Dense —à–∞—Ä—É)
        self.feature_extractor = models.Model(
            inputs=self.model.input,
            outputs=self.model.get_layer('features').output
        )
        
        return self
    
    def compile_and_train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=128):
        """–ö–æ–º–ø—ñ–ª—é—î —Ç–∞ –Ω–∞–≤—á–∞—î –º–æ–¥–µ–ª—å"""
        self.model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print(f"\n  –ù–∞–≤—á–∞–Ω–Ω—è {self.name}...")
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            verbose=0
        )
        
        train_acc = history.history['accuracy'][-1]
        val_acc = history.history['val_accuracy'][-1]
        print(f"  ‚úì Train accuracy: {train_acc:.4f}")
        print(f"  ‚úì Val accuracy: {val_acc:.4f}")
        
        return history
    
    def extract_features(self, X):
        """–í–∏—Ç—è–≥—É—î –æ–∑–Ω–∞–∫–∏ –∑ –¥–∞–Ω–∏—Ö"""
        return self.feature_extractor.predict(X, verbose=0)


class AutoencoderExtractor:
    """–ê–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä –¥–ª—è –µ–∫—Å—Ç—Ä–∞–∫—Ü—ñ—ó –æ–∑–Ω–∞–∫"""
    
    def __init__(self, input_shape, latent_dim=128, name="Autoencoder"):
        self.input_shape = input_shape
        self.latent_dim = latent_dim
        self.name = name
        self.autoencoder = None
        self.encoder = None
    
    def build(self):
        """–ë—É–¥—É—î –∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä"""
        # Encoder
        encoder_input = keras.Input(shape=self.input_shape)
        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)
        x = layers.MaxPooling2D((2, 2), padding='same')(x)
        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = layers.MaxPooling2D((2, 2), padding='same')(x)
        x = layers.Flatten()(x)
        latent = layers.Dense(self.latent_dim, activation='relu', name='latent')(x)
        
        self.encoder = models.Model(encoder_input, latent, name='encoder')
        
        # Decoder
        latent_inputs = keras.Input(shape=(self.latent_dim,))
        
        # –û–±—á–∏—Å–ª—é—î–º–æ —Ä–æ–∑–º—ñ—Ä –ø—ñ—Å–ª—è pooling
        h = self.input_shape[0] // 4
        w = self.input_shape[1] // 4
        
        x = layers.Dense(h * w * 64, activation='relu')(latent_inputs)
        x = layers.Reshape((h, w, 64))(x)
        x = layers.Conv2DTranspose(64, (3, 3), strides=2, activation='relu', padding='same')(x)
        x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='same')(x)
        
        # –§—ñ–Ω–∞–ª—å–Ω–∏–π —à–∞—Ä –º–∞—î –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ input_shape
        decoder_output = layers.Conv2D(self.input_shape[-1], (3, 3), 
                                       activation='sigmoid', padding='same')(x)
        
        decoder = models.Model(latent_inputs, decoder_output, name='decoder')
        
        # –ü–æ–≤–Ω–∏–π –∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä
        autoencoder_output = decoder(latent)
        self.autoencoder = models.Model(encoder_input, autoencoder_output, name=self.name)
        
        return self
    
    def compile_and_train(self, X_train, X_val, epochs=20, batch_size=128):
        """–ö–æ–º–ø—ñ–ª—é—î —Ç–∞ –Ω–∞–≤—á–∞—î –∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä"""
        self.autoencoder.compile(
            optimizer='adam',
            loss='mse'
        )
        
        print(f"\n  –ù–∞–≤—á–∞–Ω–Ω—è {self.name}...")
        history = self.autoencoder.fit(
            X_train, X_train,  # –í—Ö—ñ–¥ = –≤–∏—Ö—ñ–¥
            validation_data=(X_val, X_val),
            epochs=epochs,
            batch_size=batch_size,
            verbose=0
        )
        
        train_loss = history.history['loss'][-1]
        val_loss = history.history['val_loss'][-1]
        print(f"  ‚úì Train loss: {train_loss:.6f}")
        print(f"  ‚úì Val loss: {val_loss:.6f}")
        
        return history
    
    def extract_features(self, X):
        """–í–∏—Ç—è–≥—É—î –æ–∑–Ω–∞–∫–∏ (latent representation)"""
        return self.encoder.predict(X, verbose=0)


def build_pretrained_extractor(model_name, input_shape):
    """–ë—É–¥—É—î feature extractor –Ω–∞ –æ—Å–Ω–æ–≤—ñ pretrained –º–æ–¥–µ–ª—ñ"""
    
    # –î–ª—è grayscale –∑–æ–±—Ä–∞–∂–µ–Ω—å —Ç—Ä–µ–±–∞ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ –≤ RGB
    needs_rgb = (input_shape[-1] == 1)
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ wrapper –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó
    if needs_rgb:
        inputs = keras.Input(shape=input_shape)
        x = layers.Conv2D(3, (1, 1), padding='same')(inputs)  # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤ RGB
    else:
        inputs = keras.Input(shape=input_shape)
        x = inputs
    
    # –†–µ—Å–∞–π–∑ –¥–ª—è pretrained –º–æ–¥–µ–ª–µ–π (–º—ñ–Ω—ñ–º—É–º 32x32)
    target_size = max(32, input_shape[0])
    if input_shape[0] != target_size or input_shape[1] != target_size:
        x = layers.Resizing(target_size, target_size)(x)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ pretrained –º–æ–¥–µ–ª—å
    if model_name == 'VGG16':
        base_model = VGG16(weights='imagenet', include_top=False, 
                          input_shape=(target_size, target_size, 3))
    elif model_name == 'ResNet50':
        base_model = ResNet50(weights='imagenet', include_top=False,
                             input_shape=(target_size, target_size, 3))
    elif model_name == 'MobileNetV2':
        base_model = MobileNetV2(weights='imagenet', include_top=False,
                                input_shape=(target_size, target_size, 3))
    else:
        raise ValueError(f"Unknown model: {model_name}")
    
    base_model.trainable = False  # –ó–∞–º–æ—Ä–æ–∂—É—î–º–æ –≤–∞–≥–∏
    
    # –î–æ–¥–∞—î–º–æ base_model
    x = base_model(x)
    x = layers.GlobalAveragePooling2D()(x)
    
    # Feature extractor
    feature_extractor = models.Model(inputs, x, name=f'{model_name}_extractor')
    
    return feature_extractor


def visualize_conv_filters(model, layer_name, n_filters=32):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î —Ñ—ñ–ª—å—Ç—Ä–∏ –∫–æ–Ω–≤–æ–ª—é—Ü—ñ–π–Ω–æ–≥–æ —à–∞—Ä—É"""
    
    # –û—Ç—Ä–∏–º—É—î–º–æ –≤–∞–≥–∏ —à–∞—Ä—É
    layer = model.get_layer(layer_name)
    filters, biases = layer.get_weights()
    
    # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ñ—ñ–ª—å—Ç—Ä–∏
    f_min, f_max = filters.min(), filters.max()
    filters = (filters - f_min) / (f_max - f_min + 1e-8)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑—É—î–º–æ –ø–µ—Ä—à—ñ n_filters
    n_filters = min(n_filters, filters.shape[-1])
    n_cols = 8
    n_rows = (n_filters + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 1.5))
    axes = axes.flatten()
    
    for i in range(n_filters):
        ax = axes[i]
        f = filters[:, :, :, i]
        
        # –Ø–∫—â–æ —î –∫—ñ–ª—å–∫–∞ –∫–∞–Ω–∞–ª—ñ–≤, –≤—ñ–∑—å–º–µ–º–æ —Å–µ—Ä–µ–¥–Ω—î
        if f.shape[-1] > 1:
            f = f.mean(axis=-1)
        else:
            f = f.squeeze()
        
        ax.imshow(f, cmap='viridis')
        ax.set_title(f'F{i+1}', fontsize=8)
        ax.axis('off')
    
    # –í–∏–º–∏–∫–∞—î–º–æ –∑–∞–π–≤—ñ –æ—Å—ñ
    for i in range(n_filters, len(axes)):
        axes[i].axis('off')
    
    plt.suptitle(f'–§—ñ–ª—å—Ç—Ä–∏ —à–∞—Ä—É: {layer_name}', fontsize=14, weight='bold')
    plt.tight_layout()
    
    return fig


def visualize_feature_maps(model, layer_name, image, n_maps=32):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î feature maps (–∞–∫—Ç–∏–≤–∞—Ü—ñ—ó) –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è"""
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∞–∫—Ç–∏–≤–∞—Ü—ñ–π
    activation_model = models.Model(
        inputs=model.input,
        outputs=model.get_layer(layer_name).output
    )
    
    # –û—Ç—Ä–∏–º—É—î–º–æ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó
    activations = activation_model.predict(np.expand_dims(image, 0), verbose=0)
    activations = activations[0]  # –ü–µ—Ä—à–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    
    # –í—ñ–∑—É–∞–ª—ñ–∑—É—î–º–æ –ø–µ—Ä—à—ñ n_maps
    n_maps = min(n_maps, activations.shape[-1])
    n_cols = 8
    n_rows = (n_maps + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 1.5))
    axes = axes.flatten()
    
    for i in range(n_maps):
        ax = axes[i]
        feature_map = activations[:, :, i]
        
        ax.imshow(feature_map, cmap='viridis')
        ax.set_title(f'Map {i+1}', fontsize=8)
        ax.axis('off')
    
    # –í–∏–º–∏–∫–∞—î–º–æ –∑–∞–π–≤—ñ –æ—Å—ñ
    for i in range(n_maps, len(axes)):
        axes[i].axis('off')
    
    plt.suptitle(f'–ê–∫—Ç–∏–≤–∞—Ü—ñ—ó —à–∞—Ä—É: {layer_name}', fontsize=14, weight='bold')
    plt.tight_layout()
    
    return fig


def visualize_cnn_internals(custom_cnn, X_test):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è CNN"""
    print_section("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø CNN: –§–Ü–õ–¨–¢–†–ò –¢–ê –ê–ö–¢–ò–í–ê–¶–Ü–á")
    
    model = custom_cnn.model
    
    # 1. –í—ñ–∑—É–∞–ª—ñ–∑—É—î–º–æ —Ñ—ñ–ª—å—Ç—Ä–∏ –ø–µ—Ä—à–æ–≥–æ conv —à–∞—Ä—É
    print("\n  –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ñ—ñ–ª—å—Ç—Ä—ñ–≤ conv1...")
    fig1 = visualize_conv_filters(model, 'conv1', n_filters=32)
    plt.savefig(OUTPUT_DIR / 'cnn_filters_conv1.png', dpi=300, bbox_inches='tight')
    print("  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/cnn_filters_conv1.png")
    plt.close()
    
    # 2. –í—ñ–∑—É–∞–ª—ñ–∑—É—î–º–æ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É
    print("\n  –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–∫—Ç–∏–≤–∞—Ü—ñ–π...")
    test_image = X_test[0]
    
    # –ü–æ–∫–∞–∑—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    fig, axes = plt.subplots(1, 4, figsize=(16, 4))
    
    # –û—Ä–∏–≥—ñ–Ω–∞–ª
    if test_image.shape[-1] == 1:
        axes[0].imshow(test_image.squeeze(), cmap='gray')
    else:
        axes[0].imshow(test_image)
    axes[0].set_title('–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è', fontsize=12, weight='bold')
    axes[0].axis('off')
    
    # –ê–∫—Ç–∏–≤–∞—Ü—ñ—ó —Ä—ñ–∑–Ω–∏—Ö —à–∞—Ä—ñ–≤
    layer_names = ['conv1', 'conv2', 'conv3']
    for idx, layer_name in enumerate(layer_names):
        activation_model = models.Model(
            inputs=model.input,
            outputs=model.get_layer(layer_name).output
        )
        activations = activation_model.predict(np.expand_dims(test_image, 0), verbose=0)[0]
        
        # –°–µ—Ä–µ–¥–Ω—î –ø–æ –≤—Å—ñ—Ö feature maps
        mean_activation = activations.mean(axis=-1)
        
        axes[idx + 1].imshow(mean_activation, cmap='viridis')
        axes[idx + 1].set_title(f'–ê–∫—Ç–∏–≤–∞—Ü—ñ—ó: {layer_name}', fontsize=12, weight='bold')
        axes[idx + 1].axis('off')
    
    plt.suptitle('–ü—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–∞ –µ–∫—Å—Ç—Ä–∞–∫—Ü—ñ—è –æ–∑–Ω–∞–∫ —É CNN', fontsize=14, weight='bold')
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'cnn_activations_progression.png', dpi=300, bbox_inches='tight')
    print("  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/cnn_activations_progression.png")
    plt.close()
    
    # 3. –î–µ—Ç–∞–ª—å–Ω—ñ feature maps –¥–ª—è conv1
    fig2 = visualize_feature_maps(model, 'conv1', test_image, n_maps=32)
    plt.savefig(OUTPUT_DIR / 'cnn_feature_maps_conv1.png', dpi=300, bbox_inches='tight')
    print("  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/cnn_feature_maps_conv1.png")
    plt.close()


def apply_pca(features, n_components=2):
    """–ó–∞—Å—Ç–æ—Å–æ–≤—É—î PCA –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ"""
    pca = PCA(n_components=n_components, random_state=42)
    features_pca = pca.fit_transform(features)
    
    explained_variance = pca.explained_variance_ratio_.sum()
    
    return features_pca, explained_variance


def apply_tsne(features, n_components=2, perplexity=30):
    """–ó–∞—Å—Ç–æ—Å–æ–≤—É—î t-SNE –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ"""
    tsne = TSNE(n_components=n_components, random_state=42, 
                perplexity=min(perplexity, len(features) - 1))
    features_tsne = tsne.fit_transform(features)
    
    return features_tsne


def visualize_dimensionality_reduction(features, labels, method_name, extractor_name):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ PCA/t-SNE"""
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    # 2D –ø—Ä–æ–µ–∫—Ü—ñ—è
    ax = axes[0]
    scatter = ax.scatter(features[:, 0], features[:, 1], 
                        c=labels, cmap='tab10', 
                        s=20, alpha=0.6, edgecolors='black', linewidth=0.3)
    ax.set_title(f'{method_name} 2D –ø—Ä–æ–µ–∫—Ü—ñ—è', fontsize=13, weight='bold')
    ax.set_xlabel(f'{method_name}1', fontsize=11)
    ax.set_ylabel(f'{method_name}2', fontsize=11)
    ax.grid(alpha=0.3)
    plt.colorbar(scatter, ax=ax, label='–ö–ª–∞—Å')
    
    # 3D –ø—Ä–æ–µ–∫—Ü—ñ—è (—è–∫—â–æ —î)
    if features.shape[1] >= 3:
        ax = fig.add_subplot(122, projection='3d')
        scatter = ax.scatter(features[:, 0], features[:, 1], features[:, 2],
                           c=labels, cmap='tab10',
                           s=20, alpha=0.6, edgecolors='black', linewidth=0.3)
        ax.set_title(f'{method_name} 3D –ø—Ä–æ–µ–∫—Ü—ñ—è', fontsize=13, weight='bold')
        ax.set_xlabel(f'{method_name}1', fontsize=11)
        ax.set_ylabel(f'{method_name}2', fontsize=11)
        ax.set_zlabel(f'{method_name}3', fontsize=11)
    else:
        # –Ø–∫—â–æ –Ω–µ–º–∞—î 3D, —Ä–æ–±–∏–º–æ –≥—ñ—Å—Ç–æ–≥—Ä–∞–º—É –∫–ª–∞—Å—ñ–≤
        ax = axes[1]
        unique, counts = np.unique(labels, return_counts=True)
        ax.bar(unique, counts, color='steelblue', edgecolor='black')
        ax.set_title('–†–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤', fontsize=13, weight='bold')
        ax.set_xlabel('–ö–ª–∞—Å', fontsize=11)
        ax.set_ylabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤', fontsize=11)
        ax.grid(alpha=0.3, axis='y')
    
    plt.suptitle(f'{method_name}: {extractor_name}', fontsize=14, weight='bold')
    plt.tight_layout()
    
    return fig


def benchmark_extractors(dataset_name, dataset, extractors):
    """–ë–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥ —Ä—ñ–∑–Ω–∏—Ö feature extractors"""
    print(f"\n{'='*80}")
    print(f"  –ë–ï–ù–ß–ú–ê–†–ö–Ü–ù–ì –ï–ö–°–¢–†–ê–ö–¢–û–†–Ü–í: {dataset_name}")
    print('='*80)
    
    X_train = dataset['X_train']
    y_train = dataset['y_train']
    X_test = dataset['X_test']
    y_test = dataset['y_test']
    n_classes = dataset['n_classes']
    
    results = {}
    all_features = {}
    
    for extractor_name, extractor in extractors.items():
        print(f"\n[{list(extractors.keys()).index(extractor_name) + 1}/{len(extractors)}] –ï–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä: {extractor_name}")
        
        start_time = time.time()
        
        # –í–∏—Ç—è–≥—É—î–º–æ –æ–∑–Ω–∞–∫–∏
        features_train = extractor.predict(X_train, verbose=0)
        features_test = extractor.predict(X_test, verbose=0)
        
        extraction_time = time.time() - start_time
        
        # Flatten —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if len(features_train.shape) > 2:
            features_train = features_train.reshape(len(features_train), -1)
            features_test = features_test.reshape(len(features_test), -1)
        
        print(f"  ‚úì –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: {features_train.shape[1]}")
        print(f"  ‚úì –ß–∞—Å –µ–∫—Å—Ç—Ä–∞–∫—Ü—ñ—ó: {extraction_time:.2f}s")
        
        all_features[extractor_name] = (features_test, y_test)
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è –∑ K-Means
        kmeans = KMeans(n_clusters=n_classes, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(features_test)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        try:
            silhouette = silhouette_score(features_test, clusters)
            davies_bouldin = davies_bouldin_score(features_test, clusters)
            calinski = calinski_harabasz_score(features_test, clusters)
            ari = adjusted_rand_score(y_test, clusters)
            nmi = normalized_mutual_info_score(y_test, clusters)
            
            results[extractor_name] = {
                '–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å': features_train.shape[1],
                '–ß–∞—Å (s)': extraction_time,
                'Silhouette': silhouette,
                'Davies-Bouldin': davies_bouldin,
                'Calinski-Harabasz': calinski,
                'ARI': ari,
                'NMI': nmi
            }
            
            print(f"  ‚úì Silhouette: {silhouette:.4f}")
            print(f"  ‚úì ARI: {ari:.4f}")
            print(f"  ‚úì NMI: {nmi:.4f}")
            
        except Exception as e:
            print(f"  ‚ö† –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ –º–µ—Ç—Ä–∏–∫: {e}")
            results[extractor_name] = {
                '–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å': features_train.shape[1],
                '–ß–∞—Å (s)': extraction_time,
                'Silhouette': np.nan,
                'Davies-Bouldin': np.nan,
                'Calinski-Harabasz': np.nan,
                'ARI': np.nan,
                'NMI': np.nan
            }
    
    # –¢–∞–±–ª–∏—Ü—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    print(f"\nüìä –ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∞ —Ç–∞–±–ª–∏—Ü—è:")
    results_df = pd.DataFrame(results).T
    print(results_df.round(4).to_string())
    
    return results_df, all_features


def visualize_all_projections(all_features, dataset_name):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î PCA —Ç–∞ t-SNE –¥–ª—è –≤—Å—ñ—Ö –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ñ–≤"""
    print_section(f"–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø PCA –¢–ê t-SNE: {dataset_name}")
    
    n_extractors = len(all_features)
    
    # PCA –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
    print("\n  –û–±—á–∏—Å–ª–µ–Ω–Ω—è PCA –ø—Ä–æ–µ–∫—Ü—ñ–π...")
    fig_pca, axes_pca = plt.subplots(2, (n_extractors + 1) // 2, 
                                     figsize=(8 * ((n_extractors + 1) // 2), 14))
    axes_pca = axes_pca.flatten()
    
    for idx, (extractor_name, (features, labels)) in enumerate(all_features.items()):
        ax = axes_pca[idx]
        
        # PCA
        features_pca, var_explained = apply_pca(features, n_components=2)
        
        scatter = ax.scatter(features_pca[:, 0], features_pca[:, 1],
                           c=labels, cmap='tab10',
                           s=15, alpha=0.6, edgecolors='black', linewidth=0.2)
        ax.set_title(f'{extractor_name}\nVar explained: {var_explained:.2%}', 
                    fontsize=11, weight='bold')
        ax.set_xlabel('PC1', fontsize=10)
        ax.set_ylabel('PC2', fontsize=10)
        ax.grid(alpha=0.3)
        
        if idx == 0:
            plt.colorbar(scatter, ax=ax, label='–ö–ª–∞—Å')
    
    # –í–∏–º–∏–∫–∞—î–º–æ –∑–∞–π–≤—ñ –æ—Å—ñ
    for idx in range(n_extractors, len(axes_pca)):
        axes_pca[idx].axis('off')
    
    plt.suptitle(f'PCA –ü—Ä–æ–µ–∫—Ü—ñ—ó: {dataset_name}', fontsize=16, weight='bold')
    plt.tight_layout()
    
    safe_name = dataset_name.replace(' ', '_').replace('(', '').replace(')', '')
    plt.savefig(OUTPUT_DIR / f'pca_projections_{safe_name}.png', dpi=300, bbox_inches='tight')
    print(f"  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/pca_projections_{safe_name}.png")
    plt.show()
    
    # t-SNE –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
    print("\n  –û–±—á–∏—Å–ª–µ–Ω–Ω—è t-SNE –ø—Ä–æ–µ–∫—Ü—ñ–π (—Ü–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ —á–∞—Å)...")
    fig_tsne, axes_tsne = plt.subplots(2, (n_extractors + 1) // 2,
                                       figsize=(8 * ((n_extractors + 1) // 2), 14))
    axes_tsne = axes_tsne.flatten()
    
    for idx, (extractor_name, (features, labels)) in enumerate(all_features.items()):
        ax = axes_tsne[idx]
        
        # t-SNE (–±–µ—Ä–µ–º–æ –ø—ñ–¥–≤–∏–±—ñ—Ä–∫—É –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ)
        if len(features) > 1000:
            indices = np.random.choice(len(features), 1000, replace=False)
            features_subset = features[indices]
            labels_subset = labels[indices]
        else:
            features_subset = features
            labels_subset = labels
        
        features_tsne = apply_tsne(features_subset, n_components=2, perplexity=30)
        
        scatter = ax.scatter(features_tsne[:, 0], features_tsne[:, 1],
                           c=labels_subset, cmap='tab10',
                           s=15, alpha=0.6, edgecolors='black', linewidth=0.2)
        ax.set_title(f'{extractor_name}', fontsize=11, weight='bold')
        ax.set_xlabel('t-SNE1', fontsize=10)
        ax.set_ylabel('t-SNE2', fontsize=10)
        ax.grid(alpha=0.3)
        
        if idx == 0:
            plt.colorbar(scatter, ax=ax, label='–ö–ª–∞—Å')
    
    # –í–∏–º–∏–∫–∞—î–º–æ –∑–∞–π–≤—ñ –æ—Å—ñ
    for idx in range(n_extractors, len(axes_tsne)):
        axes_tsne[idx].axis('off')
    
    plt.suptitle(f't-SNE –ü—Ä–æ–µ–∫—Ü—ñ—ó: {dataset_name}', fontsize=16, weight='bold')
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / f'tsne_projections_{safe_name}.png', dpi=300, bbox_inches='tight')
    print(f"  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/tsne_projections_{safe_name}.png")
    plt.show()


def create_comparison_chart(all_results):
    """–°—Ç–≤–æ—Ä—é—î –ø–æ—Ä—ñ–≤–Ω—è–ª—å–Ω—ñ –¥—ñ–∞–≥—Ä–∞–º–∏ –¥–ª—è –≤—Å—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤"""
    print_section("–ó–í–ï–î–ï–ù–ï –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ï–ö–°–¢–†–ê–ö–¢–û–†–Ü–í")
    
    metrics = ['Silhouette', 'ARI', 'NMI']
    n_metrics = len(metrics)
    
    fig, axes = plt.subplots(1, n_metrics, figsize=(7 * n_metrics, 6))
    
    if n_metrics == 1:
        axes = [axes]
    
    for idx, metric in enumerate(metrics):
        ax = axes[idx]
        
        # –ó–±–∏—Ä–∞—î–º–æ –¥–∞–Ω—ñ –¥–ª—è –≤—Å—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
        data_to_plot = []
        labels = []
        
        for dataset_name, results_df in all_results.items():
            if metric in results_df.columns:
                data_to_plot.append(results_df[metric].values)
                labels.append(dataset_name)
        
        if data_to_plot:
            # Box plot
            positions = np.arange(len(data_to_plot))
            bp = ax.boxplot(data_to_plot, positions=positions, 
                          patch_artist=True, widths=0.6)
            
            # –ö–æ–ª—å–æ—Ä–∏
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
            for patch, color in zip(bp['boxes'], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)
            
            ax.set_xticklabels(labels, rotation=15, ha='right')
            ax.set_ylabel(metric, fontsize=12, weight='bold')
            ax.set_title(f'–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è: {metric}', fontsize=13, weight='bold')
            ax.grid(alpha=0.3, axis='y')
    
    plt.suptitle('–ë–µ–Ω—á–º–∞—Ä–∫ —è–∫–æ—Å—Ç—ñ –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ñ–≤ –æ–∑–Ω–∞–∫', fontsize=16, weight='bold')
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'extractors_comparison.png', dpi=300, bbox_inches='tight')
    print("  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/extractors_comparison.png")
    plt.show()


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    print("\n" + "="*80)
    print("  –ë–ï–ù–ß–ú–ê–†–ö–Ü–ù–ì –ï–ö–°–¢–†–ê–ö–¢–û–†–Ü–í –û–ó–ù–ê–ö (FEATURE EXTRACTORS)")
    print("  –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è CNN, PCA, t-SNE —Ç–∞ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —è–∫–æ—Å—Ç—ñ")
    print("="*80)
    
    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
    datasets = load_datasets()
    
    # 2. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
    visualize_datasets(datasets)
    
    # 3. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ñ–≤
    all_results = {}
    
    for dataset_name, dataset in datasets.items():
        print_section(f"–†–û–ë–û–¢–ê –ó –î–ê–¢–ê–°–ï–¢–û–ú: {dataset_name}")
        
        X_train = dataset['X_train']
        y_train = dataset['y_train']
        X_test = dataset['X_test']
        y_test = dataset['y_test']
        input_shape = dataset['shape']
        n_classes = dataset['n_classes']
        
        extractors = {}
        
        # 3.1. –í–ª–∞—Å–Ω–∏–π CNN
        print("\n[1/5] –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–ª–∞—Å–Ω–æ–≥–æ CNN...")
        custom_cnn = CustomCNN(input_shape, n_classes, "CustomCNN")
        custom_cnn.build()
        custom_cnn.compile_and_train(X_train, y_train, X_test, y_test, epochs=5)
        extractors['Custom CNN'] = custom_cnn.feature_extractor
        
        # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—å CNN (—Ç—ñ–ª—å–∫–∏ –¥–ª—è –ø–µ—Ä—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É)
        if dataset_name == list(datasets.keys())[0]:
            visualize_cnn_internals(custom_cnn, X_test)
        
        # 3.2. Autoencoder
        print("\n[2/5] –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Autoencoder...")
        autoencoder = AutoencoderExtractor(input_shape, latent_dim=128, name="Autoencoder")
        autoencoder.build()
        autoencoder.compile_and_train(X_train, X_test, epochs=10)
        extractors['Autoencoder'] = autoencoder.encoder
        
        # 3.3. Pretrained –º–æ–¥–µ–ª—ñ (—Ç—ñ–ª—å–∫–∏ –¥–ª—è CIFAR-10 –∞–±–æ —è–∫—â–æ —Ä–æ–∑–º—ñ—Ä –¥–æ—Å—Ç–∞—Ç–Ω—ñ–π)
        if input_shape[0] >= 28:
            print("\n[3/5] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è VGG16...")
            try:
                vgg16_extractor = build_pretrained_extractor('VGG16', input_shape)
                extractors['VGG16'] = vgg16_extractor
                print("  ‚úì VGG16 –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
            except Exception as e:
                print(f"  ‚ö† –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ VGG16: {e}")
            
            print("\n[4/5] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è ResNet50...")
            try:
                resnet_extractor = build_pretrained_extractor('ResNet50', input_shape)
                extractors['ResNet50'] = resnet_extractor
                print("  ‚úì ResNet50 –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
            except Exception as e:
                print(f"  ‚ö† –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ ResNet50: {e}")
            
            print("\n[5/5] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è MobileNetV2...")
            try:
                mobilenet_extractor = build_pretrained_extractor('MobileNetV2', input_shape)
                extractors['MobileNetV2'] = mobilenet_extractor
                print("  ‚úì MobileNetV2 –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
            except Exception as e:
                print(f"  ‚ö† –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ MobileNetV2: {e}")
        
        # 4. –ë–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥ –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ñ–≤
        results_df, all_features = benchmark_extractors(dataset_name, dataset, extractors)
        all_results[dataset_name] = results_df
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        safe_name = dataset_name.replace(' ', '_').replace('(', '').replace(')', '')
        results_df.to_csv(OUTPUT_DIR / f'benchmark_{safe_name}.csv')
        print(f"\n‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/benchmark_{safe_name}.csv")
        
        # 5. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è PCA —Ç–∞ t-SNE
        visualize_all_projections(all_features, dataset_name)
    
    # 6. –ó–≤–µ–¥–µ–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    create_comparison_chart(all_results)
    
    # –ü—ñ–¥—Å—É–º–æ–∫
    print_section("–ü–Ü–î–°–£–ú–û–ö")
    print("\n‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("\nüìÅ –°—Ç–≤–æ—Ä–µ–Ω—ñ —Ñ–∞–π–ª–∏:")
    print("  - results/dataset_samples.png - –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤")
    print("  - results/cnn_filters_conv1.png - —Ñ—ñ–ª—å—Ç—Ä–∏ –ø–µ—Ä—à–æ–≥–æ —à–∞—Ä—É")
    print("  - results/cnn_activations_progression.png - –ø—Ä–æ–≥—Ä–µ—Å—ñ—è –∞–∫—Ç–∏–≤–∞—Ü—ñ–π")
    print("  - results/cnn_feature_maps_conv1.png - –¥–µ—Ç–∞–ª—å–Ω—ñ feature maps")
    print("  - results/pca_projections_*.png - PCA –ø—Ä–æ–µ–∫—Ü—ñ—ó")
    print("  - results/tsne_projections_*.png - t-SNE –ø—Ä–æ–µ–∫—Ü—ñ—ó")
    print("  - results/extractors_comparison.png - –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ñ–≤")
    print("  - results/benchmark_*.csv - –¥–µ—Ç–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏")
    
    print("\nüìä –ó–≤–µ–¥–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤:")
    for dataset_name, results_df in all_results.items():
        print(f"\n{dataset_name}:")
        print(results_df[['Silhouette', 'ARI', 'NMI']].round(4).to_string())
    
    print("\n" + "="*80)


if __name__ == "__main__":
    main()

