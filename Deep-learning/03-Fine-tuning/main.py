"""
Fine-Tuning –ü–æ–ø–µ—Ä–µ–¥–Ω—å–æ –ù–∞–≤—á–µ–Ω–∏—Ö CNN –Ω–∞ –°–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –ö–ª–∞—Å–∞—Ö
–§–æ–∫—É—Å: –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, confusion matrices, –∞–Ω–∞–ª—ñ–∑ –ø–æ–º–∏–ª–æ–∫

–ú–µ—Ç–∞: –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ —Ä—ñ–∑–Ω—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó fine-tuning –Ω–∞ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å
—Ç–∞ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç–∏–ø–æ–≤—ñ –ø–æ–º–∏–ª–∫–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.

–ú–æ–¥–µ–ª—ñ:
- VGG16 (ImageNet pretrained)
- ResNet50 (ImageNet pretrained)
- MobileNetV2 (ImageNet pretrained)
- EfficientNetB0 (ImageNet pretrained)

–î–∞—Ç–∞—Å–µ—Ç–∏ (—Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó):
- CIFAR-10: –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç (–ª—ñ—Ç–∞–∫, –∞–≤—Ç–æ–º–æ–±—ñ–ª—å, –∫–æ—Ä–∞–±–µ–ª—å, –≤–∞–Ω—Ç–∞–∂—ñ–≤–∫–∞)
- CIFAR-10: –¢–≤–∞—Ä–∏–Ω–∏ (–ø—Ç–∞—Ö, –∫—ñ—Ç, –æ–ª–µ–Ω—å, —Å–æ–±–∞–∫–∞, –∂–∞–±–∞, –∫—ñ–Ω—å)
- CIFAR-100: –û–±–ª–∏—á—á—è –ª—é–¥–µ–π —Ç–∞ —Ç–≤–∞—Ä–∏–Ω
- Fashion-MNIST: –í–∑—É—Ç—Ç—è (—Å–∞–Ω–¥–∞–ª—ñ, –∫—Ä–æ—Å—ñ–≤–∫–∏, —á–µ—Ä–µ–≤–∏–∫–∏)

–°—Ç—Ä–∞—Ç–µ–≥—ñ—ó Fine-Tuning:
1. –ó–∞–º–æ—Ä–æ–∂–µ–Ω—ñ –±–∞–∑–æ–≤—ñ —à–∞—Ä–∏ (—Ç—ñ–ª—å–∫–∏ classifier)
2. –ß–∞—Å—Ç–∫–æ–≤–µ —Ä–æ–∑–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è (–æ—Å—Ç–∞–Ω–Ω—ñ N —à–∞—Ä—ñ–≤)
3. –ü–æ–≤–Ω–µ fine-tuning (–≤—Å—ñ —à–∞—Ä–∏ –∑ low LR)

–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó:
- –ö—Ä–∏–≤—ñ –Ω–∞–≤—á–∞–Ω–Ω—è (accuracy, loss)
- Confusion matrices
- –ù–∞–π—Ç–∏–ø–æ–≤—ñ—à—ñ –ø–æ–º–∏–ª–∫–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
- –ó–º—ñ–Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —É —á–∞—Å—ñ
- –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π fine-tuning
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2, EfficientNetB0
from tensorflow.keras.datasets import cifar10, cifar100, fashion_mnist
from tensorflow.keras.callbacks import Callback

from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report,
    precision_recall_fscore_support
)

import time
from pathlib import Path
from collections import defaultdict

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è TensorFlow
tf.random.set_seed(42)
np.random.seed(42)

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
sns.set(style="whitegrid", context="notebook")
plt.rcParams["figure.figsize"] = (14, 10)
plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'DejaVu Sans'

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
OUTPUT_DIR = Path("results")
OUTPUT_DIR.mkdir(exist_ok=True)


def print_section(title):
    """–í–∏–≤–æ–¥–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü—ñ—ó"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)


def load_specialized_datasets():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞—Ç–∞—Å–µ—Ç–∏ –∑ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏–º–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏"""
    print_section("–ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –°–ü–ï–¶–Ü–ê–õ–Ü–ó–û–í–ê–ù–ò–• –î–ê–¢–ê–°–ï–¢–Ü–í")
    
    datasets = {}
    
    # 1. CIFAR-10: –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç (4 –∫–ª–∞—Å–∏)
    print("\n[1/4] CIFAR-10 –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç...")
    (X_train_c10, y_train_c10), (X_test_c10, y_test_c10) = cifar10.load_data()
    y_train_c10 = y_train_c10.flatten()
    y_test_c10 = y_test_c10.flatten()
    
    # –ö–ª–∞—Å–∏ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É: 0-–ª—ñ—Ç–∞–∫, 1-–∞–≤—Ç–æ–º–æ–±—ñ–ª—å, 8-–∫–æ—Ä–∞–±–µ–ª—å, 9-–≤–∞–Ω—Ç–∞–∂—ñ–≤–∫–∞
    transport_classes = [0, 1, 8, 9]
    transport_mask_train = np.isin(y_train_c10, transport_classes)
    transport_mask_test = np.isin(y_test_c10, transport_classes)
    
    X_transport_train = X_train_c10[transport_mask_train].astype('float32') / 255.0
    X_transport_test = X_test_c10[transport_mask_test].astype('float32') / 255.0
    
    # –ü–µ—Ä–µ–º–∞–ø—É–≤–∞–Ω–Ω—è –º—ñ—Ç–æ–∫ 0-3
    y_transport_train = y_train_c10[transport_mask_train]
    y_transport_test = y_test_c10[transport_mask_test]
    
    label_map_transport = {0: 0, 1: 1, 8: 2, 9: 3}
    y_transport_train = np.array([label_map_transport[y] for y in y_transport_train])
    y_transport_test = np.array([label_map_transport[y] for y in y_transport_test])
    
    # –ü—ñ–¥–≤–∏–±—ñ—Ä–∫–∞ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
    train_idx = np.random.choice(len(X_transport_train), 8000, replace=False)
    test_idx = np.random.choice(len(X_transport_test), 2000, replace=False)
    
    datasets['–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç (CIFAR-10)'] = {
        'X_train': X_transport_train[train_idx],
        'y_train': y_transport_train[train_idx],
        'X_test': X_transport_test[test_idx],
        'y_test': y_transport_test[test_idx],
        'n_classes': 4,
        'class_names': ['–õ—ñ—Ç–∞–∫', '–ê–≤—Ç–æ–º–æ–±—ñ–ª—å', '–ö–æ—Ä–∞–±–µ–ª—å', '–í–∞–Ω—Ç–∞–∂—ñ–≤–∫–∞'],
        'shape': (32, 32, 3),
        'description': '–†—ñ–∑–Ω—ñ –≤–∏–¥–∏ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É'
    }
    print(f"  ‚úì –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç: train={len(datasets['–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç (CIFAR-10)']['X_train'])}, test={len(datasets['–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç (CIFAR-10)']['X_test'])}")
    
    # 2. CIFAR-10: –¢–≤–∞—Ä–∏–Ω–∏ (6 –∫–ª–∞—Å—ñ–≤)
    print("\n[2/4] CIFAR-10 –¢–≤–∞—Ä–∏–Ω–∏...")
    # –ö–ª–∞—Å–∏ —Ç–≤–∞—Ä–∏–Ω: 2-–ø—Ç–∞—Ö, 3-–∫—ñ—Ç, 4-–æ–ª–µ–Ω—å, 5-—Å–æ–±–∞–∫–∞, 6-–∂–∞–±–∞, 7-–∫—ñ–Ω—å
    animal_classes = [2, 3, 4, 5, 6, 7]
    animal_mask_train = np.isin(y_train_c10, animal_classes)
    animal_mask_test = np.isin(y_test_c10, animal_classes)
    
    X_animal_train = X_train_c10[animal_mask_train].astype('float32') / 255.0
    X_animal_test = X_test_c10[animal_mask_test].astype('float32') / 255.0
    
    y_animal_train = y_train_c10[animal_mask_train]
    y_animal_test = y_test_c10[animal_mask_test]
    
    label_map_animal = {2: 0, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5}
    y_animal_train = np.array([label_map_animal[y] for y in y_animal_train])
    y_animal_test = np.array([label_map_animal[y] for y in y_animal_test])
    
    # –ü—ñ–¥–≤–∏–±—ñ—Ä–∫–∞
    train_idx = np.random.choice(len(X_animal_train), 10000, replace=False)
    test_idx = np.random.choice(len(X_animal_test), 2500, replace=False)
    
    datasets['–¢–≤–∞—Ä–∏–Ω–∏ (CIFAR-10)'] = {
        'X_train': X_animal_train[train_idx],
        'y_train': y_animal_train[train_idx],
        'X_test': X_animal_test[test_idx],
        'y_test': y_animal_test[test_idx],
        'n_classes': 6,
        'class_names': ['–ü—Ç–∞—Ö', '–ö—ñ—Ç', '–û–ª–µ–Ω—å', '–°–æ–±–∞–∫–∞', '–ñ–∞–±–∞', '–ö—ñ–Ω—å'],
        'shape': (32, 32, 3),
        'description': '–†—ñ–∑–Ω—ñ –≤–∏–¥–∏ —Ç–≤–∞—Ä–∏–Ω'
    }
    print(f"  ‚úì –¢–≤–∞—Ä–∏–Ω–∏: train={len(datasets['–¢–≤–∞—Ä–∏–Ω–∏ (CIFAR-10)']['X_train'])}, test={len(datasets['–¢–≤–∞—Ä–∏–Ω–∏ (CIFAR-10)']['X_test'])}")
    
    # 3. Fashion-MNIST: –í–∑—É—Ç—Ç—è (3 –∫–ª–∞—Å–∏)
    print("\n[3/4] Fashion-MNIST –í–∑—É—Ç—Ç—è...")
    (X_train_fm, y_train_fm), (X_test_fm, y_test_fm) = fashion_mnist.load_data()
    
    # –ö–ª–∞—Å–∏ –≤–∑—É—Ç—Ç—è: 5-—Å–∞–Ω–¥–∞–ª—ñ, 7-–∫—Ä–æ—Å—ñ–≤–∫–∏, 9-—á–µ—Ä–µ–≤–∏–∫–∏
    footwear_classes = [5, 7, 9]
    footwear_mask_train = np.isin(y_train_fm, footwear_classes)
    footwear_mask_test = np.isin(y_test_fm, footwear_classes)
    
    X_footwear_train = X_train_fm[footwear_mask_train].astype('float32') / 255.0
    X_footwear_test = X_test_fm[footwear_mask_test].astype('float32') / 255.0
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –≤ RGB (–ø–æ–≤—Ç–æ—Ä—é—î–º–æ –∫–∞–Ω–∞–ª–∏)
    X_footwear_train = np.repeat(X_footwear_train[..., np.newaxis], 3, axis=-1)
    X_footwear_test = np.repeat(X_footwear_test[..., np.newaxis], 3, axis=-1)
    
    y_footwear_train = y_train_fm[footwear_mask_train]
    y_footwear_test = y_test_fm[footwear_mask_test]
    
    label_map_footwear = {5: 0, 7: 1, 9: 2}
    y_footwear_train = np.array([label_map_footwear[y] for y in y_footwear_train])
    y_footwear_test = np.array([label_map_footwear[y] for y in y_footwear_test])
    
    # –ü—ñ–¥–≤–∏–±—ñ—Ä–∫–∞
    train_idx = np.random.choice(len(X_footwear_train), 10000, replace=False)
    test_idx = np.random.choice(len(X_footwear_test), 2500, replace=False)
    
    datasets['–í–∑—É—Ç—Ç—è (Fashion-MNIST)'] = {
        'X_train': X_footwear_train[train_idx],
        'y_train': y_footwear_train[train_idx],
        'X_test': X_footwear_test[test_idx],
        'y_test': y_footwear_test[test_idx],
        'n_classes': 3,
        'class_names': ['–°–∞–Ω–¥–∞–ª—ñ', '–ö—Ä–æ—Å—ñ–≤–∫–∏', '–ß–µ—Ä–µ–≤–∏–∫–∏'],
        'shape': (28, 28, 3),
        'description': '–†—ñ–∑–Ω—ñ –≤–∏–¥–∏ –≤–∑—É—Ç—Ç—è'
    }
    print(f"  ‚úì –í–∑—É—Ç—Ç—è: train={len(datasets['–í–∑—É—Ç—Ç—è (Fashion-MNIST)']['X_train'])}, test={len(datasets['–í–∑—É—Ç—Ç—è (Fashion-MNIST)']['X_test'])}")
    
    # 4. CIFAR-100: –û–±–ª–∏—á—á—è (—Ç—ñ–ª—å–∫–∏ "baby", "boy", "girl", "man", "woman")
    print("\n[4/4] CIFAR-100 –õ—é–¥–∏...")
    (X_train_c100, y_train_c100), (X_test_c100, y_test_c100) = cifar100.load_data(label_mode='fine')
    y_train_c100 = y_train_c100.flatten()
    y_test_c100 = y_test_c100.flatten()
    
    # –ö–ª–∞—Å–∏ –ª—é–¥–µ–π: baby(2), boy(11), girl(35), man(46), woman(98)
    people_classes = [2, 11, 35, 46, 98]
    people_mask_train = np.isin(y_train_c100, people_classes)
    people_mask_test = np.isin(y_test_c100, people_classes)
    
    X_people_train = X_train_c100[people_mask_train].astype('float32') / 255.0
    X_people_test = X_test_c100[people_mask_test].astype('float32') / 255.0
    
    y_people_train = y_train_c100[people_mask_train]
    y_people_test = y_test_c100[people_mask_test]
    
    label_map_people = {2: 0, 11: 1, 35: 2, 46: 3, 98: 4}
    y_people_train = np.array([label_map_people[y] for y in y_people_train])
    y_people_test = np.array([label_map_people[y] for y in y_people_test])
    
    datasets['–õ—é–¥–∏ (CIFAR-100)'] = {
        'X_train': X_people_train,
        'y_train': y_people_train,
        'X_test': X_people_test,
        'y_test': y_people_test,
        'n_classes': 5,
        'class_names': ['–ù–µ–º–æ–≤–ª—è', '–•–ª–æ–ø—á–∏–∫', '–î—ñ–≤—á–∏–Ω–∫–∞', '–ß–æ–ª–æ–≤—ñ–∫', '–ñ—ñ–Ω–∫–∞'],
        'shape': (32, 32, 3),
        'description': '–û–±–ª–∏—á—á—è —Ä—ñ–∑–Ω–∏—Ö –ª—é–¥–µ–π'
    }
    print(f"  ‚úì –õ—é–¥–∏: train={len(datasets['–õ—é–¥–∏ (CIFAR-100)']['X_train'])}, test={len(datasets['–õ—é–¥–∏ (CIFAR-100)']['X_test'])}")
    
    return datasets


def load_two_class_car_vs_shoes():
    """
    –°–ø—Ä–æ—â–µ–Ω–∏–π 2-–∫–ª–∞—Å–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç:
    - –ö–ª–∞—Å 0: –ê–≤—Ç–æ–º–æ–±—ñ–ª—ñ (CIFAR-10, –∫–ª–∞—Å 1)
    - –ö–ª–∞—Å 1: –í–∑—É—Ç—Ç—è (Fashion-MNIST, –∫–ª–∞—Å 7 ‚Äì –∫—Ä–æ—Å—ñ–≤–∫–∏)

    –£—Å—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø—Ä–∏–≤–æ–¥–∏–º–æ –¥–æ —Ñ–æ—Ä–º–∞—Ç—É 32x32x3 —Ç–∞ –º–∞—Å—à—Ç–∞–±—É—î–º–æ –¥–æ [0, 1].
    –î–ª—è –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –±–µ—Ä–µ–º–æ –Ω–µ–≤–µ–ª–∏–∫—É –ø—ñ–¥–≤–∏–±—ñ—Ä–∫—É –∑ –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É.
    """
    print_section("–ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –°–ü–†–û–©–ï–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–£: –ê–í–¢–û–ú–û–ë–Ü–õ–Ü VS –í–ó–£–¢–¢–Ø")

    # 1. CIFAR-10: –ê–≤—Ç–æ–º–æ–±—ñ–ª—ñ
    (X_train_c10, y_train_c10), (X_test_c10, y_test_c10) = cifar10.load_data()
    y_train_c10 = y_train_c10.flatten()
    y_test_c10 = y_test_c10.flatten()

    car_class = 1  # automobile
    car_mask_train = y_train_c10 == car_class
    car_mask_test = y_test_c10 == car_class

    X_car_train = X_train_c10[car_mask_train].astype('float32') / 255.0
    X_car_test = X_test_c10[car_mask_test].astype('float32') / 255.0

    # 2. Fashion-MNIST: –í–∑—É—Ç—Ç—è (–∫—Ä–æ—Å—ñ–≤–∫–∏)
    (X_train_fm, y_train_fm), (X_test_fm, y_test_fm) = fashion_mnist.load_data()

    shoe_class = 7  # sneaker
    shoe_mask_train = y_train_fm == shoe_class
    shoe_mask_test = y_test_fm == shoe_class

    X_shoe_train = X_train_fm[shoe_mask_train].astype('float32') / 255.0
    X_shoe_test = X_test_fm[shoe_mask_test].astype('float32') / 255.0

    # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–∑—É—Ç—Ç—è –≤ 3 –∫–∞–Ω–∞–ª–∏ —Ç–∞ resize –¥–æ 32x32 –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ CIFAR-10
    X_shoe_train = np.repeat(X_shoe_train[..., np.newaxis], 3, axis=-1)
    X_shoe_test = np.repeat(X_shoe_test[..., np.newaxis], 3, axis=-1)

    X_shoe_train = tf.image.resize(X_shoe_train, [32, 32]).numpy()
    X_shoe_test = tf.image.resize(X_shoe_test, [32, 32]).numpy()

    # –î–ª—è –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –±–µ—Ä–µ–º–æ –æ–±–º–µ–∂–µ–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –∑ –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É
    n_train_per_class = min(2000, len(X_car_train), len(X_shoe_train))
    n_test_per_class = min(500, len(X_car_test), len(X_shoe_test))

    rng = np.random.default_rng(42)

    car_train_idx = rng.choice(len(X_car_train), n_train_per_class, replace=False)
    shoe_train_idx = rng.choice(len(X_shoe_train), n_train_per_class, replace=False)
    car_test_idx = rng.choice(len(X_car_test), n_test_per_class, replace=False)
    shoe_test_idx = rng.choice(len(X_shoe_test), n_test_per_class, replace=False)

    X_train = np.concatenate(
        [X_car_train[car_train_idx], X_shoe_train[shoe_train_idx]], axis=0
    )
    y_train = np.array(
        [0] * n_train_per_class + [1] * n_train_per_class, dtype=np.int64
    )

    X_test = np.concatenate(
        [X_car_test[car_test_idx], X_shoe_test[shoe_test_idx]], axis=0
    )
    y_test = np.array(
        [0] * n_test_per_class + [1] * n_test_per_class, dtype=np.int64
    )

    # –ü–µ—Ä–µ–º—ñ—à—É–≤–∞–Ω–Ω—è
    train_perm = rng.permutation(len(X_train))
    test_perm = rng.permutation(len(X_test))

    X_train = X_train[train_perm]
    y_train = y_train[train_perm]
    X_test = X_test[test_perm]
    y_test = y_test[test_perm]

    dataset = {
        'X_train': X_train,
        'y_train': y_train,
        'X_test': X_test,
        'y_test': y_test,
        'n_classes': 2,
        'class_names': ['–ê–≤—Ç–æ–º–æ–±—ñ–ª—å', '–í–∑—É—Ç—Ç—è'],
        'shape': (32, 32, 3),
        'description': '–ë—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è: –∞–≤—Ç–æ–º–æ–±—ñ–ª—ñ (CIFAR-10) –ø—Ä–æ—Ç–∏ –≤–∑—É—Ç—Ç—è (Fashion-MNIST)'
    }

    print(f"  ‚úì –ê–≤—Ç–æ–º–æ–±—ñ–ª—ñ: train={n_train_per_class}, test={n_test_per_class}")
    print(f"  ‚úì –í–∑—É—Ç—Ç—è:     train={n_train_per_class}, test={n_test_per_class}")
    print(f"  ‚úì –†–∞–∑–æ–º:      train={len(X_train)}, test={len(X_test)}")

    return dataset


def visualize_datasets(datasets):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ –∫–æ–∂–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É"""
    print_section("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –î–ê–¢–ê–°–ï–¢–Ü–í")
    
    n_datasets = len(datasets)
    fig, axes = plt.subplots(n_datasets, 10, figsize=(18, 3 * n_datasets))
    
    if n_datasets == 1:
        axes = axes.reshape(1, -1)
    
    for row_idx, (dataset_name, data) in enumerate(datasets.items()):
        X_train = data['X_train']
        y_train = data['y_train']
        class_names = data['class_names']
        n_classes = data['n_classes']
        
        # –í—ñ–∑—å–º–µ–º–æ –ø–æ –∫—ñ–ª—å–∫–∞ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –∑ –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É
        samples_shown = 0
        for class_idx in range(n_classes):
            # –ó–Ω–∞–π–¥–µ–º–æ –ø—Ä–∏–∫–ª–∞–¥–∏ —Ü—å–æ–≥–æ –∫–ª–∞—Å—É
            indices = np.where(y_train == class_idx)[0]
            
            # –í—ñ–∑—å–º–µ–º–æ –¥–æ 2-3 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –º—ñ—Å—Ü—è
            n_samples = min(2, 10 - samples_shown, len(indices))
            
            for i in range(n_samples):
                if samples_shown >= 10:
                    break
                
                ax = axes[row_idx, samples_shown]
                idx = indices[i]
                img = X_train[idx]
                
                if img.shape[0] == 28:  # Fashion-MNIST
                    # Resize –¥–ª—è –∫—Ä–∞—â–æ—ó –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
                    img_resized = tf.image.resize(img, [32, 32]).numpy()
                    ax.imshow(img_resized)
                else:
                    ax.imshow(img)
                
                ax.set_title(f'{class_names[class_idx]}', fontsize=9)
                ax.axis('off')
                samples_shown += 1
        
        # –í–∏–º–∫–Ω—É—Ç–∏ –∑–∞–π–≤—ñ –æ—Å—ñ
        for i in range(samples_shown, 10):
            axes[row_idx, i].axis('off')
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ä—è–¥–∫–∞
        axes[row_idx, 0].text(-0.3, 0.5, dataset_name,
                              transform=axes[row_idx, 0].transAxes,
                              fontsize=12, weight='bold',
                              rotation=90, va='center')
    
    plt.suptitle('–ü—Ä–∏–∫–ª–∞–¥–∏ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤', fontsize=16, weight='bold', y=1.00)
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'dataset_samples.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/dataset_samples.png")
    plt.show()


class TrainingMonitor(Callback):
    """Callback –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É –∑–º—ñ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è"""
    
    def __init__(self):
        super().__init__()
        self.history = defaultdict(list)
        self.layer_weights_history = defaultdict(list)
        
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        for key, value in logs.items():
            self.history[key].append(value)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–æ—Ä–º–∏ –≤–∞–≥ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
        for layer in self.model.layers:
            if hasattr(layer, 'trainable_weights') and len(layer.trainable_weights) > 0:
                weights = layer.get_weights()[0]  # –ü–µ—Ä—à–∞ –º–∞—Ç—Ä–∏—Ü—è –≤–∞–≥
                weight_norm = np.linalg.norm(weights)
                self.layer_weights_history[layer.name].append(weight_norm)


def build_finetuned_model(base_model_name, input_shape, n_classes, strategy='frozen'):
    """
    –ë—É–¥—É—î –º–æ–¥–µ–ª—å –¥–ª—è fine-tuning
    
    Parameters:
    - base_model_name: 'VGG16', 'ResNet50', 'MobileNetV2', 'EfficientNetB0'
    - input_shape: —Ä–æ–∑–º—ñ—Ä –≤—Ö–æ–¥—É
    - n_classes: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤
    - strategy: 'frozen' (—Ç—ñ–ª—å–∫–∏ classifier), 'partial' (–æ—Å—Ç–∞–Ω–Ω—ñ —à–∞—Ä–∏), 'full' (–≤—Å—ñ —à–∞—Ä–∏)
    """
    
    # –í—Ö—ñ–¥–Ω–∏–π —à–∞—Ä
    inputs = keras.Input(shape=input_shape)
    
    # Resize —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ (pretrained –º–æ–¥–µ–ª—ñ –æ—á—ñ–∫—É—é—Ç—å –º—ñ–Ω—ñ–º—É–º 32x32)
    if input_shape[0] < 32:
        x = layers.Resizing(32, 32)(inputs)
    else:
        x = inputs
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å
    if base_model_name == 'VGG16':
        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))
    elif base_model_name == 'ResNet50':
        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))
    elif base_model_name == 'MobileNetV2':
        base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))
    elif base_model_name == 'EfficientNetB0':
        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))
    else:
        raise ValueError(f"Unknown model: {base_model_name}")
    
    # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é fine-tuning
    if strategy == 'frozen':
        # –ó–∞–º–æ—Ä–æ–∂—É—î–º–æ –≤—Å—ñ –±–∞–∑–æ–≤—ñ —à–∞—Ä–∏
        base_model.trainable = False
    elif strategy == 'partial':
        # –†–æ–∑–º–æ—Ä–æ–∂—É—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ 20% —à–∞—Ä—ñ–≤
        base_model.trainable = True
        n_layers = len(base_model.layers)
        for layer in base_model.layers[:int(n_layers * 0.8)]:
            layer.trainable = False
    elif strategy == 'full':
        # –†–æ–∑–º–æ—Ä–æ–∂—É—î–º–æ –≤—Å—ñ —à–∞—Ä–∏
        base_model.trainable = True
    
    # –î–æ–¥–∞—î–º–æ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å
    x = base_model(x, training=False if strategy == 'frozen' else True)
    
    # Classifier head
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(n_classes, activation='softmax')(x)
    
    model = models.Model(inputs, outputs, name=f'{base_model_name}_{strategy}')
    
    return model


def train_model(model, X_train, y_train, X_val, y_val, strategy='frozen', epochs=20):
    """–ù–∞–≤—á–∞—î –º–æ–¥–µ–ª—å –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—é —Å—Ç—Ä–∞—Ç–µ–≥—ñ—î—é"""
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ learning rate –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
    if strategy == 'frozen':
        lr = 1e-3
    elif strategy == 'partial':
        lr = 1e-4
    else:  # full
        lr = 1e-5
    
    # –ö–æ–º–ø—ñ–ª—é—î–º–æ –º–æ–¥–µ–ª—å
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=lr),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # Callbacks
    monitor = TrainingMonitor()
    early_stop = keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True
    )
    
    reduce_lr = keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=1e-7,
        verbose=0
    )
    
    # –ù–∞–≤—á–∞–Ω–Ω—è
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=64,
        callbacks=[monitor, early_stop, reduce_lr],
        verbose=0
    )
    
    return history, monitor


def evaluate_model(model, X_test, y_test, class_names):
    """–û—Ü—ñ–Ω—é—î –º–æ–¥–µ–ª—å —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î –º–µ—Ç—Ä–∏–∫–∏"""
    
    # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
    y_pred_proba = model.predict(X_test, verbose=0)
    y_pred = np.argmax(y_pred_proba, axis=1)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(y_test, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_test, y_pred, average='weighted', zero_division=0
    )
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    
    # Classification report
    report = classification_report(
        y_test, y_pred, target_names=class_names, zero_division=0
    )
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm,
        'predictions': y_pred,
        'probabilities': y_pred_proba,
        'report': report
    }


def visualize_training_history(all_histories, dataset_name):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î —ñ—Å—Ç–æ—Ä—ñ—é –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π"""
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Training Accuracy
    ax = axes[0, 0]
    for model_name, (history, _) in all_histories.items():
        epochs = range(1, len(history.history['accuracy']) + 1)
        ax.plot(epochs, history.history['accuracy'], 
                marker='o', markersize=4, label=model_name, linewidth=2)
    ax.set_title('–¢–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ –Ω–∞–≤—á–∞–ª—å–Ω—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ', fontsize=13, weight='bold')
    ax.set_xlabel('–ï–ø–æ—Ö–∞', fontsize=11)
    ax.set_ylabel('Accuracy', fontsize=11)
    ax.legend(fontsize=9)
    ax.grid(alpha=0.3)
    
    # 2. Validation Accuracy
    ax = axes[0, 1]
    for model_name, (history, _) in all_histories.items():
        epochs = range(1, len(history.history['val_accuracy']) + 1)
        ax.plot(epochs, history.history['val_accuracy'],
                marker='s', markersize=4, label=model_name, linewidth=2)
    ax.set_title('–¢–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ', fontsize=13, weight='bold')
    ax.set_xlabel('–ï–ø–æ—Ö–∞', fontsize=11)
    ax.set_ylabel('Validation Accuracy', fontsize=11)
    ax.legend(fontsize=9)
    ax.grid(alpha=0.3)
    
    # 3. Training Loss
    ax = axes[1, 0]
    for model_name, (history, _) in all_histories.items():
        epochs = range(1, len(history.history['loss']) + 1)
        ax.plot(epochs, history.history['loss'],
                marker='o', markersize=4, label=model_name, linewidth=2)
    ax.set_title('Loss –Ω–∞ –Ω–∞–≤—á–∞–ª—å–Ω—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ', fontsize=13, weight='bold')
    ax.set_xlabel('–ï–ø–æ—Ö–∞', fontsize=11)
    ax.set_ylabel('Loss', fontsize=11)
    ax.legend(fontsize=9)
    ax.grid(alpha=0.3)
    
    # 4. Validation Loss
    ax = axes[1, 1]
    for model_name, (history, _) in all_histories.items():
        epochs = range(1, len(history.history['val_loss']) + 1)
        ax.plot(epochs, history.history['val_loss'],
                marker='s', markersize=4, label=model_name, linewidth=2)
    ax.set_title('Loss –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ', fontsize=13, weight='bold')
    ax.set_xlabel('–ï–ø–æ—Ö–∞', fontsize=11)
    ax.set_ylabel('Validation Loss', fontsize=11)
    ax.legend(fontsize=9)
    ax.grid(alpha=0.3)
    
    plt.suptitle(f'–Ü—Å—Ç–æ—Ä—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è: {dataset_name}', fontsize=16, weight='bold')
    plt.tight_layout()
    
    safe_name = dataset_name.replace(' ', '_').replace('(', '').replace(')', '')
    plt.savefig(OUTPUT_DIR / f'training_history_{safe_name}.png', dpi=300, bbox_inches='tight')
    print(f"  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/training_history_{safe_name}.png")
    plt.show()


def visualize_weight_changes(all_histories, dataset_name):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î –∑–º—ñ–Ω—É –Ω–æ—Ä–º –≤–∞–≥ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è (—ñ–ª—é—Å—Ç—Ä–∞—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó)"""
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.flatten()
    
    model_idx = 0
    for model_name, (history, monitor) in all_histories.items():
        if model_idx >= 4:
            break
        
        ax = axes[model_idx]
        
        # –í—ñ–∑—É–∞–ª—ñ–∑—É—î–º–æ –ø–µ—Ä—à—ñ 5 —à–∞—Ä—ñ–≤ –∑ –≤–∞–≥–∞–º–∏
        layer_count = 0
        for layer_name, weight_norms in monitor.layer_weights_history.items():
            if layer_count >= 5:
                break
            if len(weight_norms) > 0:
                epochs = range(1, len(weight_norms) + 1)
                ax.plot(epochs, weight_norms, marker='o', markersize=3,
                       label=layer_name[:20], linewidth=2, alpha=0.7)
                layer_count += 1
        
        ax.set_title(f'–ó–º—ñ–Ω–∞ –≤–∞–≥: {model_name}', fontsize=12, weight='bold')
        ax.set_xlabel('–ï–ø–æ—Ö–∞', fontsize=10)
        ax.set_ylabel('L2 –Ω–æ—Ä–º–∞ –≤–∞–≥', fontsize=10)
        ax.legend(fontsize=8, loc='best')
        ax.grid(alpha=0.3)
        
        model_idx += 1
    
    # –í–∏–º–∫–Ω—É—Ç–∏ –∑–∞–π–≤—ñ –æ—Å—ñ
    for i in range(model_idx, 4):
        axes[i].axis('off')
    
    plt.suptitle(f'–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {dataset_name}', 
                 fontsize=16, weight='bold')
    plt.tight_layout()
    
    safe_name = dataset_name.replace(' ', '_').replace('(', '').replace(')', '')
    plt.savefig(OUTPUT_DIR / f'weight_optimization_{safe_name}.png', dpi=300, bbox_inches='tight')
    print(f"  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/weight_optimization_{safe_name}.png")
    plt.show()


def visualize_confusion_matrices(all_results, class_names, dataset_name):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î confusion matrices –¥–ª—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π"""
    
    n_models = len(all_results)
    n_cols = 2
    n_rows = (n_models + 1) // 2
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 6 * n_rows))

    # –ü—Ä–∏–≤–æ–¥–∏–º–æ axes –¥–æ –ø–ª–∞—Å–∫–æ–≥–æ —Å–ø–∏—Å–∫—É Axes, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –º–æ–¥–µ–ª–µ–π –æ–¥–Ω–∞
    if isinstance(axes, np.ndarray):
        axes = axes.flatten()
    else:
        axes = [axes]
    
    for idx, (model_name, results) in enumerate(all_results.items()):
        ax = axes[idx]
        
        cm = results['confusion_matrix']
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ confusion matrix
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                   xticklabels=class_names, yticklabels=class_names,
                   ax=ax, cbar_kws={'label': '–ß–∞—Å—Ç–∫–∞'}, vmin=0, vmax=1)
        
        ax.set_title(f'{model_name}\nAccuracy: {results["accuracy"]:.4f}',
                    fontsize=12, weight='bold')
        ax.set_xlabel('–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–∏–π –∫–ª–∞—Å', fontsize=10)
        ax.set_ylabel('–°–ø—Ä–∞–≤–∂–Ω—ñ–π –∫–ª–∞—Å', fontsize=10)
        
        # –ü–æ–≤–æ—Ä–æ—Ç –º—ñ—Ç–æ–∫
        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)
    
    # –í–∏–º–∫–Ω—É—Ç–∏ –∑–∞–π–≤—ñ –æ—Å—ñ
    for i in range(len(all_results), len(axes)):
        axes[i].axis('off')
    
    plt.suptitle(f'Confusion Matrices: {dataset_name}', fontsize=16, weight='bold')
    plt.tight_layout()
    
    safe_name = dataset_name.replace(' ', '_').replace('(', '').replace(')', '')
    plt.savefig(OUTPUT_DIR / f'confusion_matrices_{safe_name}.png', dpi=300, bbox_inches='tight')
    print(f"  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/confusion_matrices_{safe_name}.png")
    plt.show()


def analyze_classification_errors(X_test, y_test, y_pred, y_proba, class_names, dataset_name, model_name):
    """–ê–Ω–∞–ª—ñ–∑—É—î —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑—É—î –Ω–∞–π—Ç–∏–ø–æ–≤—ñ—à—ñ –ø–æ–º–∏–ª–∫–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó"""
    
    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –ø–æ–º–∏–ª–∫–∏
    errors = y_test != y_pred
    error_indices = np.where(errors)[0]
    
    if len(error_indices) == 0:
        print(f"  –ù–µ–º–∞—î –ø–æ–º–∏–ª–æ–∫ –¥–ª—è {model_name}!")
        return
    
    # –î–ª—è –∫–æ–∂–Ω–æ—ó –ø–æ–º–∏–ª–∫–∏ –æ–±—á–∏—Å–ª—é—î–º–æ "–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å" —É –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—ñ
    error_confidences = []
    for idx in error_indices:
        confidence = y_proba[idx, y_pred[idx]]
        error_confidences.append((idx, confidence, y_test[idx], y_pred[idx]))
    
    # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—é (–≤—ñ–¥ –Ω–∞–π–≤–ø–µ–≤–Ω–µ–Ω—ñ—à–∏—Ö –¥–æ –Ω–∞–π–º–µ–Ω—à)
    error_confidences.sort(key=lambda x: x[1], reverse=True)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑—É—î–º–æ —Ç–æ–ø-20 –Ω–∞–π–≤–ø–µ–≤–Ω–µ–Ω—ñ—à–∏—Ö –ø–æ–º–∏–ª–æ–∫
    n_errors_to_show = min(20, len(error_confidences))
    n_cols = 5
    n_rows = (n_errors_to_show + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 3 * n_rows))
    axes = axes.flatten() if n_errors_to_show > 1 else [axes]
    
    for i in range(n_errors_to_show):
        idx, confidence, true_label, pred_label = error_confidences[i]
        
        ax = axes[i]
        img = X_test[idx]
        
        ax.imshow(img)
        ax.set_title(f'True: {class_names[true_label]}\n'
                    f'Pred: {class_names[pred_label]}\n'
                    f'Conf: {confidence:.2f}',
                    fontsize=9, color='red')
        ax.axis('off')
    
    # –í–∏–º–∫–Ω—É—Ç–∏ –∑–∞–π–≤—ñ –æ—Å—ñ
    for i in range(n_errors_to_show, len(axes)):
        axes[i].axis('off')
    
    plt.suptitle(f'–¢–æ–ø-{n_errors_to_show} –≤–ø–µ–≤–Ω–µ–Ω–∏—Ö –ø–æ–º–∏–ª–æ–∫: {model_name} –Ω–∞ {dataset_name}',
                 fontsize=14, weight='bold')
    plt.tight_layout()
    
    safe_dataset = dataset_name.replace(' ', '_').replace('(', '').replace(')', '')
    safe_model = model_name.replace(' ', '_').replace('(', '').replace(')', '')
    plt.savefig(OUTPUT_DIR / f'errors_{safe_dataset}_{safe_model}.png', 
                dpi=300, bbox_inches='tight')
    print(f"  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/errors_{safe_dataset}_{safe_model}.png")
    plt.show()


def compare_strategies(dataset, dataset_name):
    """–ü–æ—Ä—ñ–≤–Ω—é—î —Ä—ñ–∑–Ω—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó fine-tuning –Ω–∞ –æ–¥–Ω—ñ–π –º–æ–¥–µ–ª—ñ"""
    print_section(f"–ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –°–¢–†–ê–¢–ï–ì–Ü–ô FINE-TUNING: {dataset_name}")
    
    X_train = dataset['X_train']
    y_train = dataset['y_train']
    X_test = dataset['X_test']
    y_test = dataset['y_test']
    n_classes = dataset['n_classes']
    input_shape = dataset['shape']
    class_names = dataset['class_names']
    
    strategies = ['frozen', 'partial', 'full']
    base_model = 'ResNet50'
    
    results = {}
    
    for strategy in strategies:
        print(f"\n  –°—Ç—Ä–∞—Ç–µ–≥—ñ—è: {strategy}...")
        
        # –ë—É–¥—É—î–º–æ –º–æ–¥–µ–ª—å
        model = build_finetuned_model(base_model, input_shape, n_classes, strategy=strategy)
        
        # –¢—Ä–µ–Ω—É—î–º–æ
        history, monitor = train_model(model, X_train, y_train, X_test, y_test, 
                                      strategy=strategy, epochs=15)
        
        # –û—Ü—ñ–Ω—é—î–º–æ
        eval_results = evaluate_model(model, X_test, y_test, class_names)
        
        results[f'{base_model} ({strategy})'] = {
            'history': history,
            'monitor': monitor,
            'eval': eval_results
        }
        
        print(f"  ‚úì Accuracy: {eval_results['accuracy']:.4f}")
        print(f"  ‚úì F1-Score: {eval_results['f1']:.4f}")
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    # 1. Validation Accuracy
    ax = axes[0]
    for strategy_name, data in results.items():
        history = data['history']
        epochs = range(1, len(history.history['val_accuracy']) + 1)
        ax.plot(epochs, history.history['val_accuracy'], 
               marker='o', linewidth=2, label=strategy_name)
    ax.set_title('Validation Accuracy', fontsize=13, weight='bold')
    ax.set_xlabel('–ï–ø–æ—Ö–∞', fontsize=11)
    ax.set_ylabel('Accuracy', fontsize=11)
    ax.legend()
    ax.grid(alpha=0.3)
    
    # 2. Final Metrics
    ax = axes[1]
    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1']
    x = np.arange(len(strategies))
    width = 0.2
    
    for i, metric_name in enumerate(metrics_names):
        values = [results[f'{base_model} ({s})']['eval'][metric_name.lower()] 
                 for s in strategies]
        ax.bar(x + i * width, values, width, label=metric_name)
    
    ax.set_title('–§—ñ–Ω–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏', fontsize=13, weight='bold')
    ax.set_xlabel('–°—Ç—Ä–∞—Ç–µ–≥—ñ—è', fontsize=11)
    ax.set_ylabel('–ó–Ω–∞—á–µ–Ω–Ω—è', fontsize=11)
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels(strategies)
    ax.legend()
    ax.grid(alpha=0.3, axis='y')
    
    # 3. Training Time (approximation via epochs)
    ax = axes[2]
    n_epochs = [len(results[f'{base_model} ({s})']['history'].history['loss']) 
               for s in strategies]
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    ax.bar(strategies, n_epochs, color=colors, edgecolor='black')
    ax.set_title('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –¥–æ –∑—É–ø–∏–Ω–∫–∏', fontsize=13, weight='bold')
    ax.set_xlabel('–°—Ç—Ä–∞—Ç–µ–≥—ñ—è', fontsize=11)
    ax.set_ylabel('–ï–ø–æ—Ö–∏', fontsize=11)
    ax.grid(alpha=0.3, axis='y')
    
    plt.suptitle(f'–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π Fine-Tuning: {dataset_name}',
                 fontsize=16, weight='bold')
    plt.tight_layout()
    
    safe_name = dataset_name.replace(' ', '_').replace('(', '').replace(')', '')
    plt.savefig(OUTPUT_DIR / f'strategy_comparison_{safe_name}.png', 
                dpi=300, bbox_inches='tight')
    print(f"\n  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/strategy_comparison_{safe_name}.png")
    plt.show()
    
    return results


def benchmark_models(dataset, dataset_name):
    """–ë–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥ —Ä—ñ–∑–Ω–∏—Ö –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä –Ω–∞ –æ–¥–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ"""
    print_section(f"–ë–ï–ù–ß–ú–ê–†–ö–Ü–ù–ì –ú–û–î–ï–õ–ï–ô: {dataset_name}")
    
    X_train = dataset['X_train']
    y_train = dataset['y_train']
    X_test = dataset['X_test']
    y_test = dataset['y_test']
    n_classes = dataset['n_classes']
    input_shape = dataset['shape']
    class_names = dataset['class_names']
    
    # –ú–æ–¥–µ–ª—ñ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    model_names = ['VGG16', 'ResNet50', 'MobileNetV2', 'EfficientNetB0']
    strategy = 'partial'  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ partial fine-tuning
    
    all_results = {}
    all_histories = {}
    training_times = {}
    
    for model_name in model_names:
        print(f"\n  [{model_names.index(model_name) + 1}/{len(model_names)}] –ú–æ–¥–µ–ª—å: {model_name}...")
        
        start_time = time.time()
        
        # –ë—É–¥—É—î–º–æ –º–æ–¥–µ–ª—å
        model = build_finetuned_model(model_name, input_shape, n_classes, strategy=strategy)
        
        # –¢—Ä–µ–Ω—É—î–º–æ
        history, monitor = train_model(model, X_train, y_train, X_test, y_test,
                                      strategy=strategy, epochs=20)
        
        training_time = time.time() - start_time
        training_times[model_name] = training_time
        
        # –û—Ü—ñ–Ω—é—î–º–æ
        eval_results = evaluate_model(model, X_test, y_test, class_names)
        
        all_results[model_name] = eval_results
        all_histories[model_name] = (history, monitor)
        
        print(f"  ‚úì Accuracy: {eval_results['accuracy']:.4f}")
        print(f"  ‚úì F1-Score: {eval_results['f1']:.4f}")
        print(f"  ‚úì –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {training_time:.1f}s")
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
    visualize_training_history(all_histories, dataset_name)
    visualize_weight_changes(all_histories, dataset_name)
    visualize_confusion_matrices(all_results, class_names, dataset_name)
    
    # –ê–Ω–∞–ª—ñ–∑ –ø–æ–º–∏–ª–æ–∫ –¥–ª—è –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ
    print("\n  –ê–Ω–∞–ª—ñ–∑ –ø–æ–º–∏–ª–æ–∫ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó...")
    for model_name, results in all_results.items():
        analyze_classification_errors(
            X_test, y_test, 
            results['predictions'], 
            results['probabilities'],
            class_names, dataset_name, model_name
        )
    
    # –ó–≤–µ–¥–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    print(f"\n  üìä –ó–≤–µ–¥–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤:")
    results_data = []
    for model_name in model_names:
        results_data.append({
            '–ú–æ–¥–µ–ª—å': model_name,
            'Accuracy': all_results[model_name]['accuracy'],
            'Precision': all_results[model_name]['precision'],
            'Recall': all_results[model_name]['recall'],
            'F1-Score': all_results[model_name]['f1'],
            '–ß–∞—Å (s)': training_times[model_name]
        })
    
    df = pd.DataFrame(results_data)
    print(df.round(4).to_string(index=False))
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    safe_name = dataset_name.replace(' ', '_').replace('(', '').replace(')', '')
    df.to_csv(OUTPUT_DIR / f'benchmark_{safe_name}.csv', index=False)
    print(f"\n  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/benchmark_{safe_name}.csv")
    
    return all_results, all_histories


def create_final_comparison(all_datasets_results):
    """–°—Ç–≤–æ—Ä—é—î —Ñ—ñ–Ω–∞–ª—å–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Å—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö"""
    print_section("–§–Ü–ù–ê–õ–¨–ù–ï –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø")
    
    # –ó–±–∏—Ä–∞—î–º–æ –¥–∞–Ω—ñ
    data = []
    for dataset_name, results in all_datasets_results.items():
        for model_name, eval_results in results.items():
            data.append({
                'Dataset': dataset_name.split('(')[0].strip(),
                'Model': model_name,
                'Accuracy': eval_results['accuracy'],
                'F1-Score': eval_results['f1']
            })
    
    df = pd.DataFrame(data)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    fig, axes = plt.subplots(1, 2, figsize=(18, 6))
    
    # 1. Accuracy –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
    ax = axes[0]
    datasets = df['Dataset'].unique()
    models = df['Model'].unique()
    x = np.arange(len(datasets))
    width = 0.2
    
    for i, model in enumerate(models):
        values = [df[(df['Dataset'] == ds) & (df['Model'] == model)]['Accuracy'].values[0]
                 for ds in datasets]
        ax.bar(x + i * width, values, width, label=model)
    
    ax.set_title('Accuracy –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö', fontsize=14, weight='bold')
    ax.set_xlabel('–î–∞—Ç–∞—Å–µ—Ç', fontsize=12)
    ax.set_ylabel('Accuracy', fontsize=12)
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels(datasets, rotation=15, ha='right')
    ax.legend()
    ax.grid(alpha=0.3, axis='y')
    
    # 2. F1-Score –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
    ax = axes[1]
    for i, model in enumerate(models):
        values = [df[(df['Dataset'] == ds) & (df['Model'] == model)]['F1-Score'].values[0]
                 for ds in datasets]
        ax.bar(x + i * width, values, width, label=model)
    
    ax.set_title('F1-Score –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö', fontsize=14, weight='bold')
    ax.set_xlabel('–î–∞—Ç–∞—Å–µ—Ç', fontsize=12)
    ax.set_ylabel('F1-Score', fontsize=12)
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels(datasets, rotation=15, ha='right')
    ax.legend()
    ax.grid(alpha=0.3, axis='y')
    
    plt.suptitle('–§—ñ–Ω–∞–ª—å–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π', fontsize=16, weight='bold')
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'final_comparison.png', dpi=300, bbox_inches='tight')
    print("  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/final_comparison.png")
    plt.show()
    
    # –¢–∞–±–ª–∏—Ü—è
    print("\n  üìä –ó–≤–µ–¥–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü—è –≤—Å—ñ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤:")
    pivot_acc = df.pivot(index='Dataset', columns='Model', values='Accuracy')
    print("\nAccuracy:")
    print(pivot_acc.round(4).to_string())
    
    pivot_f1 = df.pivot(index='Dataset', columns='Model', values='F1-Score')
    print("\nF1-Score:")
    print(pivot_f1.round(4).to_string())
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    pivot_acc.to_csv(OUTPUT_DIR / 'final_accuracy_comparison.csv')
    pivot_f1.to_csv(OUTPUT_DIR / 'final_f1_comparison.csv')
    print("\n  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/final_accuracy_comparison.csv")
    print("  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/final_f1_comparison.csv")


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è (—Å–ø—Ä–æ—â–µ–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç: 2 –∫–ª–∞—Å–∏, 1 –º–æ–¥–µ–ª—å)"""
    print("\n" + "=" * 80)
    print("  FINE-TUNING PRETRAINED CNN (–°–ü–†–û–©–ï–ù–û)")
    print("  –ó–∞–¥–∞—á–∞: –ê–≤—Ç–æ–º–æ–±—ñ–ª—ñ vs –í–∑—É—Ç—Ç—è, 2 –∫–ª–∞—Å–∏, 1 –º–æ–¥–µ–ª—å")
    print("=" * 80)

    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å–ø—Ä–æ—â–µ–Ω–æ–≥–æ 2-–∫–ª–∞—Å–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
    dataset = load_two_class_car_vs_shoes()
    dataset_name = "–ê–≤—Ç–æ–º–æ–±—ñ–ª—ñ vs –í–∑—É—Ç—Ç—è"

    # 2. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–µ–∫—ñ–ª—å–∫–æ—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
    visualize_datasets({dataset_name: dataset})

    X_train = dataset['X_train']
    y_train = dataset['y_train']
    X_test = dataset['X_test']
    y_test = dataset['y_test']
    input_shape = dataset['shape']
    class_names = dataset['class_names']

    # 3. –û–¥–Ω–∞ —à–≤–∏–¥–∫–∞ –º–æ–¥–µ–ª—å + –æ–¥–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è fine-tuning
    base_model_name = 'MobileNetV2'  # –±—ñ–ª—å—à –ª–µ–≥–∫–∞ –π —à–≤–∏–¥–∫–∞ –º–æ–¥–µ–ª—å
    strategy = 'partial'             # —Ä–æ–∑–º–æ—Ä–æ–∂—É—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ —à–∞—Ä–∏

    print_section(f"–ù–ê–í–ß–ê–ù–ù–Ø –ú–û–î–ï–õ–Ü: {base_model_name} ({strategy})")
    model = build_finetuned_model(base_model_name, input_shape, dataset['n_classes'], strategy=strategy)

    # –©–æ–± —â–µ –ø—Ä–∏—Å–∫–æ—Ä–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è, –∑–º–µ–Ω—à–∏–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö
    history, monitor = train_model(
        model,
        X_train, y_train,
        X_test, y_test,          # –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–µ—Å—Ç —è–∫ –≤–∞–ª—ñ–¥–∞—Ü—ñ—é –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏
        strategy=strategy,
        epochs=10
    )

    # 4. –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
    eval_results = evaluate_model(model, X_test, y_test, class_names)

    print("\nüìä –ü–Ü–î–°–£–ú–ö–û–í–Ü –ú–ï–¢–†–ò–ö–ò:")
    print(f"  Accuracy : {eval_results['accuracy']:.4f}")
    print(f"  Precision: {eval_results['precision']:.4f}")
    print(f"  Recall   : {eval_results['recall']:.4f}")
    print(f"  F1-Score : {eval_results['f1']:.4f}")
    print("\n–î–æ–∫–ª–∞–¥–Ω–∏–π –∑–≤—ñ—Ç –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó:")
    print(eval_results['report'])

    # 5. –û—Å–Ω–æ–≤–Ω—ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–ª—è –æ–¥–Ω—ñ—î—ó –º–æ–¥–µ–ª—ñ
    all_histories = {f'{base_model_name} ({strategy})': (history, monitor)}
    all_results = {f'{base_model_name} ({strategy})': eval_results}

    visualize_training_history(all_histories, dataset_name)
    visualize_weight_changes(all_histories, dataset_name)
    visualize_confusion_matrices(all_results, class_names, dataset_name)

    print("\n  –ê–Ω–∞–ª—ñ–∑ –ø–æ–º–∏–ª–æ–∫ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó...")
    analyze_classification_errors(
        X_test, y_test,
        eval_results['predictions'],
        eval_results['probabilities'],
        class_names,
        dataset_name,
        f'{base_model_name} ({strategy})'
    )

    # –ö–æ—Ä–æ—Ç–∫–∏–π –ø—ñ–¥—Å—É–º–æ–∫
    print_section("–ü–Ü–î–°–£–ú–û–ö (–°–ü–†–û–©–ï–ù–ò–ô –°–¶–ï–ù–ê–†–Ü–ô)")
    print("\n‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("\nüìÅ –°—Ç–≤–æ—Ä–µ–Ω—ñ –æ—Å–Ω–æ–≤–Ω—ñ —Ñ–∞–π–ª–∏ (–¥–ª—è —Ü—å–æ–≥–æ —Å–ø—Ä–æ—â–µ–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫—É):")
    print("  - results/dataset_samples.png - –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–≤–æ—Ö –∫–ª–∞—Å—ñ–≤")
    print("  - results/training_history_–ê–≤—Ç–æ–º–æ–±—ñ–ª—ñ_vs_–í–∑—É—Ç—Ç—è.png - —ñ—Å—Ç–æ—Ä—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è")
    print("  - results/weight_optimization_–ê–≤—Ç–æ–º–æ–±—ñ–ª—ñ_vs_–í–∑—É—Ç—Ç—è.png - –∑–º—ñ–Ω–∞ –≤–∞–≥")
    print("  - results/confusion_matrices_–ê–≤—Ç–æ–º–æ–±—ñ–ª—ñ_vs_–í–∑—É—Ç—Ç—è.png - –º–∞—Ç—Ä–∏—Ü—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏")
    print("  - results/errors_–ê–≤—Ç–æ–º–æ–±—ñ–ª—ñ_vs_–í–∑—É—Ç—Ç—è_MobileNetV2_partial.png - —Ç–∏–ø–æ–≤—ñ –ø–æ–º–∏–ª–∫–∏")

    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()

