# –í–ò–°–ù–û–í–ö–ò: Transfer Learning —Ç–∞ –ë–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥ –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤

## üìä –û–≥–ª—è–¥ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É

–¶–µ–π –ø—Ä–æ—î–∫—Ç –ø—Ä–æ–≤—ñ–≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω–µ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è **Transfer Learning** —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥ 40 –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π –ø—Ä–µ—Ç—Ä–µ–Ω–æ–≤–∞–Ω–∏—Ö feature extractors —Ç–∞ –∫–ª–∞—Å–∏—á–Ω–∏—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç—ñ CIFAR-10.

### –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤–∫–ª—é—á–∞–ª–æ:
- üéØ **5 Feature Extractors**: VGG16, VGG19, ResNet50, MobileNetV2, InceptionV3
- ü§ñ **8 –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤**: Logistic Regression, Linear SVM, SVM (RBF), Random Forest, Gradient Boosting, K-NN, Naive Bayes, MLP
- üî¨ **40 –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π** –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä √ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä
- üìà **–î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤–ø–ª–∏–≤—É –≥–ª–∏–±–∏–Ω–∏ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è** –¥–ª—è VGG16 —Ç–∞ ResNet50
- üé® **–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –ø–æ–º–∏–ª–æ–∫** —á–µ—Ä–µ–∑ confusion matrices

---

## üèÜ –ö–õ–Æ–ß–û–í–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò

### 1. –ù–∞–π–∫—Ä–∞—â—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó Feature Extractor √ó Classifier

| –ú—ñ—Å—Ü–µ | Feature Extractor | Classifier | Accuracy | F1-Score | –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è (—Å) |
|-------|------------------|------------|----------|----------|-----------------|
| ü•á **1** | **InceptionV3** | **SVM (RBF)** | **0.5330** | **0.5358** | **10.92** |
| ü•à **2** | **InceptionV3** | **Gradient Boosting** | **0.5240** | **0.5227** | **792.42** |
| ü•â **3** | **VGG16** | **SVM (RBF)** | **0.5215** | **0.5225** | **3.13** |
| 4 | InceptionV3 | MLP | 0.5115 | 0.5105 | 6.89 |
| 5 | VGG19 | SVM (RBF) | 0.4985 | 0.4992 | 2.90 |
| 6 | VGG16 | Gradient Boosting | 0.4975 | 0.4964 | 235.31 |
| 7 | VGG16 | MLP | 0.4845 | 0.4844 | 2.56 |
| 8 | InceptionV3 | Logistic Regression | 0.4820 | 0.4825 | 2.41 |
| 9 | InceptionV3 | Random Forest | 0.4795 | 0.4737 | 0.75 |
| 10 | VGG19 | MLP | 0.4780 | 0.4776 | 2.57 |

#### üí° –û—Å–Ω–æ–≤–Ω—ñ –≤–∏—Å–Ω–æ–≤–∫–∏ –∑ —Ç–æ–ø-10:

1. **InceptionV3 + SVM (RBF) - –∞–±—Å–æ–ª—é—Ç–Ω–∏–π –ª—ñ–¥–µ—Ä** –∑ accuracy 53.3%
   - –ù–∞–π–∫—Ä–∞—â–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è –±–∞–≥–∞—Ç–æ–º–∞—Å—à—Ç–∞–±–Ω–∏—Ö features —Ç–∞ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ–≥–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞
   - –ü–æ–º—ñ—Ä–Ω–∏–π —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è (~11 —Å–µ–∫—É–Ω–¥)
   - –í—ñ–¥–º—ñ–Ω–Ω–∏–π –±–∞–ª–∞–Ω—Å —è–∫–æ—Å—Ç—ñ —Ç–∞ —à–≤–∏–¥–∫–æ—Å—Ç—ñ

2. **SVM (RBF) –¥–æ–º—ñ–Ω—É—î** - –ø—Ä–∏—Å—É—Ç–Ω—ñ–π —É 3 –∑ —Ç–æ–ø-5 –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π
   - –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ decision boundaries –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤—ñ –¥–ª—è features –∑ CNN
   - –ï—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–∞—Ü—é—î —É –≤–∏—Å–æ–∫–æ–≤–∏–º—ñ—Ä–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ –æ–∑–Ω–∞–∫

3. **InceptionV3 - –Ω–∞–π–∫—Ä–∞—â–∏–π feature extractor**
   - –ó–∞–π–Ω—è–≤ 4 –º—ñ—Å—Ü—è –∑ —Ç–æ–ø-10
   - –ë–∞–≥–∞—Ç–æ–º–∞—Å—à—Ç–∞–±–Ω—ñ inception modules –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å –±–∞–≥–∞—Ç—à—ñ features
   - 2048-–≤–∏–º—ñ—Ä–Ω—ñ features –¥–∞—é—Ç—å –≤–∏—Å–æ–∫—É –≤–∏—Ä–∞–∑–Ω—ñ—Å—Ç—å

---

### 2. –°–µ—Ä–µ–¥–Ω—è Accuracy –ø–æ Feature Extractors

| Feature Extractor | –°–µ—Ä–µ–¥–Ω—è Accuracy | –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å Features | –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ |
|-------------------|-----------------|---------------------|-----------|
| **InceptionV3** | **0.4434** ü•á | 2048 | ~23.8M |
| **VGG16** | **0.4272** ü•à | 512 | ~14.7M |
| **VGG19** | **0.4078** ü•â | 512 | ~20M |
| ResNet50 | 0.3728 | 2048 | ~25.6M |
| MobileNetV2 | 0.2618 ‚ö†Ô∏è | 1280 | ~3.5M |

#### üìä –ê–Ω–∞–ª—ñ–∑ Feature Extractors:

##### ü•á InceptionV3 - –ü–µ—Ä–µ–º–æ–∂–µ—Ü—å (44.34% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–ß–æ–º—É –Ω–∞–π–∫—Ä–∞—â–∏–π:**
- ‚úÖ **–ë–∞–≥–∞—Ç–æ–º–∞—Å—à—Ç–∞–±–Ω—ñ features**: Inception modules –æ–±—Ä–æ–±–ª—è—é—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
- ‚úÖ **–í–∏—Å–æ–∫–∞ –≤–∏—Ä–∞–∑–Ω—ñ—Å—Ç—å**: 2048-–≤–∏–º—ñ—Ä–Ω—ñ features –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å –±–∞–≥–∞—Ç—É —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü—ñ—é
- ‚úÖ **–ï—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞**: –§–∞–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ñ –∑–≥–æ—Ä—Ç–∫–∏ –¥–∞—é—Ç—å –∫—Ä–∞—â—É —è–∫—ñ—Å—Ç—å –ø—Ä–∏ –º–µ–Ω—à–∏—Ö –æ–±—á–∏—Å–ª–µ–Ω–Ω—è—Ö
- ‚úÖ **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å**: –ü—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ –∑ —É—Å—ñ–º–∞ —Ç–∏–ø–∞–º–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –°–∫–ª–∞–¥–Ω—ñ –≤—ñ–∑—É–∞–ª—å–Ω—ñ –∑–∞–¥–∞—á—ñ –∑ –æ–±'—î–∫—Ç–∞–º–∏ —Ä—ñ–∑–Ω–∏—Ö —Ä–æ–∑–º—ñ—Ä—ñ–≤
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å
- –Ñ GPU —Ç–∞ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –ø–∞–º'—è—Ç—ñ

##### ü•à VGG16 - –ù–∞–¥—ñ–π–Ω–∏–π –≤–∏–±—ñ—Ä (42.72% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ **–°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å**: –ö–ª–∞—Å–∏—á–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞
- ‚úÖ **–ü—Ä–æ—Å—Ç–æ—Ç–∞**: –õ–µ–≥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏
- ‚úÖ **–®–≤–∏–¥–∫—ñ—Å—Ç—å**: –®–≤–∏–¥—à–∏–π –∑–∞ InceptionV3 –ø—Ä–∏ –Ω–µ–≤–µ–ª–∏–∫—ñ–π –≤—Ç—Ä–∞—Ç—ñ —è–∫–æ—Å—Ç—ñ
- ‚úÖ **–ö–æ–º–ø–∞–∫—Ç–Ω—ñ features**: 512-–≤–∏–º—ñ—Ä–Ω—ñ features –ª–µ–≥—à–µ –æ–±—Ä–æ–±–ª—è—Ç–∏

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ –Ω–∞–¥—ñ–π–Ω—ñ—Å—Ç—å —Ç–∞ –ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω—ñ—Å—Ç—å
- –û–±–º–µ–∂–µ–Ω—ñ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏ (–ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ InceptionV3)
- Baseline –¥–ª—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤

##### ü•â VGG19 - –ì–ª–∏–±—à–∞ –≤–µ—Ä—Å—ñ—è (40.78% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**
- ‚ö†Ô∏è **–ü–∞—Ä–∞–¥–æ–∫—Å**: –ì–ª–∏–±—à–∞ –∑–∞ VGG16, –∞–ª–µ **–≥—ñ—Ä—à–∞** —Å–µ—Ä–µ–¥–Ω—è accuracy
- –ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:
  - Overfitting —á–µ—Ä–µ–∑ –±—ñ–ª—å—à—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
  - –î–æ–¥–∞—Ç–∫–æ–≤–∞ –≥–ª–∏–±–∏–Ω–∞ –Ω–µ –¥–∞—î –ø–µ—Ä–µ–≤–∞–≥ –Ω–∞ CIFAR-10 (32√ó32)
  - –ü–æ—Ç—Ä–µ–±—É—î –±—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö –¥–ª—è –ø–æ–≤–Ω–æ—ó —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª—É

**–í–∏—Å–Ω–æ–≤–æ–∫**: VGG16 - –∫—Ä–∞—â–∏–π –≤–∏–±—ñ—Ä –Ω—ñ–∂ VGG19 –¥–ª—è –º–∞–ª–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤

##### ‚ö†Ô∏è ResNet50 - –ù–µ—Å–ø–æ–¥—ñ–≤–∞–Ω–µ —Ä–æ–∑—á–∞—Ä—É–≤–∞–Ω–Ω—è (37.28% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–û—á—ñ–∫—É–≤–∞–Ω–Ω—è vs. –†–µ–∞–ª—å–Ω—ñ—Å—Ç—å:**
- ‚ùå –û—á—ñ–∫—É–≤–∞–ª–∏ –Ω–∞–π–∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—á–µ—Ä–µ–∑ residual connections)
- ‚ùå –í–∏—è–≤–∏–ª–æ—Å—å - –ª–∏—à–µ 4-–µ –º—ñ—Å—Ü–µ

**–ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:**
- üîπ **–ü—Ä–æ–±–ª–µ–º–∞ —ñ–∑ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è–º**: ResNet50 –º–∞—î —Å–∫–ª–∞–¥–Ω—É –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –∑ batch normalization
- üîπ **–ù–µ–≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å —Ä–æ–∑–º—ñ—Ä—É**: –ü—Ä–µ—Ç—Ä–µ–Ω–æ–≤–∞–Ω–∏–π –Ω–∞ ImageNet (224√ó224), –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –Ω–∞ CIFAR-10 (32√ó32)
- üîπ **–ü–æ—Ç—Ä–µ–±—É—î fine-tuning**: –ú–æ–∂–µ –¥–∞–≤–∞—Ç–∏ –∫—Ä–∞—â—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ä–æ–∑–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—ñ —à–∞—Ä—ñ–≤
- üîπ **2048-D features –º–æ–∂–µ –±—É—Ç–∏ –Ω–∞–¥–ª–∏—à–∫–æ–≤–∏–º** –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å CIFAR-10

**–í–∞–∂–ª–∏–≤–æ:** –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è (–¥–∏–≤. –Ω–∏–∂—á–µ) –ø–æ–∫–∞–∑—É—é—Ç—å, —â–æ ResNet50 –ø—Ä–∞—Ü—é—î **–∫—Ä–∞—â–µ –∑ –ø–æ–≤–Ω—ñ—Å—Ç—é –∑–∞–º–æ—Ä–æ–∂–µ–Ω–∏–º–∏ —à–∞—Ä–∞–º–∏**

##### üö´ MobileNetV2 - –ö–æ–º–ø—Ä–æ–º—ñ—Å –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–≤ (26.18% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–ß–æ–º—É —Ç–∞–∫ –ø–æ–≥–∞–Ω–æ:**
- ‚ö†Ô∏è **–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ, –Ω–µ —è–∫–æ—Å—Ç—ñ**
- ‚ö†Ô∏è **Depthwise separable convolutions** –≤—Ç—Ä–∞—á–∞—é—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é
- ‚ö†Ô∏è **Compression** —É linear bottlenecks –Ω–∞–¥—Ç–æ –∞–≥—Ä–µ—Å–∏–≤–Ω–∞ –¥–ª—è CIFAR-10
- ‚ö†Ô∏è Features –º–µ–Ω—à –≤–∏—Ä–∞–∑–Ω—ñ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó

**–ê–ª–µ:**
- ‚úÖ –ù–∞–π—à–≤–∏–¥—à–∏–π –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
- ‚úÖ –ù–∞–π–º–µ–Ω—à–∏–π —Ä–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ (3.5M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤)
- ‚úÖ –Ü–¥–µ–∞–ª—å–Ω–∏–π –¥–ª—è mobile/edge deployment

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- Real-time applications –Ω–∞ —Å–ª–∞–±–∫–∏—Ö –ø—Ä–∏—Å—Ç—Ä–æ—è—Ö
- –ú–æ–±—ñ–ª—å–Ω—ñ –¥–æ–¥–∞—Ç–∫–∏
- IoT –ø—Ä–∏—Å—Ç—Ä–æ—ó
- –ö–æ–ª–∏ —à–≤–∏–¥–∫—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω—ñ—à–∞ –∑–∞ accuracy

---

### 3. –°–µ—Ä–µ–¥–Ω—è Accuracy –ø–æ –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞—Ö

| Classifier | –°–µ—Ä–µ–¥–Ω—è Accuracy | –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è (—Å) | –¢–∏–ø |
|------------|-----------------|--------------------------|-----|
| **SVM (RBF)** | **0.4598** ü•á | **9.05** | –ù–µ–ª—ñ–Ω—ñ–π–Ω–∏–π |
| **Gradient Boosting** | **0.4362** ü•à | **351.09** ‚ö†Ô∏è | Ensemble |
| **MLP** | **0.4256** ü•â | **11.22** | Neural Network |
| Logistic Regression | 0.4258 | 2.63 | –õ—ñ–Ω—ñ–π–Ω–∏–π |
| Random Forest | 0.4131 | 0.44 ‚ö° | Ensemble |
| Linear SVM | 0.4138 | 270.21 | –õ—ñ–Ω—ñ–π–Ω–∏–π |
| K-Nearest Neighbors | 0.3413 | 0.005 ‚ö°‚ö° | Instance-based |
| Naive Bayes | 0.2335 ‚ö†Ô∏è | 0.038 ‚ö°‚ö° | Probabilistic |

#### üéØ –î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤:

##### ü•á SVM (RBF) - –ë–µ–∑—É–º–æ–≤–Ω–∏–π –ª—ñ–¥–µ—Ä (45.98% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–ß–æ–º—É –ø–µ—Ä–µ–º—ñ–≥:**
- ‚úÖ **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ decision boundaries**: –ö—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–æ –¥–ª—è features –∑ CNN
- ‚úÖ **Kernel trick**: –ï—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–∞—Ü—é—î —É –≤–∏—Å–æ–∫–æ–≤–∏–º—ñ—Ä–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ –±–µ–∑ —è–≤–Ω–æ–≥–æ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è
- ‚úÖ **Regularization**: –í–±—É–¥–æ–≤–∞–Ω–∏–π –º–µ—Ö–∞–Ω—ñ–∑–º –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è overfitting —á–µ—Ä–µ–∑ parameter C
- ‚úÖ **Robust –¥–æ dimensionality**: –ü—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ –∑ 512-D (VGG) —Ç–∞ 2048-D (ResNet/Inception) features

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ö†Ô∏è –Ü–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–≤—ñ–ª—å–Ω–æ—é –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö (~2-12 —Å–µ–∫)
- ‚ö†Ô∏è –ü–æ—Ç—Ä–µ–±—É—î –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ (C, gamma)

**–û–ø—Ç–∏–º–∞–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:**
- –°–µ—Ä–µ–¥–Ω—ñ —Ç–∞ –≤–µ–ª–∏–∫—ñ feature spaces (512-2048 dimensions)
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ –≤–∏—Å–æ–∫–∞ —è–∫—ñ—Å—Ç—å
- –Ñ —á–∞—Å –Ω–∞ –Ω–∞–≤—á–∞–Ω–Ω—è (~3-21 —Å–µ–∫—É–Ω–¥)

##### ü•à Gradient Boosting - –ü–æ—Ç—É–∂–Ω–∏–π –∞–ª–µ –ø–æ–≤—ñ–ª—å–Ω–∏–π (43.62% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ **–í–∏—Å–æ–∫–∞ —è–∫—ñ—Å—Ç—å**: 2-–µ –º—ñ—Å—Ü–µ —Å–µ—Ä–µ–¥ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
- ‚úÖ **–ü–æ—Å–ª—ñ–¥–æ–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è**: –ö–æ–∂–Ω–µ –¥–µ—Ä–µ–≤–æ –≤–∏–ø—Ä–∞–≤–ª—è—î –ø–æ–º–∏–ª–∫–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ
- ‚úÖ **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ**: –ú–æ–∂–µ –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ patterns

**–ö—Ä–∏—Ç–∏—á–Ω–∏–π –Ω–µ–¥–æ–ª—ñ–∫:**
- ‚ö†Ô∏è **–î—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–∏–π**: –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è **351 —Å–µ–∫—É–Ω–¥** (–º–∞–π–∂–µ 6 —Ö–≤–∏–ª–∏–Ω!)
- ‚ö†Ô∏è –ù–∞–π–ø–æ–≤—ñ–ª—å–Ω—ñ—à–∏–π –∑ —É—Å—ñ—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
- ‚ö†Ô∏è –í–∞–∂–∫–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ (–±–∞–≥–∞—Ç–æ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤)

**–í–∏—Å–Ω–æ–≤–æ–∫:** Gradient Boosting –¥–∞—î —Ö–æ—Ä–æ—à—É —è–∫—ñ—Å—Ç—å, –∞–ª–µ **–Ω–µ –≤–∞—Ä—Ç–æ trade-off** - SVM (RBF) –º–∞–π–∂–µ —Ç–∞–∫–∏–π –∂–µ —Ö–æ—Ä–æ—à–∏–π, –∞–ª–µ –≤ **39 —Ä–∞–∑—ñ–≤ —à–≤–∏–¥—à–∏–π**!

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å
- –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∏–π
- –û—Ñ–ª–∞–π–Ω –Ω–∞–≤—á–∞–Ω–Ω—è

##### ü•â MLP - –ù–µ–π—Ä–æ–Ω–Ω–∞ –º–µ—Ä–µ–∂–∞ (42.56% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**
- ü§î –¢—Ä–µ—Ç—î –º—ñ—Å—Ü–µ - –Ω–µ–ø–æ–≥–∞–Ω–æ, –∞–ª–µ **–Ω–µ –∫—Ä–∞—â–µ** –∑–∞ SVM (RBF)
- –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞: –¥–≤–∞ hidden layers (128, 64 neurons)

**–ê–Ω–∞–ª—ñ–∑:**
- ‚úÖ –ú–æ–∂–µ –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
- ‚úÖ –ü–æ–º—ñ—Ä–Ω–∏–π —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è (~11 —Å–µ–∫—É–Ω–¥)
- ‚ö†Ô∏è –ü–æ—Ç—Ä–µ–±—É—î –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏
- ‚ö†Ô∏è –†–∏–∑–∏–∫ overfitting

**–í–∏—Å–Ω–æ–≤–æ–∫:** –î–ª—è **feature classification** (–∞ –Ω–µ end-to-end –Ω–∞–≤—á–∞–Ω–Ω—è) MLP –Ω–µ –¥–∞—î –ø–µ—Ä–µ–≤–∞–≥ –Ω–∞–¥ SVM (RBF)

##### üìà Logistic Regression - –®–≤–∏–¥–∫–∏–π baseline (42.58% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–°—é—Ä–ø—Ä–∏–∑:**
- ‚úÖ –ú–∞–π–∂–µ —Ç–∞–∫–∏–π –∂–µ —Ö–æ—Ä–æ—à–∏–π —è–∫ MLP (42.58% vs 42.56%)!
- ‚úÖ **–î—É–∂–µ —à–≤–∏–¥–∫–∏–π**: –õ–∏—à–µ 2.63 —Å–µ–∫—É–Ω–¥–∏ —Å–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è
- ‚úÖ –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–∏–π

**–í–∏—Å–Ω–æ–≤–æ–∫:** –í—ñ–¥–º—ñ–Ω–Ω–∏–π –≤–∏–±—ñ—Ä –¥–ª—è **baseline** —Ç–∞ —à–≤–∏–¥–∫–∏—Ö –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤. –Ø–∫—â–æ Logistic Regression –¥–∞—î —Ö–æ—Ä–æ—à—É —è–∫—ñ—Å—Ç—å, —Ç–æ features –≤–∂–µ –¥–æ–±—Ä–µ –ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ.

##### üå≤ Random Forest - –°—Ç–∞–±—ñ–ª—å–Ω–∏–π –∞–ª–µ –Ω–µ –ª—ñ–¥–µ—Ä (41.31% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ **–ù–∞–π—à–≤–∏–¥—à–∏–π –Ω–∞–≤—á–∞–Ω–Ω—è**: –õ–∏—à–µ 0.44 —Å–µ–∫—É–Ω–¥–∏ –≤ —Å–µ—Ä–µ–¥–Ω—å–æ–º—É! ‚ö°
- ‚úÖ –ù–µ –ø–æ—Ç—Ä–µ–±—É—î feature scaling
- ‚úÖ Robust –¥–æ outliers

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ö†Ô∏è –ì—ñ—Ä—à–∞ —è–∫—ñ—Å—Ç—å –Ω—ñ–∂ SVM (RBF) –∞–±–æ Gradient Boosting
- ‚ö†Ô∏è –Ü–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è –ø–æ–≤—ñ–ª—å–Ω—ñ—à–∞ (~0.03 —Å–µ–∫)

**–í–∏—Å–Ω–æ–≤–æ–∫:** –•–æ—Ä–æ—à–∏–π –≤–∏–±—ñ—Ä –∫–æ–ª–∏ **—á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è –∫—Ä–∏—Ç–∏—á–Ω–∏–π**, –∞–ª–µ —è–∫—ñ—Å—Ç—å –Ω–µ –Ω–∞–π–≤–∏—â–∞ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç

##### üéØ Linear SVM - –ü–æ–≤—ñ–ª—å–Ω–∏–π –ª—ñ–Ω—ñ–π–Ω–∏–π (41.38% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–ü–∞—Ä–∞–¥–æ–∫—Å:**
- ‚ùå –ü—Ä–∏–±–ª–∏–∑–Ω–æ —Ç–∞–∫–∞ –∂ accuracy —è–∫ Logistic Regression (41.38% vs 42.58%)
- ‚ùå –ê–ª–µ –≤ **103 —Ä–∞–∑–∏ –ø–æ–≤—ñ–ª—å–Ω—ñ—à–∏–π** (270 —Å–µ–∫ vs 2.6 —Å–µ–∫)!

**–í–∏—Å–Ω–æ–≤–æ–∫:** **–ù–µ –≤–∞—Ä—Ç–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Linear SVM** –∫–æ–ª–∏ —î Logistic Regression. Logistic Regression —à–≤–∏–¥—à–∏–π —ñ —Ç—Ä–æ—Ö–∏ –∫—Ä–∞—â–∏–π.

##### üé≤ K-Nearest Neighbors - –ü–æ–≤—ñ–ª—å–Ω–∞ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è (34.13% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–ü—Ä–æ–±–ª–µ–º–∏:**
- ‚ö†Ô∏è –ù–∏–∑—å–∫–∞ accuracy (~34%)
- ‚ö†Ô∏è **Curse of dimensionality**: –ü–æ–≥–∞–Ω–æ –ø—Ä–∞—Ü—é—î —É –≤–∏—Å–æ–∫–æ–≤–∏–º—ñ—Ä–Ω–∏—Ö –ø—Ä–æ—Å—Ç–æ—Ä–∞—Ö (512-2048D)
- ‚ö†Ô∏è –ü–æ–≤—ñ–ª—å–Ω–∞ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è (~0.1-0.4 —Å–µ–∫)
- ‚ö†Ô∏è –í–∏–º–∞–≥–∞—î –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –≤—Å—ñ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ

**–í–∏—Å–Ω–æ–≤–æ–∫:** **–ù–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å** –¥–ª—è high-dimensional CNN features

##### üé∞ Naive Bayes - –ù–∞–π–≥—ñ—Ä—à–∏–π –≤–∏–±—ñ—Ä (23.35% —Å–µ—Ä–µ–¥–Ω—è accuracy)

**–ö—Ä–∏—Ç–∏—á–Ω—ñ –Ω–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå **–î—É–∂–µ –Ω–∏–∑—å–∫–∞ accuracy** (~23%) - –≥—ñ—Ä—à–µ –∑–∞ random guessing (10% –¥–ª—è 10 –∫–ª–∞—Å—ñ–≤)!
- ‚ùå –ü—Ä–∏–ø—É—â–µ–Ω–Ω—è –ø—Ä–æ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å features **–Ω–µ –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è** –¥–ª—è CNN features
- ‚ùå CNN features —Å–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ

**–í–∏—Å–Ω–æ–≤–æ–∫:** **–ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏** –¥–ª—è CNN features. Naive Bayes —Å—Ç–≤–æ—Ä–µ–Ω–∏–π –¥–ª—è –Ω–µ–∑–∞–ª–µ–∂–Ω–∏—Ö features (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, bag-of-words), –∞ –Ω–µ –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤ –∑ –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂.

---

### 4. üî¨ –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≥–ª–∏–±–∏–Ω–∏ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è —à–∞—Ä—ñ–≤

–¶–µ –æ–¥–Ω–µ –∑ **–Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å** –ø—Ä–æ—î–∫—Ç—É! –ü–æ–∫–∞–∑—É—î —è–∫ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–∏—Ö/trainable —à–∞—Ä—ñ–≤ –≤–ø–ª–∏–≤–∞—î –Ω–∞ —Ñ—ñ–Ω–∞–ª—å–Ω—É —è–∫—ñ—Å—Ç—å.

#### 4.1. VGG16: –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è (19 —à–∞—Ä—ñ–≤)

| –ó–∞–º–æ—Ä–æ–∂–µ–Ω—ñ —à–∞—Ä–∏ | % –ó–∞–º–æ—Ä–æ–∂–µ–Ω–∏—Ö | Accuracy | –í–∏—Å–Ω–æ–≤–æ–∫ |
|-----------------|--------------|----------|----------|
| **0 —à–∞—Ä—ñ–≤** | **0%** | **0.6905** üèÜ | **–ù–∞–π–∫—Ä–∞—â–∏–π!** |
| 4 —à–∞—Ä–∏ | 21% | 0.6730 | –í—ñ–¥–º—ñ–Ω–Ω–æ |
| 9 —à–∞—Ä—ñ–≤ | 47% | 0.6630 | –î–æ–±—Ä–µ |
| 14 —à–∞—Ä—ñ–≤ | 74% | 0.5840 | –ü–∞–¥—ñ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ |
| **19 —à–∞—Ä—ñ–≤** | **100%** | **0.4575** ‚ö†Ô∏è | **Feature extraction only** |

#### üìä –ê–Ω–∞–ª—ñ–∑ VGG16:

**–ö–ª—é—á–æ–≤—ñ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**

1. **Full Fine-tuning –ø–µ—Ä–µ–º—ñ–≥!** üèÜ
   - 0% frozen ‚Üí **69.05% accuracy**
   - –¶–µ –Ω–∞ **51% –∫—Ä–∞—â–µ** –Ω—ñ–∂ feature extraction only (45.75%)
   - –†—ñ–∑–Ω–∏—Ü—è: **23.3 percentage points** - –≤–µ–ª–∏—á–µ–∑–Ω–∞!

2. **–ß—ñ—Ç–∫–∏–π —Ç—Ä–µ–Ω–¥ –∑–º–µ–Ω—à–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ**:
   - –ë—ñ–ª—å—à–µ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–∏—Ö —à–∞—Ä—ñ–≤ ‚Üí –Ω–∏–∂—á–∞ accuracy
   - –õ—ñ–Ω—ñ–π–Ω–∞ –¥–µ–≥—Ä–∞–¥–∞—Ü—ñ—è: 0% ‚Üí 21% ‚Üí 47% ‚Üí 74% ‚Üí 100%
   - Accuracy: 69.05% ‚Üí 67.30% ‚Üí 66.30% ‚Üí 58.40% ‚Üí 45.75%

3. **–û–ø—Ç–∏–º–∞–ª—å–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è –¥–ª—è VGG16: –ú—ñ–Ω—ñ–º—É–º –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è**
   - –ù–∞–π–∫—Ä–∞—â–µ: —Ä–æ–∑–º–æ—Ä–æ–∑–∏—Ç–∏ **–≤—Å—ñ —à–∞—Ä–∏** (0% frozen)
   - –ü—Ä–∏–π–Ω—è—Ç–Ω–æ: –∑–∞–º–æ—Ä–æ–∑–∏—Ç–∏ –¥–æ 20-25% —Ä–∞–Ω–Ω—ñ—Ö —à–∞—Ä—ñ–≤
   - Avoid: –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è –±—ñ–ª—å—à–µ 50% —à–∞—Ä—ñ–≤

**–ß–æ–º—É Full Fine-tuning –ø—Ä–∞—Ü—é—î –¥–ª—è VGG16:**
- ‚úÖ VGG16 –º–∞—î –ø—Ä–æ—Å—Ç—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—É –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É
- ‚úÖ –ù–µ–º–∞—î —Å–∫–ª–∞–¥–Ω–∏—Ö batch normalization layers
- ‚úÖ 5000 train –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–æ—Å—Ç–∞—Ç–Ω—å–æ —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ overfitting
- ‚úÖ –î—Ä—ñ–±–Ω–∏–π learning rate (0.0001) –∑–∞–ø–æ–±—ñ–≥–∞—î catastrophic forgetting

#### 4.2. ResNet50: –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è (175 —à–∞—Ä—ñ–≤)

| –ó–∞–º–æ—Ä–æ–∂–µ–Ω—ñ —à–∞—Ä–∏ | % –ó–∞–º–æ—Ä–æ–∂–µ–Ω–∏—Ö | Accuracy | –í–∏—Å–Ω–æ–≤–æ–∫ |
|-----------------|--------------|----------|----------|
| 0 —à–∞—Ä—ñ–≤ | 0% | 0.4175 | Overfitting ‚ö†Ô∏è |
| 43 —à–∞—Ä–∏ | 25% | 0.3290 | –î—É–∂–µ –ø–æ–≥–∞–Ω–æ |
| 87 —à–∞—Ä—ñ–≤ | 50% | 0.3065 | –ù–∞–π–≥—ñ—Ä—à–µ ‚ùå |
| 131 —à–∞—Ä—ñ–≤ | 75% | 0.3625 | –ü–æ–≥–∞–Ω–æ |
| **175 —à–∞—Ä—ñ–≤** | **100%** | **0.4570** üèÜ | **–ù–∞–π–∫—Ä–∞—â–∏–π!** |

#### üìä –ê–Ω–∞–ª—ñ–∑ ResNet50:

**–ö–ª—é—á–æ–≤—ñ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**

1. **–ü–æ–≤–Ω–∞ –ø—Ä–æ—Ç–∏–ª–µ–∂–Ω—ñ—Å—Ç—å VGG16!** ü§Ø
   - 100% frozen ‚Üí **45.70% accuracy** (–Ω–∞–π–∫—Ä–∞—â–µ)
   - 0% frozen ‚Üí **41.75% accuracy**
   - Feature extraction –ø—Ä–∞—Ü—é—î **–∫—Ä–∞—â–µ** –Ω—ñ–∂ fine-tuning!

2. **U-–ø–æ–¥—ñ–±–Ω–∞ –∫—Ä–∏–≤–∞**:
   - –ù–∞–π–≥—ñ—Ä—à–∞ accuracy –ø—Ä–∏ 50% frozen (30.65%)
   - –ö—Ä–∞—â–µ –Ω–∞ –∫—Ä–∞—è—Ö: 0% –∞–±–æ 100% frozen

3. **–ß–æ–º—É ResNet50 —Ç–∞–∫ —Å–∫–ª–∞–¥–Ω–æ fine-tune:**
   - ‚ö†Ô∏è **Batch Normalization layers**: –ú–∞—é—Ç—å running statistics –∑ ImageNet
   - ‚ö†Ô∏è **175 —à–∞—Ä—ñ–≤** - –¥—É–∂–µ –≥–ª–∏–±–æ–∫–∞ –º–µ—Ä–µ–∂–∞
   - ‚ö†Ô∏è **Residual connections**: –°–∫–ª–∞–¥–Ω—ñ gradient flows
   - ‚ö†Ô∏è **5000 –∑–æ–±—Ä–∞–∂–µ–Ω—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ** –¥–ª—è fine-tuning —Ç–∞–∫–æ—ó –≥–ª–∏–±–æ–∫–æ—ó –º–µ—Ä–µ–∂—ñ

4. **–ü–∞—Ä—Ü—ñ–∞–ª—å–Ω–µ —Ä–æ–∑–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è –Ω–∞–π–≥—ñ—Ä—à–µ**:
   - 25-75% frozen –¥–∞—é—Ç—å –Ω–∞–π–≥—ñ—Ä—à—É —è–∫—ñ—Å—Ç—å
   - "Worst of both worlds": –Ω—ñ features –∑ ImageNet, –Ω—ñ –ø–æ–≤–Ω–∞ –∞–¥–∞–ø—Ç–∞—Ü—ñ—è

**–û–ø—Ç–∏–º–∞–ª—å–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è –¥–ª—è ResNet50:**
- üèÜ **–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —è–∫ feature extractor** (100% frozen)
- ‚ö†Ô∏è **–£–Ω–∏–∫–∞—Ç–∏ –ø–∞—Ä—Ü—ñ–∞–ª—å–Ω–æ–≥–æ fine-tuning** (25-75% frozen)
- üî¨ –Ø–∫—â–æ fine-tuning –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–π: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ discriminative learning rates –∞–±–æ gradual unfreezing

---

#### 4.3. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è VGG16 vs ResNet50

| –ê—Å–ø–µ–∫—Ç | VGG16 | ResNet50 |
|--------|-------|----------|
| **–ù–∞–π–∫—Ä–∞—â–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è** | Full fine-tuning (0% frozen) | Feature extraction (100% frozen) |
| **–ù–∞–π–∫—Ä–∞—â–∞ accuracy** | **69.05%** üèÜ | 45.70% |
| **Feature extraction only** | 45.75% | **45.70%** (–º–∞–π–∂–µ –æ–¥–Ω–∞–∫–æ–≤–æ!) |
| **–°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å fine-tuning** | –õ–µ–≥–∫–æ | –î—É–∂–µ —Å–∫–ª–∞–¥–Ω–æ |
| **–ö—ñ–ª—å–∫—ñ—Å—Ç—å —à–∞—Ä—ñ–≤** | 19 | 175 |
| **Batch Normalization** | –ù–µ–º–∞—î | –¢–∞–∫ (–ø—Ä–æ–±–ª–µ–º–∞!) |
| **–ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –º–∞–ª–∏—Ö –¥–∞–Ω–∏—Ö** | ‚úÖ –¢–∞–∫ (–∑ fine-tuning) | ‚ùå –ù—ñ (—Ç—ñ–ª—å–∫–∏ feature extraction) |

#### üìà –ó–∞–≥–∞–ª—å–Ω—ñ –≤–∏—Å–Ω–æ–≤–∫–∏ –ø—Ä–æ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è:

1. **–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –º–∞—î –∑–Ω–∞—á–µ–Ω–Ω—è!**
   - –ü—Ä–æ—Å—Ç–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ (VGG16) ‚Üí –ª–µ–≥–∫–æ fine-tune
   - –°–∫–ª–∞–¥–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ (ResNet50) ‚Üí –∫—Ä–∞—â–µ freeze

2. **Batch Normalization - –ø—Ä–æ–±–ª–µ–º–∞**:
   - BN layers –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑ ImageNet
   - –ü—Ä–∏ fine-tuning –º–æ–∂—É—Ç—å –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—É–≤–∞—Ç–∏ –∑ –Ω–æ–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏
   - –í–∏–º–∞–≥–∞—é—Ç—å —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏

3. **–†–æ–∑–º—ñ—Ä –¥–∞—Ç–∞—Å–µ—Ç—É –∫—Ä–∏—Ç–∏—á–Ω–∏–π**:
   - 5000 –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–ª—è VGG16 fine-tuning
   - –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–ª—è ResNet50 fine-tuning
   - –ü—Ä–∞–≤–∏–ª–æ: –±—ñ–ª—å—à–∞ –º–µ—Ä–µ–∂–∞ ‚Üí –ø–æ—Ç—Ä—ñ–±–Ω–æ –±—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö

4. **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó**:
   - **VGG16/VGG19**: –†–æ–∑–º–æ—Ä–æ–∑–∏—Ç–∏ –≤—Å—ñ –∞–±–æ –º–∞–π–∂–µ –≤—Å—ñ —à–∞—Ä–∏
   - **ResNet50/InceptionV3/MobileNetV2**: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —è–∫ feature extractors (100% frozen)
   - **–ú–∞–ª—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (<10K)**: –£–Ω–∏–∫–∞—Ç–∏ –≥–ª–∏–±–æ–∫–æ–≥–æ fine-tuning
   - **–í–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (>50K)**: –ú–æ–∂–Ω–∞ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–≤–∞—Ç–∏ –∑ gradual unfreezing

---

## üí° –ì–û–õ–û–í–ù–Ü –í–ò–°–ù–û–í–ö–ò –¢–ê –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á

### 1. üèÜ –ù–∞–π–∫—Ä–∞—â–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è: InceptionV3 + SVM (RBF)

**Accuracy: 53.30%**

**–ß–æ–º—É —Ü—è –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è –ø–µ—Ä–µ–º–∞–≥–∞—î:**
- ‚úÖ **–ë–∞–≥–∞—Ç–æ–º–∞—Å—à—Ç–∞–±–Ω—ñ features**: InceptionV3 –æ–±—Ä–æ–±–ª—è—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
- ‚úÖ **–ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è**: RBF kernel –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–æ–¥–µ–ª—é—î —Å–∫–ª–∞–¥–Ω—ñ decision boundaries
- ‚úÖ **–û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –±–∞–ª–∞–Ω—Å**: –í–∏—Å–æ–∫–∞ —è–∫—ñ—Å—Ç—å –ø—Ä–∏ –ø–æ–º—ñ—Ä–Ω–æ–º—É —á–∞—Å—ñ –Ω–∞–≤—á–∞–Ω–Ω—è (~11 —Å–µ–∫)
- ‚úÖ **Robust**: –ü—Ä–∞—Ü—é—î —Å—Ç–∞–±—ñ–ª—å–Ω–æ, –Ω–µ overfit

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å –Ω–∞ –º–∞–ª–∏—Ö/—Å–µ—Ä–µ–¥–Ω—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
- –Ñ GPU –¥–ª—è –µ–∫—Å—Ç—Ä–∞–∫—Ü—ñ—ó features
- –ö—Ä–∏—Ç–∏—á–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ (—è–∫—â–æ —á–∞—Å –∫—Ä–∏—Ç–∏—á–Ω–∏–π):**
- **VGG16 + SVM (RBF)** - –º–∞–π–∂–µ —Ç–∞–∫–∞ –∂ —è–∫—ñ—Å—Ç—å (52.15%), –∞–ª–µ —à–≤–∏–¥—à–∞ –µ–∫—Å—Ç—Ä–∞–∫—Ü—ñ—è

---

### 2. üî¨ Fine-tuning vs Feature Extraction: –ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏!

#### ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Fine-tuning:

**VGG16/VGG19:**
- üèÜ **Full fine-tuning –¥–∞—î +50% improvement** (45.75% ‚Üí 69.05%)
- –ü—Ä–æ—Å—Ç–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑ BN layers
- –ü—Ä–∞—Ü—é—î –Ω–∞–≤—ñ—Ç—å –Ω–∞ 5000 –∑–æ–±—Ä–∞–∂–µ–Ω—å

**–Ø–∫ —Ä–æ–±–∏—Ç–∏:**
```python
# –†–æ–∑–º–æ—Ä–æ–∑–∏—Ç–∏ –≤—Å—ñ —à–∞—Ä–∏
for layer in base_model.layers:
    layer.trainable = True

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –º–∞–ª–∏–π learning rate
optimizer = Adam(lr=0.0001)

# 3-5 epochs –¥–æ—Å—Ç–∞—Ç–Ω—å–æ
model.fit(epochs=3)
```

#### ‚ùå –£–Ω–∏–∫–∞—Ç–∏ Fine-tuning:

**ResNet50/InceptionV3/MobileNetV2:**
- Feature extraction (100% frozen) –ø—Ä–∞—Ü—é—î **–∫—Ä–∞—â–µ**
- Batch Normalization –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—É—î –ø—Ä–∏ fine-tuning
- –ü–æ—Ç—Ä–µ–±—É—î >10K –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ fine-tuning

**–Ø–∫ —Ä–æ–±–∏—Ç–∏:**
```python
# –ó–∞–º–æ—Ä–æ–∑–∏—Ç–∏ –≤—Å—ñ —à–∞—Ä–∏
base_model.trainable = False

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —è–∫ feature extractor
features = base_model.predict(X)

# –ù–∞–≤—á–∏—Ç–∏ –ø—Ä–æ—Å—Ç–∏–π –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä
clf = SVC(kernel='rbf')
clf.fit(features, y)
```

---

### 3. üéØ –í–∏–±—ñ—Ä Feature Extractor: Flowchart

```
–ü–æ—Ç—Ä—ñ–±–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å?
‚îú‚îÄ –¢–∞–∫ ‚Üí InceptionV3
‚îÇ         + Feature extraction only (100% frozen)
‚îÇ         + SVM (RBF) –∞–±–æ Gradient Boosting
‚îÇ         = 52-53% accuracy
‚îÇ
‚îî‚îÄ –ù—ñ ‚Üí –Ñ –æ–±–º–µ–∂–µ–Ω–Ω—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ/–ø–∞–º'—è—Ç—ñ?
        ‚îú‚îÄ –¢–∞–∫ ‚Üí VGG16 (–∫–æ–º–ø–∞–∫—Ç–Ω—ñ 512-D features)
        ‚îÇ         + –ú–æ–∂–Ω–∞ fine-tune –¥–ª—è +50% boost
        ‚îÇ         + SVM (RBF) –∞–±–æ MLP
        ‚îÇ         = 48-52% (feature extraction)
        ‚îÇ         = 69% (full fine-tuning!)
        ‚îÇ
        ‚îî‚îÄ –ù—ñ ‚Üí –ë–∞–ª–∞–Ω—Å ‚Üí VGG16 –∞–±–æ InceptionV3
                  –£–Ω–∏–∫–∞—Ç–∏ ‚Üí ResNet50 (—Å–∫–ª–∞–¥–Ω–∏–π –¥–ª—è –º–∞–ª–∏—Ö –¥–∞–Ω–∏—Ö)
                  –£–Ω–∏–∫–∞—Ç–∏ ‚Üí MobileNetV2 (–Ω–∏–∑—å–∫–∞ —è–∫—ñ—Å—Ç—å –Ω–∞ CIFAR-10)
```

---

### 4. ü§ñ –í–∏–±—ñ—Ä –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞: Decision Tree

```
–Ø–∫–∞ –≥–æ–ª–æ–≤–Ω–∞ –≤–∏–º–æ–≥–∞?

1. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å:
   ‚Üí SVM (RBF) üèÜ
   ‚úì 45.98% —Å–µ—Ä–µ–¥–Ω—è accuracy
   ‚úì ~9 —Å–µ–∫—É–Ω–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è
   ‚úì –ù–∞–π–∫—Ä–∞—â–∏–π –≤–∏–±—ñ—Ä!

2. –®–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è:
   ‚Üí Random Forest ‚ö°
   ‚úì 0.44 —Å–µ–∫—É–Ω–¥–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
   ‚úì 41.31% accuracy (–ø—Ä–∏–π–Ω—è—Ç–Ω–æ)
   
   –ê–±–æ ‚Üí Logistic Regression ‚ö°‚ö°
   ‚úì 2.63 —Å–µ–∫—É–Ω–¥–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
   ‚úì 42.58% accuracy
   ‚úì –í—ñ–¥–º—ñ–Ω–Ω–∏–π baseline

3. –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å:
   ‚Üí Logistic Regression üìä
   ‚úì –ú–æ–∂–Ω–∞ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ weights
   ‚úì –ó—Ä–æ–∑—É–º—ñ–ª—ñ decision boundaries

4. –Ñ –±–∞–≥–∞—Ç–æ —á–∞—Å—É:
   ‚Üí Gradient Boosting
   ‚úì 43.62% accuracy (—Ç—Ä–æ—Ö–∏ –∫—Ä–∞—â–µ)
   ‚ö†Ô∏è –ê–ª–µ 351 —Å–µ–∫—É–Ω–¥–∞ –Ω–∞–≤—á–∞–Ω–Ω—è!
   ‚ö†Ô∏è Trade-off –Ω–µ –≤–∞—Ä—Ç–æ

‚ùå –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:
   √ó K-Nearest Neighbors (34.13%) - curse of dimensionality
   √ó Naive Bayes (23.35%) - –ø–æ—Ä—É—à–µ–Ω–Ω—è assumption –ø—Ä–æ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å
   √ó Linear SVM (41.38%) - Logistic Regression –∫—Ä–∞—â–µ —ñ —à–≤–∏–¥—à–µ
```

---

### 5. üìä –ü—ñ–¥—Å—É–º–∫–æ–≤–∞ —Ç–∞–±–ª–∏—Ü—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π

| –°—Ü–µ–Ω–∞—Ä—ñ–π | Feature Extractor | –°—Ç—Ä–∞—Ç–µ–≥—ñ—è | Classifier | –û—á—ñ–∫—É–≤–∞–Ω–∞ Accuracy |
|----------|------------------|-----------|------------|-------------------|
| **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å** | InceptionV3 | 100% frozen | SVM (RBF) | **~53%** üèÜ |
| **–ë–∞–ª–∞–Ω—Å —è–∫—ñ—Å—Ç—å/—à–≤–∏–¥–∫—ñ—Å—Ç—å** | VGG16 | 100% frozen | SVM (RBF) | ~52% |
| **–ù–∞–π–∫—Ä–∞—â–∞ –∞–±—Å–æ–ª—é—Ç–Ω–∞ —è–∫—ñ—Å—Ç—å** | VGG16 | 0% frozen (fine-tune) | SVM (RBF) | **~69%** üèÜüèÜ |
| **–®–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è** | VGG16 | 100% frozen | Random Forest | ~47% ‚ö° |
| **–®–≤–∏–¥–∫–∏–π baseline** | VGG16 | 100% frozen | Logistic Regression | ~46% ‚ö° |
| **Mobile/Edge** | MobileNetV2 | 100% frozen | Logistic Regression | ~28% ‚ö°‚ö° |
| **–ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏** | VGG16 | Fine-tune | –ë—É–¥—å-—è–∫–∏–π | ~45-69% |

---

### 6. üéì –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ —ñ–Ω—Å–∞–π—Ç–∏ —Ç–∞ best practices

#### 6.1. Transfer Learning –ø—Ä–∞—Ü—é—î, –∞–ª–µ –Ω–µ –º–∞–≥—ñ—è

**–©–æ –ø—Ä–∞—Ü—é—î:**
- ‚úÖ Pretrained features –∑ ImageNet –¥–æ–±—Ä–µ –ø–µ—Ä–µ–Ω–æ—Å—è—Ç—å—Å—è –Ω–∞ CIFAR-10
- ‚úÖ –ù–∞–≤—ñ—Ç—å –±–µ–∑ fine-tuning –æ—Ç—Ä–∏–º—É—î–º–æ 45-53% accuracy
- ‚úÖ –ù–∞–±–∞–≥–∞—Ç–æ –∫—Ä–∞—â–µ –Ω—ñ–∂ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –Ω—É–ª—è (–ø–æ—Ç—Ä–µ–±—É–≤–∞–ª–æ –± >50K –∑–æ–±—Ä–∞–∂–µ–Ω—å)

**–û–±–º–µ–∂–µ–Ω–Ω—è:**
- ‚ö†Ô∏è 53% - –Ω–µ state-of-the-art (SOTA ~99% –Ω–∞ CIFAR-10)
- ‚ö†Ô∏è –ü—Ä–∏—á–∏–Ω–∞: –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ–ª—å–∫–∏ 5000 train –∑–æ–±—Ä–∞–∂–µ–Ω—å
- ‚ö†Ô∏è End-to-end trained CNN –∑ —É—Å—ñ–º–∞ 50K –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–∞–≤ –±–∏ ~95%
- ‚ö†Ô∏è Transfer learning - —Ü–µ –∫–æ–º–ø—Ä–æ–º—ñ—Å –¥–ª—è –º–∞–ª–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤

#### 6.2. –†–æ–∑–º—ñ—Ä features –º–∞—î –∑–Ω–∞—á–µ–Ω–Ω—è

**–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**
- InceptionV3 (2048-D) > VGG16 (512-D) > MobileNetV2 (1280-D compressed)
- –ê–ª–µ ResNet50 (2048-D) –ø—Ä–∞—Ü—é—î –≥—ñ—Ä—à–µ ‚Üí –¥–µ–ª–æ –Ω–µ —Ç—ñ–ª—å–∫–∏ –≤ —Ä–æ–∑–º—ñ—Ä—ñ!

**–í–∏—Å–Ω–æ–≤–æ–∫:**
- –Ø–∫—ñ—Å—Ç—å features –≤–∞–∂–ª–∏–≤—ñ—à–∞ –∑–∞ —Ä–æ–∑–º—ñ—Ä
- Inception modules –¥–∞—é—Ç—å –∫—Ä–∞—â—ñ features –Ω—ñ–∂ –ø—Ä–æ—Å—Ç–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞
- Compression (—è–∫ —É MobileNetV2) —à–∫–æ–¥–∏—Ç—å —è–∫–æ—Å—Ç—ñ

#### 6.3. SVM (RBF) - —ñ–¥–µ–∞–ª—å–Ω–∏–π –¥–ª—è CNN features

**–ß–æ–º—É:**
- ‚úÖ –í–∏—Å–æ–∫–æ–≤–∏–º—ñ—Ä–Ω—ñ –ø—Ä–æ—Å—Ç–æ—Ä–∏ (512-2048D) - –ø—Ä–∏—Ä–æ–¥–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ –¥–ª—è SVM
- ‚úÖ CNN features —á–∞—Å—Ç–æ –ª—ñ–Ω—ñ–π–Ω–æ –Ω–µ—Ä–æ–∑–¥—ñ–ª–∏–º—ñ –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ
- ‚úÖ RBF kernel –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å —É –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω–æ-–≤–∏–º—ñ—Ä–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä
- ‚úÖ Regularization —á–µ—Ä–µ–∑ C parameter –∑–∞–ø–æ–±—ñ–≥–∞—î overfitting

**–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è:**
- SVM (RBF): 45.98%
- Linear SVM: 41.38%
- –†—ñ–∑–Ω–∏—Ü—è: 4.6% - —Å—É—Ç—Ç—î–≤–æ!

#### 6.4. Ensemble –º–µ—Ç–æ–¥–∏ –ø–µ—Ä–µ–±—ñ–ª—å—à–µ–Ω—ñ –¥–ª—è —Ü—ñ—î—ó –∑–∞–¥–∞—á—ñ

**Gradient Boosting vs SVM (RBF):**
- GB: 43.62% accuracy, 351 —Å–µ–∫—É–Ω–¥
- SVM: 45.98% accuracy, 9 —Å–µ–∫—É–Ω–¥

**–í–∏—Å–Ω–æ–≤–æ–∫:**
- SVM –∫—Ä–∞—â–µ (+2.36%) —ñ —à–≤–∏–¥—à–∏–π (39√ó)!
- Ensemble –º–µ—Ç–æ–¥–∏ –∫–æ—Ä–∏—Å–Ω—ñ –¥–ª—è tabular data, –∞–ª–µ –¥–ª—è CNN features –ø—Ä–æ—Å—Ç—ñ—à—ñ –º–µ—Ç–æ–¥–∏ –ø—Ä–∞—Ü—é—é—Ç—å –∫—Ä–∞—â–µ
- –ú–æ–∂–ª–∏–≤–æ CNN –≤–∂–µ —Ä–æ–±–∏—Ç—å "ensembling" —á–µ—Ä–µ–∑ –±–∞–≥–∞—Ç–æ —à–∞—Ä—ñ–≤

#### 6.5. Batch Normalization - –¥—Ä—É–≥ —ñ –≤–æ—Ä–æ–≥

**–î—Ä—É–≥ (–ø—Ä–∏ feature extraction):**
- ‚úÖ –°—Ç–∞–±—ñ–ª—ñ–∑—É—î features
- ‚úÖ –ù–æ—Ä–º–∞–ª—ñ–∑—É—î —Ä–æ–∑–ø–æ–¥—ñ–ª–∏

**–í–æ—Ä–æ–≥ (–ø—Ä–∏ fine-tuning):**
- ‚ö†Ô∏è Running statistics –∑ ImageNet
- ‚ö†Ô∏è –ö–æ–Ω—Ñ–ª—ñ–∫—Ç—É—î –∑ –Ω–æ–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏
- ‚ö†Ô∏è –ü–æ—Ç—Ä–µ–±—É—î —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ–≥–æ –ø—ñ–¥—Ö–æ–¥—É:
  - Training mode vs inference mode
  - –í–∏–±—ñ—Ä–∫–æ–≤–µ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è BN layers

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
- –Ø–∫—â–æ –º–æ–¥–µ–ª—å –º–∞—î BN ‚Üí –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —è–∫ feature extractor
- –Ø–∫—â–æ —Ç—Ä–µ–±–∞ fine-tune ‚Üí –∑–∞–º–æ—Ä–æ–∑–∏—Ç–∏ BN layers –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ discriminative learning rates

#### 6.6. Confusion pairs –∑–∞–∫–æ–Ω–æ–º—ñ—Ä–Ω—ñ

**–û—á—ñ–∫—É–≤–∞–Ω—ñ –ø–æ–º–∏–ª–∫–∏ (–∑ —Ç–∏–ø–æ–≤–∏—Ö –ø–æ–º–∏–ª–æ–∫):**

1. **–í—ñ–∑—É–∞–ª—å–Ω–æ —Å—Ö–æ–∂—ñ –∫–ª–∞—Å–∏:**
   - –ö—ñ—Ç ‚Üî –°–æ–±–∞–∫–∞ (–¥–æ–º–∞—à–Ω—ñ —Ç–≤–∞—Ä–∏–Ω–∏, fur texture)
   - –ê–≤—Ç–æ–º–æ–±—ñ–ª—å ‚Üî –í–∞–Ω—Ç–∞–∂—ñ–≤–∫–∞ (–∫–æ–ª–µ—Å–∞, –≤—ñ–∫–Ω–∞, —Ñ–æ—Ä–º–∞)
   - –û–ª–µ–Ω—å ‚Üî –ö—ñ–Ω—å (—á–æ—Ç–∏—Ä–∏–Ω–æ–≥—ñ, —Å—Ö–æ–∂—ñ –ø—Ä–æ–ø–æ—Ä—Ü—ñ—ó)

2. **–ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ —Å—Ö–æ–∂—ñ:**
   - –ü—Ç–∞—Ö ‚Üî –õ—ñ—Ç–∞–∫ (–æ–±–∏–¥–≤–∞ –ª—ñ—Ç–∞—é—Ç—å, –∫–æ–Ω—Ç—É—Ä–∏)
   - –ö–æ—Ä–∞–±–µ–ª—å ‚Üî –í–∞–Ω—Ç–∞–∂—ñ–≤–∫–∞ (—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç)

3. **–†—ñ—à–µ–Ω–Ω—è –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫:**
   - üì∏ –ë—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö confusion –∫–ª–∞—Å—ñ–≤
   - üîÑ Data augmentation
   - üéØ Class-weighted loss
   - üß† Attention mechanisms
   - üìä Focal loss –¥–ª—è hard examples

#### 6.7. –ß–∞—Å - –∫—Ä–∏—Ç–∏—á–Ω–∏–π —Ñ–∞–∫—Ç–æ—Ä

**Feature extraction:**
- VGG16: ~30-60 —Å–µ–∫—É–Ω–¥ –Ω–∞ 5000 –∑–æ–±—Ä–∞–∂–µ–Ω—å
- InceptionV3: ~60-90 —Å–µ–∫—É–Ω–¥ (–ø–æ—Ç—Ä–µ–±—É—î resize)
- ResNet50: ~40-70 —Å–µ–∫—É–Ω–¥

**Classifier training:**
- –®–≤–∏–¥–∫—ñ (<5 —Å–µ–∫): Logistic Regression, Naive Bayes, KNN
- –°–µ—Ä–µ–¥–Ω—ñ (5-30 —Å–µ–∫): SVM (RBF), Random Forest, MLP
- –ü–æ–≤—ñ–ª—å–Ω—ñ (>60 —Å–µ–∫): Gradient Boosting, Linear SVM

**–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è:**
- ‚úÖ –í–∏—Ç—è–≥–Ω—É—Ç–∏ features –æ–¥–∏–Ω —Ä–∞–∑, –∑–±–µ—Ä–µ–≥—Ç–∏
- ‚úÖ –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–≤–∞—Ç–∏ –∑ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞–º–∏ –Ω–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∏—Ö features
- ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –±–∞—Ç—á–∏–Ω–≥ –¥–ª—è –µ–∫—Å—Ç—Ä–∞–∫—Ü—ñ—ó
- ‚úÖ GPU –¥–ª—è feature extraction, CPU –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤

---

## üöÄ –ü–û–î–ê–õ–¨–®–Ü –î–û–°–õ–Ü–î–ñ–ï–ù–ù–Ø

### 1. –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ

#### 1.1. –ë—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö
- üìà –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –≤—Å—ñ 50K train –∑–æ–±—Ä–∞–∂–µ–Ω—å CIFAR-10
- üìà –û—á—ñ–∫—É–≤–∞–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: 53% ‚Üí 65-70%

#### 1.2. Data Augmentation
```python
augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
    layers.RandomContrast(0.1),
])
```
- üìà –û—á—ñ–∫—É–≤–∞–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: +5-10%

#### 1.3. Ensemble –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ñ–≤
```python
# –û–±'—î–¥–Ω–∞—Ç–∏ features –∑ –¥–µ–∫—ñ–ª—å–∫–æ—Ö –º–æ–¥–µ–ª–µ–π
features_vgg16 = extract_vgg16(X)      # 512-D
features_inception = extract_inception(X)  # 2048-D
features_combined = np.hstack([features_vgg16, features_inception])  # 2560-D

clf = SVC(kernel='rbf')
clf.fit(features_combined, y)
```
- üìà –û—á—ñ–∫—É–≤–∞–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: +3-7%

#### 1.4. Advanced fine-tuning strategies

**Gradual Unfreezing:**
```python
# Epoch 1-3: –¢—ñ–ª—å–∫–∏ —Ç–æ–ø layers
for layer in base_model.layers[:-10]:
    layer.trainable = False

# Epoch 4-6: –ë—ñ–ª—å—à–µ layers
for layer in base_model.layers[:-20]:
    layer.trainable = False

# Epoch 7-10: –©–µ –±—ñ–ª—å—à–µ
```

**Discriminative Learning Rates:**
```python
# –†—ñ–∑–Ω—ñ learning rates –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —á–∞—Å—Ç–∏–Ω
early_layers_lr = 1e-5
mid_layers_lr = 1e-4
top_layers_lr = 1e-3
```

### 2. –Ü–Ω—à—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏

#### 2.1. EfficientNet (State-of-the-art)
```python
from tensorflow.keras.applications import EfficientNetB0

# Compound scaling: depth, width, resolution
# –û—á—ñ–∫—É–≤–∞–Ω–Ω—è: –Ω–∞–π–∫—Ä–∞—â–∞ —è–∫—ñ—Å—Ç—å
```

#### 2.2. Vision Transformer (ViT)
```python
# Transformer –¥–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω—å
# Pretrained –Ω–∞ ImageNet-21K
# –û—á—ñ–∫—É–≤–∞–Ω–Ω—è: –¥—É–∂–µ —Ö–æ—Ä–æ—à—ñ features
```

#### 2.3. DenseNet
```python
from tensorflow.keras.applications import DenseNet121

# Dense connections –º—ñ–∂ –≤—Å—ñ–º–∞ —à–∞—Ä–∞–º–∏
# Feature reuse
```

### 3. –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä —Ç—É–Ω–Ω—ñ–Ω–≥

#### 3.1. SVM (RBF) optimization
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]
}

grid = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)
grid.fit(features_train, y_train)

# –û—á—ñ–∫—É–≤–∞–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: +2-5%
```

#### 3.2. Optimal freezing depth
```python
# –ë—ñ–ª—å—à –¥–µ—Ç–∞–ª—å–Ω–∏–π –ø–æ—à—É–∫ –¥–ª—è VGG16
freeze_ratios = np.linspace(0, 1, 20)  # 20 —Ç–æ—á–æ–∫

# –ó–Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º—É–º –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
```

### 4. Explainability —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

#### 4.1. Grad-CAM
```python
# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–∞–∂–ª–∏–≤–∏—Ö —Ä–µ–≥—ñ–æ–Ω—ñ–≤
# –©–æ –º–æ–¥–µ–ª—å "–¥–∏–≤–∏—Ç—å—Å—è" –ø—Ä–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
```

#### 4.2. t-SNE / UMAP –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è features
```python
from sklearn.manifold import TSNE
import umap

# –ü—Ä–æ—î–∫—Ü—ñ—è –≤–∏—Å–æ–∫–æ–≤–∏–º—ñ—Ä–Ω–∏—Ö features —É 2D
# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –∫–ª–∞—Å—ñ–≤
```

#### 4.3. Feature importance
```python
# –Ø–∫—ñ dimensions —É feature vector –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ?
# –ê–Ω–∞–ª—ñ–∑ –¥–ª—è Random Forest / Gradient Boosting
```

### 5. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è Production

#### 5.1. Knowledge Distillation
```python
# Teacher: InceptionV3 + SVM (53% accuracy)
# Student: MobileNetV2 + –ø—Ä–æ—Å—Ç–∏–π classifier
# –¶—ñ–ª—å: –ó–±–µ—Ä–µ–≥—Ç–∏ —è–∫—ñ—Å—Ç—å, –∑–º–µ–Ω—à–∏—Ç–∏ —Ä–æ–∑–º—ñ—Ä
```

#### 5.2. Quantization
```python
# Float32 ‚Üí Int8
# 4√ó –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É
# –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –≤—Ç—Ä–∞—Ç–∞ —è–∫–æ—Å—Ç—ñ
```

#### 5.3. Pruning
```python
# –í–∏–¥–∞–ª–∏—Ç–∏ –Ω–µ–≤–∞–∂–ª–∏–≤—ñ weights
# Sparse model –¥–ª—è —à–≤–∏–¥—à–æ—ó —ñ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—ó
```

---

## üìö –ü–†–ê–ö–¢–ò–ß–ù–Ü –í–ò–°–ù–û–í–ö–ò

### –î–ª—è –ø–æ—á–∞—Ç–∫—ñ–≤—Ü—ñ–≤:

1. **–ü–æ—á–Ω—ñ—Ç—å –∑ –ø—Ä–æ—Å—Ç–æ–≥–æ:**
   - VGG16 —è–∫ feature extractor (100% frozen)
   - Logistic Regression —è–∫ baseline
   - –û—á—ñ–∫—É–π—Ç–µ ~46% accuracy –∑–∞ 5 —Ö–≤–∏–ª–∏–Ω

2. **–ü–æ—Ç—ñ–º —Å–ø—Ä–æ–±—É–π—Ç–µ:**
   - SVM (RBF) –∑–∞–º—ñ—Å—Ç—å Logistic Regression ‚Üí +6% accuracy
   - InceptionV3 –∑–∞–º—ñ—Å—Ç—å VGG16 ‚Üí —â–µ +1-2% accuracy

3. **–Ø–∫—â–æ —î —á–∞—Å:**
   - –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–π—Ç–µ –∑ fine-tuning VGG16
   - –ú–æ–∂–µ—Ç–µ –æ—Ç—Ä–∏–º–∞—Ç–∏ –¥–æ 69% accuracy!

### –î–ª—è –¥–æ—Å–≤—ñ–¥—á–µ–Ω–∏—Ö:

1. **–î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —è–∫–æ—Å—Ç—ñ:**
   - InceptionV3 + SVM (RBF) feature extraction
   - –ê–ë–û VGG16 full fine-tuning + SVM (RBF)
   - Ensemble –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ñ–≤
   - –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä —Ç—É–Ω–Ω—ñ–Ω–≥
   - –û—á—ñ–∫—É–π—Ç–µ 70-75% accuracy

2. **–î–ª—è production:**
   - –ë–∞–ª–∞–Ω—Å —è–∫—ñ—Å—Ç—å/—à–≤–∏–¥–∫—ñ—Å—Ç—å: VGG16 + Logistic Regression
   - Knowledge distillation –¥–ª—è mobile
   - Quantization –¥–ª—è edge devices

3. **–î–ª—è –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å:**
   - EfficientNet, ViT, DenseNet
   - Advanced fine-tuning strategies
   - Explainability methods

---

## üéØ –ü–Ü–î–°–£–ú–û–ö

### ‚úÖ –©–æ –º–∏ –¥—ñ–∑–Ω–∞–ª–∏—Å—è:

1. **Transfer Learning –ø—Ä–∞—Ü—é—î** - –Ω–∞–≤—ñ—Ç—å –Ω–∞ 5000 –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–æ—Å—è–≥–∞—î–º–æ 53-69% accuracy
2. **InceptionV3 - –Ω–∞–π–∫—Ä–∞—â–∏–π feature extractor** –¥–ª—è CIFAR-10 (44.34% —Å–µ—Ä–µ–¥–Ω—è)
3. **SVM (RBF) - –Ω–∞–π–∫—Ä–∞—â–∏–π –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä** –¥–ª—è CNN features (45.98% —Å–µ—Ä–µ–¥–Ω—è)
4. **VGG16 + Full fine-tuning = 69% accuracy** - –Ω–∞–π–∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!
5. **ResNet50 –∫—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ frozen** - fine-tuning –¥–∞—î –≥—ñ—Ä—à—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
6. **MobileNetV2 –Ω–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å** –¥–ª—è CIFAR-10 (—Ç—ñ–ª—å–∫–∏ 26% accuracy)
7. **Gradient Boosting –Ω–µ –≤–∞—Ä—Ç–æ** - SVM –∫—Ä–∞—â–µ —ñ –≤ 39√ó —à–≤–∏–¥—à–∏–π
8. **–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –º–∞—î –∑–Ω–∞—á–µ–Ω–Ω—è** - —Ä—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å —Ä—ñ–∑–Ω–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π
9. **Batch Normalization —É—Å–∫–ª–∞–¥–Ω—é—î fine-tuning** - –∫—Ä–∞—â–µ freeze –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π

### üèÜ –ù–∞–π–∫—Ä–∞—â—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó:

| –ú—ñ—Å—Ü–µ | –ö–æ–º–±—ñ–Ω–∞—Ü—ñ—è | Accuracy | Use Case |
|-------|-----------|----------|----------|
| ü•á | **VGG16 Full Fine-tuning + SVM (RBF)** | **69.05%** | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å |
| ü•à | **InceptionV3 + SVM (RBF)** | **53.30%** | –ë–µ–∑ fine-tuning |
| ü•â | **VGG16 + SVM (RBF)** | **52.15%** | –ë–∞–ª–∞–Ω—Å —à–≤–∏–¥–∫—ñ—Å—Ç—å/—è–∫—ñ—Å—Ç—å |

### üí° –ì–æ–ª–æ–≤–Ω–∏–π —ñ–Ω—Å–∞–π—Ç:

> **Transfer Learning - —Ü–µ –Ω–µ –ø—Ä–æ—Å—Ç–æ "–∑–∞–º–æ—Ä–æ–∑–∏—Ç–∏ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å —ñ –¥–æ–¥–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä". –¶–µ –º–∏—Å—Ç–µ—Ü—Ç–≤–æ –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å—É –º—ñ–∂:
> - –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º pretrained –∑–Ω–∞–Ω—å (–∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è)
> - –ê–¥–∞–ø—Ç–∞—Ü—ñ—î—é –¥–æ –Ω–æ–≤–æ—ó –∑–∞–¥–∞—á—ñ (fine-tuning)
> - –í–∏–±–æ—Ä–æ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—ó –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ (–ø—Ä–æ—Å—Ç–∞ vs —Å–∫–ª–∞–¥–Ω–∞)
> - –í–∏–±–æ—Ä–æ–º –ø—ñ–¥—Ö–æ–¥—è—â–æ–≥–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞ (–ª—ñ–Ω—ñ–π–Ω–∏–π vs –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏–π)**

**–£—Å–ø—ñ—Ö –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ü–∏—Ö trade-offs —ñ –≤–∏–±–æ—Ä—É —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó, –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—ó –¥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –∑–∞–¥–∞—á—ñ, –¥–∞—Ç–∞—Å–µ—Ç—É —Ç–∞ –æ–±–º–µ–∂–µ–Ω—å! üöÄ**

---

## üìû –ö–æ–Ω—Ç–∞–∫—Ç —Ç–∞ –ø–æ–¥—è–∫–∞

–î—è–∫—É—é –∑–∞ —É–≤–∞–≥—É –¥–æ —Ü—å–æ–≥–æ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è! –°–ø–æ–¥—ñ–≤–∞—é—Å—è, —Ü—ñ –≤–∏—Å–Ω–æ–≤–∫–∏ –¥–æ–ø–æ–º–æ–∂—É—Ç—å —É –≤–∞—à–∏—Ö –≤–ª–∞—Å–Ω–∏—Ö –ø—Ä–æ—î–∫—Ç–∞—Ö –∑ Transfer Learning.

**Happy Transfer Learning! üéìüöÄ**

---

*–î–∞—Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è: 2025*  
*–ü—Ä–æ—î–∫—Ç: Deep Learning - Transfer Learning Benchmarking*  
*–î–∞—Ç–∞—Å–µ—Ç: CIFAR-10 (5000 train, 2000 test)*  
*Framework: TensorFlow/Keras + scikit-learn*

