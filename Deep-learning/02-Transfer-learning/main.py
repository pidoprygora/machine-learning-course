"""
Transfer Learning —Ç–∞ –ë–µ–Ω—á–º–∞—Ä–∫—ñ–Ω–≥ –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
–§–æ–∫—É—Å: –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤ –ø–æ–≤–µ—Ä—Ö —Ä—ñ–∑–Ω–∏—Ö feature extractors

–ú–µ—Ç–∞: –î–æ—Å–ª—ñ–¥–∏—Ç–∏ —è–∫ —Ä—ñ–∑–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó pretrained –º–æ–¥–µ–ª–µ–π —Ç–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
–≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å, –∞ —Ç–∞–∫–æ–∂ –≤–∏–≤—á–∏—Ç–∏ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –≤—ñ–¥ –≥–ª–∏–±–∏–Ω–∏ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è.

Feature Extractors:
- VGG16 (pretrained ImageNet)
- VGG19 (pretrained ImageNet)
- ResNet50 (pretrained ImageNet)
- MobileNetV2 (pretrained ImageNet)
- InceptionV3 (pretrained ImageNet)

–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏:
- Logistic Regression
- SVM (–ª—ñ–Ω—ñ–π–Ω–∏–π —Ç–∞ RBF)
- Random Forest
- Gradient Boosting
- K-Nearest Neighbors
- Naive Bayes
- MLP (Neural Network)

–î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è:
- –¢–æ—á–Ω—ñ—Å—Ç—å –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π
- –í–ø–ª–∏–≤ –≥–ª–∏–±–∏–Ω–∏ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è —à–∞—Ä—ñ–≤
- Confusion matrices
- –¢–∏–ø–æ–≤—ñ –ø–æ–º–∏–ª–∫–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
- –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—ó
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.applications import (
    VGG16, VGG19, ResNet50, MobileNetV2, InceptionV3
)
from tensorflow.keras.datasets import cifar10

from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    f1_score, precision_score, recall_score
)

import time
from pathlib import Path
from collections import defaultdict

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
tf.random.set_seed(42)
np.random.seed(42)

sns.set(style="whitegrid", context="notebook")
plt.rcParams["figure.figsize"] = (14, 8)
plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'DejaVu Sans'

OUTPUT_DIR = Path("results")
OUTPUT_DIR.mkdir(exist_ok=True)

# –ù–∞–∑–≤–∏ –∫–ª–∞—Å—ñ–≤ CIFAR-10
CIFAR10_CLASSES = [
    '–ª—ñ—Ç–∞–∫', '–∞–≤—Ç–æ–º–æ–±—ñ–ª—å', '–ø—Ç–∞—Ö', '–∫—ñ—Ç', '–æ–ª–µ–Ω—å',
    '—Å–æ–±–∞–∫–∞', '–∂–∞–±–∞', '–∫—ñ–Ω—å', '–∫–æ—Ä–∞–±–µ–ª—å', '–≤–∞–Ω—Ç–∞–∂—ñ–≤–∫–∞'
]


def print_section(title):
    """–í–∏–≤–æ–¥–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü—ñ—ó"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)


def load_cifar10_data(n_train=5000, n_test=2000):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î CIFAR-10 –¥–∞—Ç–∞—Å–µ—Ç"""
    print_section("–ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø CIFAR-10")
    
    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    
    # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
    X_train = X_train.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0
    
    # Flatten labels
    y_train = y_train.flatten()
    y_test = y_test.flatten()
    
    # –ü—ñ–¥–º–Ω–æ–∂–∏–Ω–∞ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
    indices_train = np.random.choice(len(X_train), n_train, replace=False)
    indices_test = np.random.choice(len(X_test), n_test, replace=False)
    
    X_train = X_train[indices_train]
    y_train = y_train[indices_train]
    X_test = X_test[indices_test]
    y_test = y_test[indices_test]
    
    print(f"‚úì Train set: {X_train.shape} | {len(np.unique(y_train))} –∫–ª–∞—Å—ñ–≤")
    print(f"‚úì Test set: {X_test.shape} | {len(np.unique(y_test))} –∫–ª–∞—Å—ñ–≤")
    
    return X_train, y_train, X_test, y_test


def visualize_dataset_samples(X_train, y_train):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ –¥–∞—Ç–∞—Å–µ—Ç—É"""
    print_section("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –î–ê–¢–ê–°–ï–¢–£")
    
    fig, axes = plt.subplots(2, 10, figsize=(15, 3.5))
    axes = axes.flatten()
    
    for i in range(10):
        # –ü–æ 2 –ø—Ä–∏–∫–ª–∞–¥–∏ –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É
        indices = np.where(y_train == i)[0][:2]
        
        for j, idx in enumerate(indices):
            ax = axes[i + j * 10]
            ax.imshow(X_train[idx])
            if j == 0:
                ax.set_title(f'{CIFAR10_CLASSES[i]}', fontsize=10, weight='bold')
            ax.axis('off')
    
    plt.suptitle('CIFAR-10: –ü—Ä–∏–∫–ª–∞–¥–∏ –∑–æ–±—Ä–∞–∂–µ–Ω—å', fontsize=14, weight='bold')
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'dataset_samples.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/dataset_samples.png")
    plt.show()


def build_feature_extractor(model_name, freeze_layers=None):
    """
    –°—Ç–≤–æ—Ä—é—î feature extractor –∑ pretrained –º–æ–¥–µ–ª—ñ
    
    Args:
        model_name: –Ω–∞–∑–≤–∞ –º–æ–¥–µ–ª—ñ ('VGG16', 'VGG19', 'ResNet50', 'MobileNetV2', 'InceptionV3')
        freeze_layers: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —à–∞—Ä—ñ–≤ –¥–ª—è –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è (None = –≤—Å—ñ)
    """
    input_shape = (32, 32, 3)
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ input –¥–ª—è —Ä–µ—Å–∞–π–∑—É
    inputs = keras.Input(shape=input_shape)
    
    # –†–µ—Å–∞–π–∑ –¥–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É –¥–ª—è pretrained –º–æ–¥–µ–ª–µ–π
    if model_name == 'InceptionV3':
        target_size = 75  # –º—ñ–Ω—ñ–º—É–º –¥–ª—è InceptionV3
    else:
        target_size = 32
    
    x = inputs
    if target_size != 32:
        x = layers.Resizing(target_size, target_size)(x)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è base –º–æ–¥–µ–ª—ñ
    if model_name == 'VGG16':
        base_model = VGG16(weights='imagenet', include_top=False,
                          input_shape=(target_size, target_size, 3))
    elif model_name == 'VGG19':
        base_model = VGG19(weights='imagenet', include_top=False,
                          input_shape=(target_size, target_size, 3))
    elif model_name == 'ResNet50':
        base_model = ResNet50(weights='imagenet', include_top=False,
                             input_shape=(target_size, target_size, 3))
    elif model_name == 'MobileNetV2':
        base_model = MobileNetV2(weights='imagenet', include_top=False,
                                input_shape=(target_size, target_size, 3))
    elif model_name == 'InceptionV3':
        base_model = InceptionV3(weights='imagenet', include_top=False,
                                input_shape=(target_size, target_size, 3))
    else:
        raise ValueError(f"Unknown model: {model_name}")
    
    # –ó–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è —à–∞—Ä—ñ–≤
    if freeze_layers is not None:
        # –†–æ–∑–º–æ—Ä–æ–∂—É—î–º–æ –≤—Å—ñ —à–∞—Ä–∏
        for layer in base_model.layers:
            layer.trainable = True
        
        # –ó–∞–º–æ—Ä–æ–∂—É—î–º–æ –ø–µ—Ä—à—ñ freeze_layers
        for layer in base_model.layers[:freeze_layers]:
            layer.trainable = False
    else:
        # –ó–∞–º–æ—Ä–æ–∂—É—î–º–æ –≤—Å—ñ —à–∞—Ä–∏
        base_model.trainable = False
    
    # –î–æ–¥–∞—î–º–æ pooling
    x = base_model(x, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    
    # Feature extractor
    feature_extractor = models.Model(inputs, x, name=f'{model_name}_extractor')
    
    return feature_extractor, base_model


def extract_features(model, X_data, batch_size=64):
    """–í–∏—Ç—è–≥—É—î –æ–∑–Ω–∞–∫–∏ –∑ –¥–∞–Ω–∏—Ö"""
    features = model.predict(X_data, batch_size=batch_size, verbose=0)
    return features


def get_classifiers():
    """–ü–æ–≤–µ—Ä—Ç–∞—î —Å–ª–æ–≤–Ω–∏–∫ –∑ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞–º–∏"""
    classifiers = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Linear SVM': LinearSVC(max_iter=3000, random_state=42),
        'SVM (RBF)': SVC(kernel='rbf', random_state=42, max_iter=1000),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),
        'Naive Bayes': GaussianNB(),
        'MLP': MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)
    }
    return classifiers


def benchmark_classifiers(features_train, y_train, features_test, y_test, scaler=None):
    """
    –ë–µ–Ω—á–º–∞—Ä–∫ —Ä—ñ–∑–Ω–∏—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
    
    Returns:
        DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    classifiers = get_classifiers()
    results = {}
    predictions = {}
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è (–≤–∞–∂–ª–∏–≤–æ –¥–ª—è –¥–µ—è–∫–∏—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤)
    if scaler is None:
        scaler = StandardScaler()
        features_train_scaled = scaler.fit_transform(features_train)
    else:
        features_train_scaled = scaler.transform(features_train)
    
    features_test_scaled = scaler.transform(features_test)
    
    for clf_name, clf in classifiers.items():
        print(f"\n  [{list(classifiers.keys()).index(clf_name) + 1}/{len(classifiers)}] {clf_name}...")
        
        start_time = time.time()
        
        try:
            # –ù–∞–≤—á–∞–Ω–Ω—è
            clf.fit(features_train_scaled, y_train)
            train_time = time.time() - start_time
            
            # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
            start_inference = time.time()
            y_pred = clf.predict(features_test_scaled)
            inference_time = time.time() - start_inference
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred, average='weighted')
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted')
            
            results[clf_name] = {
                'Accuracy': accuracy,
                'F1-Score': f1,
                'Precision': precision,
                'Recall': recall,
                'Train Time (s)': train_time,
                'Inference Time (s)': inference_time
            }
            
            predictions[clf_name] = y_pred
            
            print(f"    ‚úì Accuracy: {accuracy:.4f} | F1: {f1:.4f} | Train: {train_time:.2f}s")
            
        except Exception as e:
            print(f"    ‚ö† –ü–æ–º–∏–ª–∫–∞: {e}")
            results[clf_name] = {
                'Accuracy': np.nan,
                'F1-Score': np.nan,
                'Precision': np.nan,
                'Recall': np.nan,
                'Train Time (s)': np.nan,
                'Inference Time (s)': np.nan
            }
            predictions[clf_name] = None
    
    results_df = pd.DataFrame(results).T
    return results_df, predictions, scaler


def benchmark_extractors_with_classifiers(X_train, y_train, X_test, y_test):
    """–ë–µ–Ω—á–º–∞—Ä–∫ —Ä—ñ–∑–Ω–∏—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ñ–≤ —Ç–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤"""
    print_section("–ë–ï–ù–ß–ú–ê–†–ö: –ï–ö–°–¢–†–ê–ö–¢–û–†–ò √ó –ö–õ–ê–°–ò–§–Ü–ö–ê–¢–û–†–ò")
    
    extractors = ['VGG16', 'VGG19', 'ResNet50', 'MobileNetV2', 'InceptionV3']
    all_results = {}
    all_predictions = {}
    all_features = {}
    scalers = {}
    
    for extractor_name in extractors:
        print(f"\n{'='*80}")
        print(f"  Feature Extractor: {extractor_name}")
        print('='*80)
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
        print(f"\n  –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {extractor_name}...")
        try:
            feature_extractor, _ = build_feature_extractor(extractor_name)
            
            # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –æ–∑–Ω–∞–∫
            print(f"  –ï–∫—Å—Ç—Ä–∞–∫—Ü—ñ—è –æ–∑–Ω–∞–∫...")
            start_time = time.time()
            features_train = extract_features(feature_extractor, X_train)
            features_test = extract_features(feature_extractor, X_test)
            extraction_time = time.time() - start_time
            
            print(f"  ‚úì –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: {features_train.shape[1]}")
            print(f"  ‚úì –ß–∞—Å –µ–∫—Å—Ç—Ä–∞–∫—Ü—ñ—ó: {extraction_time:.2f}s")
            
            all_features[extractor_name] = (features_train, features_test)
            
            # –ë–µ–Ω—á–º–∞—Ä–∫ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
            results_df, predictions, scaler = benchmark_classifiers(
                features_train, y_train, features_test, y_test
            )
            
            all_results[extractor_name] = results_df
            all_predictions[extractor_name] = predictions
            scalers[extractor_name] = scaler
            
            # –í–∏–≤–µ–¥–µ–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—ñ
            print(f"\n  üìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è {extractor_name}:")
            print(results_df[['Accuracy', 'F1-Score', 'Train Time (s)']].round(4).to_string())
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
            safe_name = extractor_name.replace('/', '_')
            results_df.to_csv(OUTPUT_DIR / f'benchmark_{safe_name}.csv')
            print(f"\n  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/benchmark_{safe_name}.csv")
            
        except Exception as e:
            print(f"  ‚ö† –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ä–æ–±–æ—Ç—ñ –∑ {extractor_name}: {e}")
            continue
    
    return all_results, all_predictions, all_features, scalers


def study_freezing_depth(X_train, y_train, X_test, y_test, model_name='VGG16'):
    """
    –î–æ—Å–ª—ñ–¥–∂—É—î –≤–ø–ª–∏–≤ –≥–ª–∏–±–∏–Ω–∏ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è —à–∞—Ä—ñ–≤ –Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å
    
    Args:
        model_name: –º–æ–¥–µ–ª—å –¥–ª—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É
    """
    print_section(f"–î–û–°–õ–Ü–î–ñ–ï–ù–ù–Ø –ì–õ–ò–ë–ò–ù–ò –ó–ê–ú–û–†–û–ñ–£–í–ê–ù–ù–Ø: {model_name}")
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å —â–æ–± –¥—ñ–∑–Ω–∞—Ç–∏—Å—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å —à–∞—Ä—ñ–≤
    _, base_model = build_feature_extractor(model_name)
    total_layers = len(base_model.layers)
    
    print(f"\n  –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —à–∞—Ä—ñ–≤: {total_layers}")
    
    # –¢–µ—Å—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ –≥–ª–∏–±–∏–Ω–∏ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è
    freeze_configs = [
        0,  # –í—Å—ñ —à–∞—Ä–∏ trainable
        total_layers // 4,
        total_layers // 2,
        3 * total_layers // 4,
        total_layers  # –í—Å—ñ —à–∞—Ä–∏ frozen
    ]
    
    results = {}
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ –æ–¥–∏–Ω –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä (Logistic Regression) –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
    print(f"\n  –¢–µ—Å—Ç—É—î–º–æ {len(freeze_configs)} –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è...")
    print(f"  –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä: Logistic Regression\n")
    
    for freeze_layers in freeze_configs:
        percent_frozen = (freeze_layers / total_layers) * 100
        print(f"  [{freeze_configs.index(freeze_layers) + 1}/{len(freeze_configs)}] " +
              f"–ó–∞–º–æ—Ä–æ–∂–µ–Ω–æ: {freeze_layers}/{total_layers} —à–∞—Ä—ñ–≤ ({percent_frozen:.0f}%)...")
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
            feature_extractor, _ = build_feature_extractor(model_name, freeze_layers)
            
            # –Ø–∫—â–æ —î –Ω–µ–∑–∞–º–æ—Ä–æ–∂–µ–Ω—ñ —à–∞—Ä–∏, –ø–æ—Ç—Ä—ñ–±–Ω–æ –¥–æ—Ç—Ä–µ–Ω—É–≤–∞—Ç–∏
            if freeze_layers < total_layers:
                print(f"      –î–æ—Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –Ω–µ–∑–∞–º–æ—Ä–æ–∂–µ–Ω–∏—Ö —à–∞—Ä—ñ–≤...")
                
                # –î–æ–¥–∞—î–º–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–∏–π —à–∞—Ä
                inputs = feature_extractor.input
                x = feature_extractor.output
                outputs = layers.Dense(10, activation='softmax')(x)
                full_model = models.Model(inputs, outputs)
                
                full_model.compile(
                    optimizer=keras.optimizers.Adam(learning_rate=0.0001),
                    loss='sparse_categorical_crossentropy',
                    metrics=['accuracy']
                )
                
                # Fine-tuning (–∫—ñ–ª—å–∫–∞ epochs)
                full_model.fit(
                    X_train, y_train,
                    validation_split=0.2,
                    epochs=3,
                    batch_size=64,
                    verbose=0
                )
                
                # –û–Ω–æ–≤–ª—é—î–º–æ feature extractor
                feature_extractor = models.Model(
                    inputs=full_model.input,
                    outputs=full_model.layers[-2].output  # –î–æ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ Dense
                )
            
            # –ï–∫—Å—Ç—Ä–∞–∫—Ü—ñ—è –æ–∑–Ω–∞–∫
            features_train = extract_features(feature_extractor, X_train)
            features_test = extract_features(feature_extractor, X_test)
            
            # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
            scaler = StandardScaler()
            features_train_scaled = scaler.fit_transform(features_train)
            features_test_scaled = scaler.transform(features_test)
            
            clf = LogisticRegression(max_iter=1000, random_state=42)
            clf.fit(features_train_scaled, y_train)
            
            y_pred = clf.predict(features_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            
            results[freeze_layers] = {
                'frozen_layers': freeze_layers,
                'percent_frozen': percent_frozen,
                'accuracy': accuracy
            }
            
            print(f"      ‚úì Accuracy: {accuracy:.4f}")
            
        except Exception as e:
            print(f"      ‚ö† –ü–æ–º–∏–ª–∫–∞: {e}")
            results[freeze_layers] = {
                'frozen_layers': freeze_layers,
                'percent_frozen': percent_frozen,
                'accuracy': np.nan
            }
    
    results_df = pd.DataFrame(results).T
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    visualize_freezing_depth(results_df, model_name)
    
    return results_df


def visualize_freezing_depth(results_df, model_name):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å accuracy –≤—ñ–¥ –≥–ª–∏–±–∏–Ω–∏ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è"""
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # –ì—Ä–∞—Ñ—ñ–∫ 1: Accuracy vs. –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–∏—Ö —à–∞—Ä—ñ–≤
    ax = axes[0]
    ax.plot(results_df['frozen_layers'], results_df['accuracy'], 
            marker='o', linewidth=2, markersize=8, color='#2E86AB')
    ax.fill_between(results_df['frozen_layers'], results_df['accuracy'], 
                     alpha=0.3, color='#2E86AB')
    ax.set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–∏—Ö —à–∞—Ä—ñ–≤', fontsize=12, weight='bold')
    ax.set_ylabel('Accuracy', fontsize=12, weight='bold')
    ax.set_title(f'{model_name}: –í–ø–ª–∏–≤ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è —à–∞—Ä—ñ–≤', fontsize=13, weight='bold')
    ax.grid(alpha=0.3)
    
    # –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞ –≥—Ä–∞—Ñ—ñ–∫
    for idx, row in results_df.iterrows():
        ax.annotate(f'{row["accuracy"]:.3f}', 
                   xy=(row['frozen_layers'], row['accuracy']),
                   xytext=(5, 5), textcoords='offset points',
                   fontsize=9)
    
    # –ì—Ä–∞—Ñ—ñ–∫ 2: Accuracy vs. –≤—ñ–¥—Å–æ—Ç–æ–∫ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–∏—Ö —à–∞—Ä—ñ–≤
    ax = axes[1]
    ax.plot(results_df['percent_frozen'], results_df['accuracy'],
            marker='s', linewidth=2, markersize=8, color='#A23B72')
    ax.fill_between(results_df['percent_frozen'], results_df['accuracy'],
                     alpha=0.3, color='#A23B72')
    ax.set_xlabel('–í—ñ–¥—Å–æ—Ç–æ–∫ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–∏—Ö —à–∞—Ä—ñ–≤ (%)', fontsize=12, weight='bold')
    ax.set_ylabel('Accuracy', fontsize=12, weight='bold')
    ax.set_title(f'{model_name}: Accuracy vs. Freeze Ratio', fontsize=13, weight='bold')
    ax.grid(alpha=0.3)
    
    # –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è
    for idx, row in results_df.iterrows():
        ax.annotate(f'{row["accuracy"]:.3f}',
                   xy=(row['percent_frozen'], row['accuracy']),
                   xytext=(5, 5), textcoords='offset points',
                   fontsize=9)
    
    plt.suptitle(f'–î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≥–ª–∏–±–∏–Ω–∏ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è —à–∞—Ä—ñ–≤: {model_name}',
                 fontsize=14, weight='bold')
    plt.tight_layout()
    
    safe_name = model_name.replace('/', '_')
    plt.savefig(OUTPUT_DIR / f'freezing_depth_{safe_name}.png', dpi=300, bbox_inches='tight')
    print(f"\n‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/freezing_depth_{safe_name}.png")
    plt.show()


def create_comparison_heatmap(all_results):
    """–°—Ç–≤–æ—Ä—é—î heatmap –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—Å—ñ—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π"""
    print_section("HEATMAP: –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –í–°–Ü–• –ö–û–ú–ë–Ü–ù–ê–¶–Ü–ô")
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞—Ç—Ä–∏—Ü—é accuracy
    extractors = list(all_results.keys())
    classifiers = list(all_results[extractors[0]].index)
    
    matrix = np.zeros((len(classifiers), len(extractors)))
    
    for i, clf in enumerate(classifiers):
        for j, ext in enumerate(extractors):
            matrix[i, j] = all_results[ext].loc[clf, 'Accuracy']
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    fig, ax = plt.subplots(figsize=(12, 8))
    
    im = ax.imshow(matrix, cmap='YlGnBu', aspect='auto', vmin=0, vmax=1)
    
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –æ—Å–µ–π
    ax.set_xticks(np.arange(len(extractors)))
    ax.set_yticks(np.arange(len(classifiers)))
    ax.set_xticklabels(extractors, rotation=45, ha='right')
    ax.set_yticklabels(classifiers)
    
    # –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –≤ –∫–ª—ñ—Ç–∏–Ω–∫–∏
    for i in range(len(classifiers)):
        for j in range(len(extractors)):
            text = ax.text(j, i, f'{matrix[i, j]:.3f}',
                          ha="center", va="center", color="black" if matrix[i, j] > 0.5 else "white",
                          fontsize=9, weight='bold')
    
    ax.set_xlabel('Feature Extractor', fontsize=12, weight='bold')
    ax.set_ylabel('Classifier', fontsize=12, weight='bold')
    ax.set_title('Accuracy: –ï–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∏ √ó –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏', fontsize=14, weight='bold')
    
    # Colorbar
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('Accuracy', fontsize=11, weight='bold')
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'heatmap_all_combinations.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/heatmap_all_combinations.png")
    plt.show()


def plot_confusion_matrices(all_predictions, y_test, extractor_name, top_n=4):
    """
    –í—ñ–∑—É–∞–ª—ñ–∑—É—î confusion matrices –¥–ª—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
    
    Args:
        all_predictions: —Å–ª–æ–≤–Ω–∏–∫ –∑ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è–º–∏
        y_test: —Å–ø—Ä–∞–≤–∂–Ω—ñ –º—ñ—Ç–∫–∏
        extractor_name: –Ω–∞–∑–≤–∞ –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
        top_n: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–ø –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
    """
    predictions = all_predictions[extractor_name]
    
    # –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ None –∑–Ω–∞—á–µ–Ω–Ω—è
    valid_predictions = {k: v for k, v in predictions.items() if v is not None}
    
    if len(valid_predictions) == 0:
        print(f"‚ö† –ù–µ–º–∞—î –≤–∞–ª—ñ–¥–Ω–∏—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å –¥–ª—è {extractor_name}")
        return
    
    # –ë–µ—Ä–µ–º–æ top_n –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤ (–∑–∞ –∞–ª—Ñ–∞–≤—ñ—Ç–æ–º, –º–æ–∂–Ω–∞ –∑–º—ñ–Ω–∏—Ç–∏ –ª–æ–≥—ñ–∫—É)
    classifiers = list(valid_predictions.keys())[:top_n]
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    axes = axes.flatten()
    
    for idx, clf_name in enumerate(classifiers):
        if idx >= len(axes):
            break
            
        ax = axes[idx]
        y_pred = valid_predictions[clf_name]
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
        im = ax.imshow(cm_normalized, cmap='Blues', aspect='auto', vmin=0, vmax=1)
        
        # –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è
        for i in range(len(CIFAR10_CLASSES)):
            for j in range(len(CIFAR10_CLASSES)):
                text = ax.text(j, i, f'{cm[i, j]}\n({cm_normalized[i, j]:.2f})',
                             ha="center", va="center",
                             color="white" if cm_normalized[i, j] > 0.5 else "black",
                             fontsize=7)
        
        ax.set_xticks(np.arange(len(CIFAR10_CLASSES)))
        ax.set_yticks(np.arange(len(CIFAR10_CLASSES)))
        ax.set_xticklabels(CIFAR10_CLASSES, rotation=45, ha='right', fontsize=8)
        ax.set_yticklabels(CIFAR10_CLASSES, fontsize=8)
        ax.set_xlabel('–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–∏–π –∫–ª–∞—Å', fontsize=10)
        ax.set_ylabel('–°–ø—Ä–∞–≤–∂–Ω—ñ–π –∫–ª–∞—Å', fontsize=10)
        
        accuracy = accuracy_score(y_test, y_pred)
        ax.set_title(f'{clf_name}\nAccuracy: {accuracy:.3f}', fontsize=11, weight='bold')
    
    plt.suptitle(f'Confusion Matrices: {extractor_name}', fontsize=14, weight='bold')
    plt.tight_layout()
    
    safe_name = extractor_name.replace('/', '_')
    plt.savefig(OUTPUT_DIR / f'confusion_matrices_{safe_name}.png', dpi=300, bbox_inches='tight')
    print(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/confusion_matrices_{safe_name}.png")
    plt.show()


def find_typical_errors(X_test, y_test, y_pred, extractor_name, clf_name, n_examples=10):
    """
    –ó–Ω–∞—Ö–æ–¥–∏—Ç—å —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑—É—î –Ω–∞–π—Ç–∏–ø–æ–≤—ñ—à—ñ –ø–æ–º–∏–ª–∫–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
    
    Args:
        X_test: —Ç–µ—Å—Ç–æ–≤—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        y_test: —Å–ø—Ä–∞–≤–∂–Ω—ñ –º—ñ—Ç–∫–∏
        y_pred: –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –º—ñ—Ç–∫–∏
        n_examples: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –ø–æ–º–∏–ª–æ–∫
    """
    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –ø–æ–º–∏–ª–∫–∏
    errors_indices = np.where(y_test != y_pred)[0]
    
    if len(errors_indices) == 0:
        print("  ‚úì –ù–µ–º–∞—î –ø–æ–º–∏–ª–æ–∫!")
        return
    
    # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –Ω–∞–π—á–∞—Å—Ç—ñ—à—ñ –ø–æ–º–∏–ª–∫–∏ (confused pairs)
    confusion_pairs = defaultdict(int)
    for idx in errors_indices:
        true_label = y_test[idx]
        pred_label = y_pred[idx]
        pair = (true_label, pred_label)
        confusion_pairs[pair] += 1
    
    # –¢–æ–ø –ø–æ–º–∏–ª–∫–æ–≤—ñ –ø–∞—Ä–∏
    top_confusions = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)[:5]
    
    print(f"\n  üìä –¢–æ–ø-5 –Ω–∞–π—á–∞—Å—Ç—ñ—à–∏—Ö –ø–æ–º–∏–ª–æ–∫:")
    for (true_cls, pred_cls), count in top_confusions:
        print(f"    {CIFAR10_CLASSES[true_cls]:>12} ‚Üí {CIFAR10_CLASSES[pred_cls]:<12} : {count} —Ä–∞–∑—ñ–≤")
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ–º–∏–ª–æ–∫
    n_to_show = min(n_examples, len(errors_indices))
    selected_errors = np.random.choice(errors_indices, n_to_show, replace=False)
    
    n_cols = 5
    n_rows = (n_to_show + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    axes = axes.flatten()
    
    for i, idx in enumerate(selected_errors):
        ax = axes[i]
        
        img = X_test[idx]
        true_label = y_test[idx]
        pred_label = y_pred[idx]
        
        ax.imshow(img)
        ax.set_title(f'True: {CIFAR10_CLASSES[true_label]}\nPred: {CIFAR10_CLASSES[pred_label]}',
                    fontsize=9, color='red', weight='bold')
        ax.axis('off')
    
    # –í–∏–º–∫–Ω—É—Ç–∏ –∑–∞–π–≤—ñ –æ—Å—ñ
    for i in range(n_to_show, len(axes)):
        axes[i].axis('off')
    
    plt.suptitle(f'–¢–∏–ø–æ–≤—ñ –ø–æ–º–∏–ª–∫–∏: {extractor_name} + {clf_name}',
                 fontsize=14, weight='bold')
    plt.tight_layout()
    
    safe_ext = extractor_name.replace('/', '_')
    safe_clf = clf_name.replace(' ', '_')
    plt.savefig(OUTPUT_DIR / f'typical_errors_{safe_ext}_{safe_clf}.png',
                dpi=300, bbox_inches='tight')
    print(f"  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/typical_errors_{safe_ext}_{safe_clf}.png")
    plt.show()


def plot_accuracy_comparison(all_results):
    """–ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∏–π –≥—Ä–∞—Ñ—ñ–∫ accuracy –¥–ª—è –≤—Å—ñ—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π"""
    print_section("–ü–û–†–Ü–í–ù–Ø–ù–ù–Ø ACCURACY")
    
    extractors = list(all_results.keys())
    
    fig, ax = plt.subplots(figsize=(14, 7))
    
    x = np.arange(len(all_results[extractors[0]]))
    width = 0.15
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']
    
    for i, extractor in enumerate(extractors):
        accuracies = all_results[extractor]['Accuracy'].values
        offset = width * (i - len(extractors) / 2)
        ax.bar(x + offset, accuracies, width, label=extractor, color=colors[i % len(colors)], alpha=0.8)
    
    ax.set_xlabel('Classifier', fontsize=12, weight='bold')
    ax.set_ylabel('Accuracy', fontsize=12, weight='bold')
    ax.set_title('–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è Accuracy: –†—ñ–∑–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ñ–≤ —Ç–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤',
                 fontsize=14, weight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(all_results[extractors[0]].index, rotation=45, ha='right')
    ax.legend(title='Feature Extractor', fontsize=10)
    ax.grid(alpha=0.3, axis='y')
    ax.set_ylim([0, 1])
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'accuracy_comparison.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/accuracy_comparison.png")
    plt.show()


def plot_time_comparison(all_results):
    """–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —á–∞—Å—É –Ω–∞–≤—á–∞–Ω–Ω—è"""
    print_section("–ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ß–ê–°–£ –ù–ê–í–ß–ê–ù–ù–Ø")
    
    extractors = list(all_results.keys())
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # Train time
    ax = axes[0]
    for extractor in extractors:
        train_times = all_results[extractor]['Train Time (s)'].values
        ax.plot(train_times, marker='o', label=extractor, linewidth=2)
    
    ax.set_xlabel('Classifier Index', fontsize=11, weight='bold')
    ax.set_ylabel('Train Time (s)', fontsize=11, weight='bold')
    ax.set_title('–ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤', fontsize=12, weight='bold')
    ax.legend()
    ax.grid(alpha=0.3)
    
    # Inference time
    ax = axes[1]
    for extractor in extractors:
        inference_times = all_results[extractor]['Inference Time (s)'].values
        ax.plot(inference_times, marker='s', label=extractor, linewidth=2)
    
    ax.set_xlabel('Classifier Index', fontsize=11, weight='bold')
    ax.set_ylabel('Inference Time (s)', fontsize=11, weight='bold')
    ax.set_title('–ß–∞—Å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤', fontsize=12, weight='bold')
    ax.legend()
    ax.grid(alpha=0.3)
    
    plt.suptitle('–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ', fontsize=14, weight='bold')
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'time_comparison.png', dpi=300, bbox_inches='tight')
    print("‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/time_comparison.png")
    plt.show()


def create_summary_report(all_results):
    """–°—Ç–≤–æ—Ä—é—î –ø—ñ–¥—Å—É–º–∫–æ–≤–∏–π –∑–≤—ñ—Ç"""
    print_section("–ü–Ü–î–°–£–ú–ö–û–í–ò–ô –ó–í–Ü–¢")
    
    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–∫—Ä–∞—â—É –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—é
    best_accuracy = 0
    best_combo = ("", "")
    
    for extractor, results_df in all_results.items():
        max_acc_idx = results_df['Accuracy'].idxmax()
        max_acc = results_df.loc[max_acc_idx, 'Accuracy']
        
        if max_acc > best_accuracy:
            best_accuracy = max_acc
            best_combo = (extractor, max_acc_idx)
    
    print(f"\nüèÜ –ù–ê–ô–ö–†–ê–©–ê –ö–û–ú–ë–Ü–ù–ê–¶–Ü–Ø:")
    print(f"  Feature Extractor: {best_combo[0]}")
    print(f"  Classifier: {best_combo[1]}")
    print(f"  Accuracy: {best_accuracy:.4f}")
    
    # –°–µ—Ä–µ–¥–Ω—è accuracy –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
    print(f"\nüìä –°–ï–†–ï–î–ù–Ø ACCURACY –ü–û –ï–ö–°–¢–†–ê–ö–¢–û–†–ê–•:")
    for extractor, results_df in all_results.items():
        mean_acc = results_df['Accuracy'].mean()
        print(f"  {extractor:>15}: {mean_acc:.4f}")
    
    # –°–µ—Ä–µ–¥–Ω—è accuracy –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞
    print(f"\nüìä –°–ï–†–ï–î–ù–Ø ACCURACY –ü–û –ö–õ–ê–°–ò–§–Ü–ö–ê–¢–û–†–ê–•:")
    classifiers = list(all_results[list(all_results.keys())[0]].index)
    for clf in classifiers:
        accuracies = [all_results[ext].loc[clf, 'Accuracy'] for ext in all_results.keys()]
        mean_acc = np.mean(accuracies)
        print(f"  {clf:>20}: {mean_acc:.4f}")
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–≤–µ–¥–µ–Ω—É —Ç–∞–±–ª–∏—Ü—é
    summary_data = []
    for extractor, results_df in all_results.items():
        for clf_name in results_df.index:
            row = {
                'Feature Extractor': extractor,
                'Classifier': clf_name,
                'Accuracy': results_df.loc[clf_name, 'Accuracy'],
                'F1-Score': results_df.loc[clf_name, 'F1-Score'],
                'Train Time (s)': results_df.loc[clf_name, 'Train Time (s)']
            }
            summary_data.append(row)
    
    summary_df = pd.DataFrame(summary_data)
    summary_df = summary_df.sort_values('Accuracy', ascending=False)
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    summary_df.to_csv(OUTPUT_DIR / 'summary_all_results.csv', index=False)
    print(f"\n‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ: results/summary_all_results.csv")
    
    print(f"\nüìã –¢–û–ü-10 –ö–û–ú–ë–Ü–ù–ê–¶–Ü–ô:")
    print(summary_df.head(10)[['Feature Extractor', 'Classifier', 'Accuracy']].to_string(index=False))


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    print("\n" + "="*80)
    print("  TRANSFER LEARNING –¢–ê –ë–ï–ù–ß–ú–ê–†–ö–Ü–ù–ì –ö–õ–ê–°–ò–§–Ü–ö–ê–¢–û–†–Ü–í")
    print("  –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π Feature Extractors √ó Classifiers")
    print("="*80)
    
    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    X_train, y_train, X_test, y_test = load_cifar10_data(n_train=5000, n_test=2000)
    
    # 2. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞—Ç–∞—Å–µ—Ç—É
    visualize_dataset_samples(X_train, y_train)
    
    # 3. –ë–µ–Ω—á–º–∞—Ä–∫ –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ñ–≤ —Ç–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
    all_results, all_predictions, all_features, scalers = benchmark_extractors_with_classifiers(
        X_train, y_train, X_test, y_test
    )
    
    # 4. –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≥–ª–∏–±–∏–Ω–∏ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è (VGG16 —è–∫ –ø—Ä–∏–∫–ª–∞–¥)
    print("\n")
    freezing_results = study_freezing_depth(X_train, y_train, X_test, y_test, model_name='VGG16')
    freezing_results.to_csv(OUTPUT_DIR / 'freezing_depth_VGG16.csv')
    
    # –¢–∞–∫–æ–∂ –¥–ª—è ResNet50
    print("\n")
    freezing_results_resnet = study_freezing_depth(X_train, y_train, X_test, y_test, model_name='ResNet50')
    freezing_results_resnet.to_csv(OUTPUT_DIR / 'freezing_depth_ResNet50.csv')
    
    # 5. –ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω—ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
    create_comparison_heatmap(all_results)
    plot_accuracy_comparison(all_results)
    plot_time_comparison(all_results)
    
    # 6. Confusion matrices –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
    print_section("–°–¢–í–û–†–ï–ù–ù–Ø CONFUSION MATRICES")
    for extractor_name in all_results.keys():
        print(f"\n  Confusion matrices –¥–ª—è {extractor_name}...")
        plot_confusion_matrices(all_predictions, y_test, extractor_name, top_n=4)
    
    # 7. –ê–Ω–∞–ª—ñ–∑ —Ç–∏–ø–æ–≤–∏—Ö –ø–æ–º–∏–ª–æ–∫ –¥–ª—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π
    print_section("–ê–ù–ê–õ–Ü–ó –¢–ò–ü–û–í–ò–• –ü–û–ú–ò–õ–û–ö")
    
    # –ó–Ω–∞–π–¥–µ–º–æ —Ç–æ–ø-3 –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó
    top_combos = []
    for extractor, results_df in all_results.items():
        for clf_name in results_df.index:
            acc = results_df.loc[clf_name, 'Accuracy']
            top_combos.append((acc, extractor, clf_name))
    
    top_combos.sort(reverse=True)
    
    for i, (acc, extractor, clf_name) in enumerate(top_combos[:3]):
        print(f"\n[{i+1}/3] {extractor} + {clf_name} (Accuracy: {acc:.4f})")
        
        y_pred = all_predictions[extractor][clf_name]
        if y_pred is not None:
            find_typical_errors(X_test, y_test, y_pred, extractor, clf_name, n_examples=10)
    
    # 8. –ü—ñ–¥—Å—É–º–∫–æ–≤–∏–π –∑–≤—ñ—Ç
    create_summary_report(all_results)
    
    # –ü—ñ–¥—Å—É–º–æ–∫
    print_section("–ó–ê–í–ï–†–®–ï–ù–ù–Ø")
    print("\n‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("\nüìÅ –°—Ç–≤–æ—Ä–µ–Ω—ñ —Ñ–∞–π–ª–∏:")
    print("  - results/dataset_samples.png - –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–∞—Ç–∞—Å–µ—Ç—É")
    print("  - results/benchmark_*.csv - —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞")
    print("  - results/freezing_depth_*.png - –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –≤—ñ–¥ –≥–ª–∏–±–∏–Ω–∏ –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è")
    print("  - results/heatmap_all_combinations.png - heatmap –≤—Å—ñ—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π")
    print("  - results/accuracy_comparison.png - –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è accuracy")
    print("  - results/time_comparison.png - –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ")
    print("  - results/confusion_matrices_*.png - confusion matrices")
    print("  - results/typical_errors_*.png - —Ç–∏–ø–æ–≤—ñ –ø–æ–º–∏–ª–∫–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó")
    print("  - results/summary_all_results.csv - –∑–≤–µ–¥–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü—è")
    print("\n" + "="*80)


if __name__ == "__main__":
    main()

